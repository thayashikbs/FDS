[["index.html", "金融データサイエンス 1 はじめに 1.1 コースの目的 1.2 統計学/統計的手法の学習について 1.3 ビジネス応用における統計学の最近の趨勢 1.4 本書内の記載の注意点", " 金融データサイエンス 林 高樹 2025-08-10 (内容は随時更新されます) 1 はじめに 1.1 コースの目的 近年の”データサイエンス”分野の発展の中で, その支柱の分野の一つとしての 統計学の重要性が高まっている. いまや, 簡単なデータ分析であれば, ChatGPTなどの生成AIが, 人間に代わって分析を行ったり, 分析のためのプログラムを書いたりしてくれる時代となった. しかしながら, 少なくとも現状の技術水準では生成AIの出力の 正確性は保証されておらず, 生成AIの出力の正しさを確認できるのは人間である. 当面は, 人間の手による分析, 人間の頭による分析方法論の正しい理解や結果の解釈が求められるだろう. AIブームより前に始まっていた”ビッグデータ”の時代において, 多種多様かつ大量の, 組織内・外のデータを分析する技術は 会社経営において決定的に重要となっている. 一方, 統計学やデータサイエンスの一人の初学者としては, いきなり 高度なITスキルを前提とする最新の機械学習系手法を駆使したビッグデータ“解析を行おうとするのでなく, まずは, 古典的な統計的方法論を正しく“スモール・データセット”に応用できるようになることが重要である. 本コースは, 多変量解析を中心にさまざまな統計的データ分析の手法を学び, これらの手法を学術研究や実務に応用できるようになるための基盤作り目指す. 統計ソフトウェアRを利用しながら学んでいく. 1.2 統計学/統計的手法の学習について 学習の目標 統計学/統計的手法は, サイエンスとアートの二つの側面があることから, 的確な応用を行うためには, 両面を同時に, バランスよく学ぶ必要がある. 統計理論を正確に理解する 数学的に正しい概念や手続きの理解 統計手法の実践 (運用法) を学ぶ 業務経験や知識, 統計的手法の経験則, 費用対効果等の判断 ※ 統計学は数学(の一分野)ではない (数学を道具として学問体系が作られいる) 一方の理解が不十分であると, 妥当な分析が行えず, 不正確あるいは間違った分析結果や解釈につながるリスクがある. 統計学/統計的手法の学習方法 3つの要素・ルート, それに対応した教科書・参考書がある. すなわち, 入門 (文章&amp;図表主体): 手法の概念, 用途, 特徴の大雑把な理解を図る 理論 (数式主体): 手法の理論的・技術的側面, 詳細の正確な理解を図る 実習 (コード主体): 手を動かすことで手法を経験, 実践力をつける 統計学を学習するにあたっては, 理解の段階に応じて, これらの要素を, 少しずつ万遍なく学びながら, “スパイラル”状に次の段階に進んでいくのが最も効果的であると筆者は考える. すなわち, 理論の学習を全くやらず, 入門と実習のみを学習するようなアプローチは, 理論的理解のないまま統計的分析を実践することになるため危険である. 筆者の経験上, プログラミングの得意な”エンジニア系”のデータサイエンティストにはそのような傾向を持つ人が少なからずいると感じている. 書籍ごとに目的や想定する読者層は異なり, それに対応するようにこれらの要素の割合が異なる. したがって, 学習者は自身の学習目的に照らして適切な本を選択する必要がある. 本書では, Rを用いながら代表的な統計手法を学んでいく. 統計学の教科書例 Rコードによる分析例を示しながら, 各手法や理論の解説を行っている書籍は 多数存在するが, バランス良くこれらを配置していると筆者が感じる教科書のタイトルを幾つか紹介する. 【統計学/R】 山田剛史, 杉澤武俊, 村井潤一郎 (2008), Rによるやさしい統計学, オーム社 【データ分析/R】 Kosuke Imai (2017), Quantitative Social Science: An Introduction, Princeton University Press (今井耕介(著), 粕谷祐子, 原田勝孝, 久保浩樹 (訳) (2018), 社会科学のためのデータ分析入門(上)(下), 岩波書店) 【機械学習/R】 R. James, G., Witten, D., Hastie, T., Tibshirani (2013), An Introduction to Statistical Learning: with Applications in R, Wiley. (James他(著), 落海浩, 首藤信通 (訳) (2018), Rによる統計的学習入門, 朝倉書店) 参考として, 次の書籍は, コードを載せずまた数式を使った説明も殆どなしに, 文章主体で (計量経済学の) 手法の概念や分析結果の解釈の仕方を平易に説明している良書である. 山本 勲 (2015), 実証分析のための計量経済学, 中央経済社 統計学/統計的手法の学習ステップ 入門・初級ステップ - レベル①: ソフトウェアを正しく動かせる - 目的に応じた適切な手法の選択 - 適切なデータの加工, ソフトウェアの操作 - 出力結果 (帳票, 図表) の正しい見方 - レベル②: 手法の背後にある理論を理解する - (②A) 概念や定義の正しい理解 (言葉やイメージ) - (②B) 数式による厳密な理解 ※ ①の達成度を高めるためには, ②の理解を高める必要 ① ⇒ ② ⇔ ① 中級ステップ - 特定の分野における (計量経済学, 心理学, 疫学, …) 統計的手法の理解と実践が出来るようになる 1.3 ビジネス応用における統計学の最近の趨勢 統計学, さらには中核分野として内包するデータサイエンス分野において扱う 対象データの特徴として以下のような傾向がみられる 大規模化 (“ビッグデータ”) レコード数 n → 大, 変数の数 p → 大 データ数より説明変数が多い場合も （“n&lt;p問題”) 従来の統計学: 「n 小・中規模」, かつ, 「n&gt;p」 高頻度・高速化 (従来) 四半期・月次… → 1日内, 秒, ミリ秒, …, リアルタイム 非構造化 画像, 音声, テキスト等 自動化 衛星画像, アクセスログ, IoTデータ, ウェアラブル・データ, … “マルチモーダル”化 テキスト・画像・音声・動画など複数の種類のデータを一括して処理 (AIによる)自動生成 一方, 経営(学)分野への応用の観点では次のような傾向がある. 文章や発言内容の自然言語処理・テキスト解析技術の重要性の高まり BERT, GPT-4, … 生成系AI技術の活用 テキスト (ChatGPT, Claude, Gemini, Microsoft Copilot, …), 画像 (DALLE-E, Stable Solution, Canva AI, Adobe Firefly, Midjourney, …), 音声・音楽 (ElevenLabs, Voicemod AI, Suno AI, Boomy, Soundraw, …), 動画 (Runway ML, Pika Labs, Synthesia, HeyGen DeepBrain AI, …), 等 複数のデータソースの有機的な組合せ活用の重要性 財務諸表等の“ハードデータ” × SNS等から得られた“ソフトデータ” 外部ソース・データ × 社内業務データ … 1.4 本書内の記載の注意点 読者への注) パス名は、各自のPC環境に応じて適宜変更すること "],["r言語の基本.html", "2 R言語の基本 2.1 Rの基本プログラミング 2.2 データの型や構造 2.3 データの操作・演算 2.4 R関数 2.5 データの可視化 2.6 ファイル入出力 2.7 パッケージtidyverse", " 2 R言語の基本 2.1 Rの基本プログラミング 主な参考文献： 金 (2017),『Rによるデータサイエンス』, 森北出版. 山田他 (2008),『Rによるやさしい統計学』, オーム社. Venables, Smith, and R Development Core Team (03), R入門. http://minato.sip21c.org/swtips/R-jp-docs/R-intro-170.jp.pdf R Core Team (2024). R Language Definition, R Foundation for Statistical Computing. https://cran.r-project.org/doc/manuals/r-release/R-lang.pdf 本コースは, 基本的なRプログラミングにもっぱら限定 よりモダンなプログラミング (本コース終了後) → tidyverse 例. 松村他, 『RユーザーのためのRStudio[実践]入門]』, 技術評論社 Rコーディングスタイルの例 Google, “Google’s R Style Guide”, https://google.github.io/styleguide/Rguide.html Hadley Wickham, “Tidyverse Style Guide”, https://style.tidyverse.org/ 2.1.1 基本操作 数値 (ベクトル), 演算の直接評価 2 + 3 ## [1] 5 c(1, 2, 3, 4) ## [1] 1 2 3 4 1:4 ## [1] 1 2 3 4 変数xに値を格納. 変数xに対する演算 - 基本形: 変数名 &lt;- 代入する値 x &lt;- c(1, 2, 3, 4, 5) x = c(1, 2, 3, 4, 5) x ## [1] 1 2 3 4 5 (x &lt;- c(1, 2, 3, 4, 5)) # 代入と表示を同時に実行 ## [1] 1 2 3 4 5 x^2 ## [1] 1 4 9 16 25 x**2 ## [1] 1 4 9 16 25 xに関数を適用 mean(x) ## [1] 3 var(x) ## [1] 2.5 sd(x) ## [1] 1.581139 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 # sqrt, summary, ... その他, R言語の基本 - 空白は無視される - Pythonと異なり, インデントは意味を持たない - 一行に二つのコマンドを入力する場合は, 間をセミコロン (;) で区切る - 行頭がシャープ (#) で始まる行は丸々無視される (コメント行) - 中括弧 {} は通常、コードのブロックを作成するために使用. 主に, 条件文、ループ、関数などのブロック構造を定義する際に使用. 2.1.2 基本構文 Rでは, データの加工や分析を行う際などに, 分析者自らの手で処理の手順をプログラミングをすることができる. forループ # 繰り返し処理 (forループ) for (変数名 in 変数のリスト){ 1回分の処理内容 } ``` #### if文 {-} ``` # 条件分岐 (if文) # if (条件式) 処理1 else 処理2 x &lt;- 0 for(i in 1:10){x &lt;- x + i} x # aaa &lt;- c(1, 3, 5) for (a in aaa) print (a) ## [1] 55 ## [1] 1 ## [1] 3 ## [1] 5 2.1.3 自作関数 同様な処理を”パラメータ”を変えながら何度も実行する場合は, 関数を作っておくと便利である. ※ 関数に付与する名前として, Rですでに使われている関数名や, Rで特別な意味を持つ値 (T, Fなど)は避けること # 自作関数の作成 関数名 &lt;- function(引数1, 引数2, ...){ 処理内容 } myfunc &lt;- function(y){ x &lt;- 0 for (i in 1:y) x &lt;- x + i return(x) } # 実行例 myfunc(10) myfunc2 &lt;- function(y){ if(y &gt; 10) print(&quot;yes&quot;) else print(&quot;no&quot;) } # 実行例 myfunc2(5) ## [1] 55 ## [1] &quot;no&quot; 2.1.4 パッケージのインストール &amp; 読み込み #lda # lda関数 → このままだエラー発生 library() # インストール済パッケージ一覧 search() # 読み込み済みパッケージ一覧 library(MASS) # MASSパッケージの読み込み(ロード) search() # アタッチされたパッケージのリスト表示 lda install.packages(&quot;DAAG&quot;) # http://cran.r-project.org # http://cran.r-project.org/web/packages/googleVis/index.html # パッケージインストローラー 2.1.5 ヘルプ 関数のヘルプ R関数helpを使うか, RStudioのプルダウンメニューやHelpペインを使用 help(&quot;fivenum&quot;) # 関数fivenumのヘルプ ?fivenum 2.2 データの型や構造 ここで, R言語の基礎を理解するのに重要な二つの概念について, 初心者を念頭に正確性を犠牲にしながら概要について述べる. 実際はここでの記載よりもはるかに複雑で, 技術的にも難易度が高い. 包括的かつ技術的に正確な内容については, 例えば, https://adv-r.hadley.nz を参照されたい. 2.2.1 データの値の種類 (“データ型”) Rでは, データの取る値の主要な種類 (type) として, 実数型 (double), 整数型 (integer), 文字列型 (character), 論理型 (logical) がある. また, 実数型, 整数型はまとめて数値型 (numeric) とも呼ばれる. 初心者は, 実数型と整数型の違いは気にしなくても良い. # 実数型 3.14 2.718 # 整数型 1L 5L # 文字列型 &quot;KBS&quot; &quot;日吉&quot; # 論理型 TRUE # または, T FALSE # または, F 次に, データの値の種類として, 上記以外に応用上知っておきたいものとして, 因子型 (factor),日付型 (Date)がある. 因子型は, カテゴリーデータに対して, 日付型は日付や時刻を表すデータに対して使うことができる. Rでは, カテゴリーデータ (ベクトル) を因子型としてオブジェクトに格納しておけば, その後の統計分析においてわざわざダミー変数を作る操作は (おおむね) 不要となる. また, 日付型として格納したデータは日付や時間に関する処理において効果を発揮する. 与えられたデータに対して, R組み込み関数である factor(), as.Date() を適用することでこれらの型に変換することができる. 少しだけ発展的な内容になるが, 因子型は整数型を値に持つベクトル, 日付型は実数型を値に持つベクトルとしてR内部で扱われる (ベクトルやオブジェクトについては次に述べる). # 因子型 factor(c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;, &quot;M&quot;, &quot;L&quot;, &quot;M&quot;)) # L/M/Hの3水準の因子型ベクトル (長さ5) # 日付型 as.Date(&quot;2023-10-02&quot;) 2.2.2 データの配列の仕方 (“データ構造”) Rでは, データの配置の仕方の種類の主要なものとして, ベクトル (vector), リスト (list), 行列 (matrix), 配列(array), データフレーム (data frame) などがある. 分析に応じて, 適切なデータの構造にして処理を行う必要がある. # ベクトル c(3.14, 2.718) c(&quot;KBS&quot;, &quot;日吉&quot;) # リスト list(&quot;KBS&quot;, 1962L, 1:10) # 行列 matrix(1:8, nrow = 2, byrow = T) # 配列 arra(1:12, c(2, 3, 2)) # データフレーム data.frame(name = c(&quot;Steve&quot;, &quot;Top&quot;), income = c(40000, 50000)) ちなみに, これらの”データ構造”には階層関係があり, 行列や配列はベクトルの特別な場合, リストはベクトルの特別な場合, データフレームはリストの特別な場合である. データフレームは, リスト (異なる種類のデータを同時に要素として持つ) でありながら, リストの各要素 (ベクトル) の長さが等しく, 2次元の行列の形式にデータが並べられたものである. R言語では, ベクトルが最も基本的な”データ構造”である. Rを用いた統計分析では, データフレームを用いるケースが非常に多いため, データフレームを使えるようになることが必須である. Rで分析を行う場合には, データや関数 (データ処理するための手続きを書いたコード) をオブジェクト (object) と呼ばれる”箱”に名前を付けて一旦格納し, その名前を呼び出す形で処理を実行するのが便利である. 量的変数や質的変数を同時に持つデータセットの分析には, データフレームが便利である. オブジェクトにはクラス (class) というオブジェクトの持つデータ構造の種類の属性が付与される. なお, Rには, type, class, modeの3つの”型”が存在し混乱しやすい. 初心者は違いを気にする必要はなく, 大雑把に, 上の”データ型”は関数 typeof(), “データ構造”は関数 class() により調べることができると知っていれば十分である. 興味のある読者は以下を参照: https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Attributes 特別な値 Rにおける分析において注意や対処が必要な, データの取り得る特別な値として, - NA (欠損値) - NULL (非存在) - NaN (非数値) - Inf (無限大) これらの値をテストする関数が用意されている. is.na() is.null() is.nan() is.infinite() is.finite() # NAの含まれている例 x &lt;- c(1, NA, 3, 4, 5) x == NA ## [1] NA NA NA NA NA is.na(x) ## [1] FALSE TRUE FALSE FALSE FALSE mean(x) ## [1] NA mean(x, na.rm = T) ## [1] 3.25 # NULLの含まれている例 x &lt;- 1:5 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x ## a b c d e ## 1 2 3 4 5 names(x) &lt;- NULL x ## [1] 1 2 3 4 5 # NaN, Infの発生例 0 / 0 ## [1] NaN 1 / 0 ## [1] Inf 上の”データ型”や”データ構造”を調べる関数も用意されている. # 整数値を持つ行列の例 abc &lt;- matrix(1:8, nrow = 2, byrow = T) is.numeric(abc) ## [1] TRUE is.integer(abc) ## [1] TRUE is.matrix(abc) ## [1] TRUE typeof(abc) ## [1] &quot;integer&quot; class(abc) ## [1] &quot;matrix&quot; &quot;array&quot; mode(abc) ## [1] &quot;numeric&quot; str(abc) ## int [1:2, 1:4] 1 5 2 6 3 7 4 8 2.2.3 ベクトル # 以下は, 互いに等価 aaa &lt;- c(2, 4, 6, 8) # 変数aaaに数値ベクトル(2,4,6,8)を割り当てる aaa = c(2, 4, 6, 8) aaa = seq(2, 8, 2) c(2, 4, 6, 8) -&gt; aaa aaa &lt;- 1:4 * 2 # assign(&quot;aaa&quot;, c(2, 4, 6, 8)) # 値を割り当てる際に, &quot;環境&quot;を指定することができる # ベクトルの長さ length(aaa) # 文字列ベクトル bbb &lt;- c(&quot;東京&quot;, &quot;埼玉&quot;, &quot;千葉&quot;, &quot;神奈川&quot;) # ベクトルの各要素に名前 (ラベル) を付与 names(aaa) &lt;- bbb # ベクトル要素の取り出し bbb[1] bbb[c(2, 4)] bbb[c(T, F, T, F)] # インデックスの値がT (TRUE) の要素の取り出し bbb[c(T, F, F)] # 注意 ## [1] 4 ## [1] &quot;東京&quot; ## [1] &quot;埼玉&quot; &quot;神奈川&quot; ## [1] &quot;東京&quot; &quot;千葉&quot; ## [1] &quot;東京&quot; &quot;神奈川&quot; 2.2.4 行列 matrix(0, 3, 4) # 全要素0の3x4-行列 matrix(0:4, 3, 4) # 行列の値に使うベクトル (0:4) の長さと 行数 (3)・列数 (4) が不一致 ## Warning in matrix(0:4, 3, 4): data length [5] is not a sub-multiple or multiple ## of the number of rows [3] ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [,1] [,2] [,3] [,4] ## [1,] 0 3 1 4 ## [2,] 1 4 2 0 ## [3,] 2 0 3 1 ccc &lt;- matrix(c(3, 2, 1, 6, 5, 4), 2, 3) # 2x3-行列 ccc[1, 1] # (1, 1)成分 ## [1] 3 ccc[1, ] # 第1行(行ベクトル) ## [1] 3 1 5 ccc[, 1] # 第1列(列ベクトル) ## [1] 3 2 ccc[-2, ] # 2行目を除く → 2x4-行列 ## [1] 3 1 5 ccc[, -2] # 2列目を除く → 3x3-行列 ## [,1] [,2] ## [1,] 3 5 ## [2,] 2 4 dim(ccc); nrow(ccc); ncol(ccc) # セミコロン(;)により, 複数のコマンドを1行に収め, 順次実行 ## [1] 2 3 ## [1] 2 ## [1] 3 ccc[2, 3] &lt;- 10 # (2, 3)成分に値10を代入 # 行列にラベルを付与 colnames(ccc) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) # 列ラベル rownames(ccc) &lt;- c(&quot;2012&quot;, &quot;2013&quot;) # 行ラベル ccc ## 大阪 京都 名古屋 ## 2012 3 1 5 ## 2013 2 6 10 t(ccc) # 転置 ## 2012 2013 ## 大阪 3 2 ## 京都 1 6 ## 名古屋 5 10 2.2.5 リスト ベクトル, 行列, 配列, リスト等の異なる型(&amp;異なる長さ)のオブジェクトを一つにまとめたオブジェクト L1 &lt;- list(rep(&quot;A&quot;, 3), 1:0, matrix(1:8, 2, 4)) L1[[1]] # 1番目の要素(変数)の取り出し k &lt;- list (name = &quot;Taro&quot;, salary = 50000, male = T) k2 &lt;- list (&quot;Taro&quot;, 50000, T) # 要素名 (タグ)なしの場合 k$sal # 要素名は省略形可 # リストはベクトルの一種 (recursive vector) # 一方, 通常のベクトルはatomic vector (それ以上分解できない) # vector()からリスト生成する場合 z &lt;- vector (mode = &quot;list&quot;) z[[&quot;abd&quot;]] &lt;- 5 k[1:2] # 元のリストの部分リスト k2 &lt;- k[2] class(k2); str(k2) k2a &lt;- k[[2]] # 2番目の要素(変数)の取り出し (要素の型を持つ結果を返す) # k[[1:2]] # --&gt; エラー class(k2a); str(k2a) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [1] 50000 ## $name ## [1] &quot;Taro&quot; ## ## $salary ## [1] 50000 ## ## [1] &quot;list&quot; ## List of 1 ## $ salary: num 50000 ## [1] &quot;numeric&quot; ## num 50000 リストの要素追加・削除 z &lt;- list(a = &quot;abcd&quot;, b = 10) z$c &lt;- &quot;piano&quot; z[[4]] &lt;- 15 z[5:6] &lt;- c(TRUE, FALSE) z$b &lt;- NULL # xxx &lt;- 1:10 yyy &lt;- 0.5 * xxx + rnorm(10) lm_res &lt;- lm(yyy ~ xxx) is.list(lm_res) lm_res[[1]] lm_res$coef lm_res[&quot;coefficients&quot;] ## [1] TRUE ## (Intercept) xxx ## 0.07544151 0.45963969 ## (Intercept) xxx ## 0.07544151 0.45963969 ## $coefficients ## (Intercept) xxx ## 0.07544151 0.45963969 2.2.6 データフレーム リストの特別な場合 長さが等しい複数のベクトルを要素に持つリスト 数値と文字列などの異なるデータが混在するデータを行列のように扱える Rにおける様々な統計分析において多用される kids &lt;- c(&quot;taro&quot;, &quot;hanako&quot;) ages &lt;- c(10, 8) d &lt;- data.frame(kids, ages, stringsAsFactors = FALSE) # 注: stringsAsFactors = T: 文字ベクトルをfactorとして扱う d str(d) # 以下の3つは等価な操作 d[[1]] # データフレームの第1列 (リストの一番目の要素) を取り出す(→ 文字列ベクトル) d$kids # 変数(kids)のように取り出す d[, 1] # 行列のように操作 (--&gt; 便利) # ただし, d[1] # 第1列をデータフレーム (リスト) として取り出す df1 &lt;- data.frame(letters[1:3], 3:1) rownames(df1) &lt;- c(&quot;大阪&quot;, &quot;京都&quot;, &quot;名古屋&quot;) colnames(df1) &lt;- c(&quot;方言種類&quot;, &quot;順位&quot;) class(df1) is.vector(df1) ## kids ages ## 1 taro 10 ## 2 hanako 8 ## &#39;data.frame&#39;: 2 obs. of 2 variables: ## $ kids: chr &quot;taro&quot; &quot;hanako&quot; ## $ ages: num 10 8 ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## [1] &quot;taro&quot; &quot;hanako&quot; ## kids ## 1 taro ## 2 hanako ## [1] &quot;data.frame&quot; ## [1] FALSE 2.3 データの操作・演算 2.3.1 ベクトルの結合, ソート vec1 = 1:4 vec2 = 2:5 rbind(vec1, vec2) # ベクトルの行方向への結合 cbind(vec1, vec2) # べクトルの列方向への結合 vec3 &lt;- c(2, 5, 1, 3) sort(vec3) # 昇順 rev(vec3) # 順番を逆転させる ccc[, order(ccc[&quot;2012&quot;, ])] ccc[, sort.list(ccc[&quot;2012&quot;, ])] ## [,1] [,2] [,3] [,4] ## vec1 1 2 3 4 ## vec2 2 3 4 5 ## vec1 vec2 ## [1,] 1 2 ## [2,] 2 3 ## [3,] 3 4 ## [4,] 4 5 ## [1] 1 2 3 5 ## [1] 3 1 5 2 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 ## 京都 大阪 名古屋 ## 2012 1 3 5 ## 2013 6 2 10 2.3.2 二項演算 x &lt;- c(1, 3, 5, 2); y &lt;- c(-3, 1, -1, -2) x + y x * y x / y x ^ 2 x &lt; y ## [1] -2 4 4 0 ## [1] -3 3 -5 -4 ## [1] -0.3333333 3.0000000 -5.0000000 -1.0000000 ## [1] 1 9 25 4 ## [1] FALSE FALSE FALSE FALSE 2.3.3 論理演算 lx &lt;- c(T, T, F); ly &lt;- c(F, F, F) lx &amp; ly lx &amp;&amp; ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない lx | ly lx || ly # 最初の要素間の論理演算が成り立つと, 以降の演算は行わない 2.3.4 条件式 # ==, &gt;, &lt;, &gt;=, &lt;= # &amp;&amp;, || 2.3.5 行列演算 A &lt;- matrix(c(1, 2, 3, 4, 5, 6), 3, 2) B &lt;- matrix(c(2, 1, -1, -2), 2, 2) A %*% B # 行列の積 # diag # 対角行列 # solve # 逆行列 ## [,1] [,2] ## [1,] 6 -9 ## [2,] 9 -12 ## [3,] 12 -15 2.4 R関数 2.4.1 数学基本関数 # sum; sqrt; abs # exp; log; log10; log2; sin; cos # round; ceiling; floor 2.4.2 基本統計量の計算 # mean, max; min; range; median; quantile # var; sd # summary # table # cov; cor 統計 &lt;- c(rep(&quot;好き&quot;, 8), rep(&quot;嫌い&quot;, 7)) 数学 &lt;- c(rep(&quot;好き&quot;, 6), rep(&quot;嫌い&quot;, 9)) table(統計, 数学) # クロス集計表, ベクトルは同一長 ## 数学 ## 統計 好き 嫌い ## 好き 6 2 ## 嫌い 0 7 2.4.3 確率分布 # dxxx(q) # 確率密度, q:確率点 # pxxx(q) # 累積確率, q:確率点 # qxxx(p) # 確率点, p:確率 # rxxx(n) # 乱数, n:個数 # ------------------------------------------------------------------ # xxx部分: # unif(x, min, max) # 一様分布 # norm(x, mean, sd) # 正規分布 # exp(x, rate) # 指数分布 # binom(x, size, prob) # ２項分布 # pois(x, lambda) # ポアソン分布 # t(x, df) # t分布 # chisq(x, df) # カイ2乗分布 # f(x, df1, df2) # F分布 curve(xを含んだ式, from = xの左端点, to = xの右端点) # 関数のグラフ描画 curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4) curve(dnorm(x, mean = 1, sd = 2), from = -4, to = 4, add = T) # 問：t分布(自由度4)の形状は? 2.4.4 その他便利な関数 # sweep # scale # ifelse ifelse(統計 == &quot;好き&quot;, 1, 0) # apply (X, MARGIN, FUN, ...) apply(ccc, 1, sum) apply(ccc, 2, sum) colMeans(ccc) rowMeans(ccc) ## [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 ## 2012 2013 ## 9 18 ## 大阪 京都 名古屋 ## 5 7 15 ## 大阪 京都 名古屋 ## 2.5 3.5 7.5 ## 2012 2013 ## 3 6 2.5 データの可視化 # 棒グラフ barplot(ccc) barplot(ccc, beside = T) barplot(ccc, beside = T, col = c(&quot;lightblue&quot;, &quot;lavender&quot;), main = &quot;test&quot;) # apply(ccc, 1, pie) # pie # hist # 折れ線グラフ (行列の各列(変数)の同時プロット) matplot(ccc, type = &quot;l&quot;) matplot(t(ccc), type = &quot;l&quot;) # 箱ひげ図 boxplot(ccc) boxplot(t(ccc)) # 散布図 # plot pairs(ccc) #install.packages(&quot;car&quot;) #library(car) #scatterplot(ccc) # install.packages(&quot;scatterplot3d&quot;); library(scatterplot3d) # scatterplot3d # その他のグラフ # coplot; mosaic plot; stars; faces; persp; image; contour # その他 # windows() #新しいグラフィック・ウィンドウを開く # par(mfrow = c(2, 2)) # より洗練されたグラフ. やや難易度が高いがモダンなアプローチ # install.packages(&quot;ggplot2&quot;, dependencies = T) library(ggplot2) ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) + geom_point(aes(colour = Species)) + geom_smooth(method = &quot;lm&quot;, colour = &quot;lightblue&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.6 ファイル入出力 2.6.1 テキストファイル読み込み ファイル読み込み用のR関数には, ファイルの格納場所 (パス) とファイル名を知らせる必要がある パスを省略すると, 現在のディレクトリ (getwd関数で確認可能) 下でファイルを探す. もし, 存在しなければ, エラーとなる ここでは, あらかじめ, 各自のPCのデスクトップ上に, “BStat_2024”という名前のフォルダ (ディレクトリ) を作成していると想定 ファイルは, カンマ(, )で区切られたcsv形式や, タブで区切られたtsv形式の テキストファイルであるとする. data1 &lt;- read.table(&quot;/[パス名]/ファイル名&quot;, header = T, row.names = 1) # オプション # header = T: 1行目が列ラベル # row.names = 1: 1列目が行ラベル または, file.path関数を使ってファイルの格納されているパス(経路)を指定しても良い. # ユーザー(yamada)が, デスクトップ(Desktop)フォルダの下に授業用フォルダ(BStat_2024)を作成した場合のパスの指定 fpath &lt;- file.path(&quot;~&quot;, &quot;Desktop&quot;, &quot;BStat_2024&quot;) # または fpath &lt;- file.path(&quot;Users&quot;, &quot;yamada&quot;, &quot;Desktop&quot;, &quot;BStat_2024&quot;) # ifile &lt;- file.path(fpath, &quot;) # 例えば, # data1 &lt;- read.table(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data.txt&quot;, sep = &quot;, &quot;) # または # data1 &lt;- read.csv(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data.txt&quot;, header = T, row.names = 1) # 代替的に # data2 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data.txt&quot;, sep = &quot;, &quot;) #matrix(scan(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data.txt&quot;, sep = &quot;, &quot;), 3, 4, byrow = T) # matrix(scan(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data.txt&quot;, sep = &quot;, &quot;), 3, 4) # scan()において, 数値, 文字が混在している場合, 列ごとにデータ属性を指定する必要 # data2.txt # a 1 2 # b 2 3 # c 3 4 # data3 &lt;- scan(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data.txt&quot;, sep = &quot;, &quot;, list(x = &quot;&quot;, y = 0, z = 0)) # data.frame(data2) # データフレーム化 # パスを指定せずに, テキストファイルの置かれているフォルダに移動してから # ファイル名のみを使って読み込んでも良い # setwd(&quot;/Users/[アカウント名]/Desktop/BStat_2024&quot;) data1 &lt;- read.table(&quot;data.txt&quot;, sep = &quot;, &quot;) # パッケージ&quot;foreign&quot;により, SAS, SPSS等のファイル形式のデータの読み込みが可能 2.6.2 テキストファイル書き出し write(data3, &quot;/[パス名]/ファイル名&quot;) # 例えば, # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BStat_2024/data3_out1.txt&quot;) # write.table(data3, &quot;/Users/[アカウント名]/Desktop/BStat_2024/data3_out1.txt&quot;, append = T) # write.csv() # sink(&quot;/Users/[アカウント名]/Desktop/BStat_2024/data3_out1.txt&quot;) data1; data2 sink() 2.7 パッケージtidyverse tidyverseは, Hadley Wickhamによって開発が進められているRパッケージ (群) である. データのインポート, 整理, 加工, 可視化, 分析を簡単かつ効率的に行うための一連のツールを提供する. tidyverseの中核をなすパッケージには以下のものがある: ggplot2: データの可視化を行うためのパッケージ. レイヤーの概念を用い, データポイント, 統計的変換, スケール, 軸, 凡例など, グラフの各要素を個別に定義し, 組み合わせることができる. これにより, 高度にカスタマイズされたグラフを容易に作成可能. dplyr: データの操作と変形を行うためのパッケージ. フィルタリング, 並べ替え, 集約など, データフレームに対する一般的な操作を簡単かつ直感的に行うための関数を提供. tidyr: データの整理と整形を簡単にするためのパッケージ. データセットのレイアウトを整形する等のクリーニングのタスクに対応しながら,「tidy形式」としてデータを再構築するツールを提供. 例えば, データを多数の列に広げる「wide形式」と データをより少ない列にまとめるが行を増やす「long形式」間の変換, 欠損値への適切な対処, 一列を複数列に分割あるいは複数列を一列に結合する等の処理. readr: さまざまな形式のテキストデータ (例えば, csv, tsv形式) を読み込み, Rのデータフレームとして効率的にインポートするためのパッケージ. 標準のR関数よりも高速に動作し, ファイルの読み込み時によくある問題 (データ型の自動認識, 欠損値の扱い等) をより柔軟に処理. さらに, 便利な機能を持つパッケージとして, purrr: リストと関数型プログラミングを扱うためのパッケージ. リストの操作, 要素の繰り返し処理, 条件に基づく要素の抽出など, 複雑なデータ構造の操作を簡単にする関数を提供. tibble: データフレームをより現代的かつ柔軟に扱うためのパッケージで, 印刷時の見やすさ, 列名の非標準的な文字の扱い, サブセット操作の改善など, データフレームを強化し使いやすさを改善. stringr: 文字列データの操作を行うためのパッケージ. Rの標準文字列操作機能よりも一貫性と可読性に優れたインターフェースを提供し、文字列の検索, 置換, 分割, 結合などのタスクを簡単に行うことが可能. forcats: 因子 (カテゴリカルデータ) を扱うためのパッケージ. 因子水準の順序変更, 要約, 結合, 分離など, 因子型のデータを操作するための便利な関数を提供. lubridate: 日付と時刻のデータを扱うためのパッケージ. 日付や時刻の加算・減算, 部分的な抽出, 時間差の計算など, 操作を直感的かつ効率的にするための関数を提供. tidyverseは, データを「tidy」（整然とした）形式で扱うことに焦点を当てている. tidyデータの原則では, 各変数が列に, 各観測値が行に, 各種類の観測単位がテーブルに配置される. この原則に従うことで, データ分析がより直感的で効率的になる. tidyverseパッケージは, Rでのデータ分析作業を容易にし, コードをより読みやすく, 書きやすくすることを指向している. それぞれのパッケージは単独で使用することも出来るが, 一緒に使用することでより使い勝手が向上し便利である. データサイエンスにおける日常的なタスクを簡潔に, かつ効率的に行うための強力なツールセットと言える. tidyverseのホームページ https://tidyverse.tidyverse.org/ 2.7.1 Q: Rの初心者はtidyverseから勉強することは可能か? 今日では, R言語の基本を学ばずにいきなりtidyverseから勉強することは, tidyverseを入口としてR言語の学習を始めるユーザーも多いと思われる. 特にデータ分析やデータサイエンスに焦点を当てている初心者にとっては代替的な選択肢である. tidyverseは, データの取り扱いを直感的かつ効率的にすることを目的として設計されており, その構文は初心者にとって学びやすいように工夫されている. tidyverseの利点: 直感的な構文: tidyverseの関数は覚えやすく, 理解しやすい構文を持っているため, R言語の初心者でも扱いやすい. データ分析のワークフローを強化: tidyverseはデータのインポート, 整理, 加工, 可視化, 分析という一連のデータ分析プロセスに対応するツールを提供する. これにより, データ分析の基本的な流れを簡単に学ぶことができる. 広範なコミュニティとサポート: tidyverseはRユーザー内に多くの熱狂的なファンがいて, コミュニティを形成している. オンラインでのサポートや学習リソースが豊富にある. 注意点: R言語の基本概念の理解の必要性: R言語の基本的な概念 (変数の割り当て, 関数の使用方法, データ型など) は、tidyverseを効率的に使用するためにも理解しておく必要あり. 限定的な機能: tidyverseだけではカバーできないR言語の機能も多くある. すなわち, tidyverseから学習を始めても, いずれはR言語のより広範な機能やパッケージにも目を向けることが必要. tidyverseに含まれるパッケージを利用すると, 確かに多くの複雑な処理が簡潔かつエレガントに書けたりすることがあり, その機能性を実感することも多い. しかし, Rプログラミングを行っている中で, R言語の基本を知らないと困るようなことの方が多い. よって, 筆者の考えでは, Rの初学者はいきなりtidyverse系を軸に学習を開始するよりは, tidyverseに含まれるパッケージはあくまでRの多数あるパッケージの一部であると位置付けて, 標準的なRを学びながらこれらのパッケージの用法を学ぶというスタンスで良い. 本コースは, tidyverse系の扱いはこのような方針に従って進めるものとする. 2.7.2 tidyverseの基本 サンプルコードの出所: ChatGPT (GPT-4) 以下, tidyverseの基本を理解するために, データのインポート, 加工, そして可視化のステップを含むシンプルなチュートリアルを紹介する. ここでは, tidyverseの中のreadr, dplyr, ggplot2の三つのパッケージを使用する. これらはtidyverseの中で最もよく使用されるパッケージである. ステップ 1: tidyverseをインストールして読み込む まず, tidyverseパッケージをインストールし, ライブラリに読み込む. # tidyverseパッケージのインストール # install.packages(&quot;tidyverse&quot;) # ライブラリに読み込む library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors ステップ 2: データをインポートする tidyverseには様々なサンプルデータが含まれている. ここでは, mtcarsデータセットを使用する. mtcarsは, 1974年のMotor Trend US誌に掲載された32台の自動車に関するデータである. # mtcarsデータセットを使用する data &lt;- mtcars ステップ 3: データを加工する dplyrを使用してデータを加工する. ここでは, mpg（ガロンあたりのマイル数）が20を超える車両のみを選択し、cyl（シリンダー数）ごとの平均mpgを計算する. # dplyrを使ってデータをフィルタリングし、集約する filtered_data &lt;- data %&gt;% filter(mpg &gt; 20) %&gt;% group_by(cyl) %&gt;% summarise(mean_mpg = mean(mpg)) # 結果を表示 print(filtered_data) ## # A tibble: 2 × 2 ## cyl mean_mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 ## 2 6 21.1 ステップ 4: データを可視化する 最後に, ggplot2を使ってデータの可視化を行う. ここでは, cylごとのmean_mpgを棒グラフで表示する. # ggplot2を使ってデータを可視化 ggplot(filtered_data, aes(x = factor(cyl), y = mean_mpg)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;blue&quot;) + theme_minimal() + labs(title = &quot;Cylinder-wise Mean MPG&quot;, x = &quot;Number of Cylinders&quot;, y = &quot;Mean MPG&quot;) 関数ggplot()の別の使用例として, マルチパネル化した散布図を示す. # データを可視化する # mpgとhpの関係を示す散布図を作成し、cylごとに異なるパネルに表示する ggplot(data, aes(x = mpg, y = hp)) + geom_point() + facet_wrap(~cyl) + theme_minimal() + theme(panel.background = element_rect(fill = &quot;gray&quot;)) + labs(title = &quot;Scatterplot of MPG vs HP by Cylinder&quot;, x = &quot;Miles Per Gallon (MPG)&quot;, y = &quot;Horsepower (HP)&quot;) "],["線形回帰分析.html", "3 線形回帰分析 3.1 重回帰分析の基本操作 3.2 変数の選択 3.3 説明変数に質的変数を含む回帰 3.4 説明変数に質的変数を含む回帰 (2) 3.5 多項式回帰 3.6 関数factor()について", " 3 線形回帰分析 はじめに, コードの可読性を高めるため, パッケージtidyverseをロードしておく. 例えば, tidyverse内にあるパッケージmagrittrの提供する機能であるパイプ (演算子) %&gt;% を関数head()と組合せて使用し, 出力量を抑える. library(tidyverse) 3.1 重回帰分析の基本操作 データ1: 1ルーム賃貸マンション - 1ルーム賃貸マンション, 家賃データ, 50件 (仮想データ) - rent: 月額家賃 (円) - area: 専有面積 (平米) - yrs: 築後年数 (年) - dist: 最寄駅からの徒歩距離 (m) データの読み込み rentdat &lt;- read.csv(&quot;rentdat.csv&quot;, header = T) head(rentdat) # R標準の記法 #&gt; rent area yrs dist #&gt; 1 60000 18.45 8.73 837.46 #&gt; 2 61000 19.84 13.33 520.86 #&gt; 3 74000 22.45 8.26 433.77 #&gt; 4 77000 26.81 5.94 1192.32 #&gt; 5 59000 17.62 3.85 815.17 #&gt; 6 86000 26.68 4.19 373.87 # または, パイプ (%&gt;%) を利用して, # rentdat %&gt;% head() 実行に先立ち, pairs()やcor()を使い, 変数間の従属性や, 相関係数の大きさを確認する. pairs(rentdat) cor(rentdat) #&gt; rent area yrs dist #&gt; rent 1.0000000 0.84098526 -0.16885266 -0.36727009 #&gt; area 0.8409853 1.00000000 0.05454398 -0.02291733 #&gt; yrs -0.1688527 0.05454398 1.00000000 -0.05812975 #&gt; dist -0.3672701 -0.02291733 -0.05812975 1.00000000 # パイプ (%&gt;%) を利用しても良い # rendat %&gt;% pairs() # rentdat %&gt;5 cor() 回帰実行 関数lm()を使用して最小二乗法による適合を行う. 実行結果はsummary()で確認する. res_lm &lt;- lm(rent ~ ., data = rentdat) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ ., data = rentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6732 -2379 -1016 2286 7256 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.469 3652.405 8.833 1.81e-11 *** #&gt; area 2397.144 142.744 16.793 &lt; 2e-16 *** #&gt; yrs -745.440 159.275 -4.680 2.55e-05 *** #&gt; dist -12.443 1.733 -7.180 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 3531 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 実行結果の取り出し # 回帰係数の取り出し coef(res_lm) # 関数の利用 #&gt; (Intercept) area yrs dist #&gt; 32261.46944 2397.14374 -745.44017 -12.44341 res_lm$coef # 省略形による指示可能 #&gt; (Intercept) area yrs dist #&gt; 32261.46944 2397.14374 -745.44017 -12.44341 # res_lm$coefficients # 適合値 (予測値) の取り出し fitted(res_lm) %&gt;% head() # head()により, 最初の6行のみ表示 (デフォルト) #&gt; 1 2 3 4 5 6 #&gt; 59560.22 63402.81 74522.43 77264.45 61485.70 88441.65 #res_lm$fitted # 残差の取り出し resid(res_lm) # 関数の利用 #&gt; 1 2 3 4 5 6 7 #&gt; 439.7807 -2402.8084 -522.4320 -264.4500 -2485.7017 -2441.6518 -1029.7327 #&gt; 8 9 10 11 12 13 14 #&gt; 5176.2213 -6732.0408 2427.4126 376.6794 4555.9941 -1754.1073 1534.2533 #&gt; 15 16 17 18 19 20 21 #&gt; 647.1293 -2143.6007 5146.9150 4191.1927 2626.1939 5839.5466 1751.7939 #&gt; 22 23 24 25 26 27 28 #&gt; -5750.1494 -2990.2238 -5767.8528 -2307.8140 -3235.3851 -114.3409 -3829.2694 #&gt; 29 30 31 32 33 34 35 #&gt; 715.0065 -3332.8895 7256.4052 2590.8913 3537.0448 1388.8917 6999.9397 #&gt; 36 37 38 39 40 41 42 #&gt; -3321.0903 -2191.1362 5489.4176 -3603.0246 -1025.7680 -1625.3943 -1887.0020 #&gt; 43 44 45 46 47 48 49 #&gt; -1650.4408 1863.8094 -1133.3323 -2663.5164 5523.2614 -1554.5109 -1311.7071 #&gt; 50 #&gt; -1006.4070 res_lm$resid %&gt;% head() #&gt; 1 2 3 4 5 6 #&gt; 439.7807 -2402.8084 -522.4320 -264.4500 -2485.7017 -2441.6518 # res_lm$residuals # 回帰係数の信頼区間の計算 confint(res_lm) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 24909.55913 39613.379763 #&gt; area 2109.81414 2684.473338 #&gt; yrs -1066.04436 -424.835985 #&gt; dist -15.93178 -8.955039 モデル診断 plot(res_lm$fitted.values, rentdat$rent) # モデル診断: y観測値 vs y適合値 abline(a = 0, b = 1) plot(res_lm$fitted.values, res_lm$residuals) # モデル診断: y適合値 vs 残差 abline(h = 0) par(mfrow=c(2,2)) plot(res_lm) # → resid(res_lm) 適合モデルを使った予測 内挿予測 (適合値の計算) predict(res_lm) %&gt;% head() #&gt; 1 2 3 4 5 6 #&gt; 59560.22 63402.81 74522.43 77264.45 61485.70 88441.65 外挿予測 例. 専有面積=18.8平米, 築後年数=13年, 駅距離=800m, または100mの物件の賃料は? new &lt;- data.frame(area = 18.8, dist = c(800, 100), yrs = 13) predict(res_lm, newdata = new) #&gt; 1 2 #&gt; 57682.32 66392.71 #predict.lm(res_lm, newdata = new) #res_lm$residuals # resid(res_lm) また, summary()には最小二乗推定の各種結果が格納されている. str(summary(res_lm)) #&gt; List of 11 #&gt; $ call : language lm(formula = rent ~ ., data = rentdat) #&gt; $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language rent ~ area + yrs + dist #&gt; .. ..- attr(*, &quot;variables&quot;)= language list(rent, area, yrs, dist) #&gt; .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... #&gt; .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. .. .. ..$ : chr [1:4] &quot;rent&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. .. .. ..$ : chr [1:3] &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1 #&gt; .. ..- attr(*, &quot;intercept&quot;)= int 1 #&gt; .. ..- attr(*, &quot;response&quot;)= int 1 #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; #&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(rent, area, yrs, dist) #&gt; .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; #&gt; .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;rent&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; $ residuals : Named num [1:50] 440 -2403 -522 -264 -2486 ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; $ coefficients : num [1:4, 1:4] 32261.5 2397.1 -745.4 -12.4 3652.4 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; #&gt; $ aliased : Named logi [1:4] FALSE FALSE FALSE FALSE #&gt; ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; $ sigma : num 3531 #&gt; $ df : int [1:3] 4 46 4 #&gt; $ r.squared : num 0.884 #&gt; $ adj.r.squared: num 0.876 #&gt; $ fstatistic : Named num [1:3] 117 3 46 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; #&gt; $ cov.unscaled : num [1:4, 1:4] 1.069834 -0.035212 -0.017107 -0.000183 -0.035212 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;area&quot; &quot;yrs&quot; &quot;dist&quot; #&gt; - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; summary(res_lm)$r.squared # R2 #&gt; [1] 0.8837688 # summary(res_lm)[&quot;r.squared&quot;] # 別の指定方法 summary(res_lm)$adj.r.squared # 補正R2 #&gt; [1] 0.8761885 summary(res_lm)$coef #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.46944 3652.405183 8.832938 1.805979e-11 #&gt; area 2397.14374 142.744411 16.793258 3.002396e-21 #&gt; yrs -745.44017 159.275119 -4.680205 2.548844e-05 #&gt; dist -12.44341 1.733012 -7.180222 4.893016e-09 標準化 (偏) 回帰係数 あらかじめ変数を標準化しておいてからlm()を実行すると, 標準化偏回帰係数が得られる. # 標準(化)回帰係数 srentdat &lt;- scale(rentdat) # scale()の返り値はリスト型 → データフレームへ変換 srentdat &lt;- data.frame(srentdat) sres_lm &lt;- lm(rent ~ area + yrs + dist, data = srentdat) summary(sres_lm) #&gt; #&gt; Call: #&gt; lm(formula = rent ~ area + yrs + dist, data = srentdat) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.6708 -0.2371 -0.1013 0.2278 0.7231 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -5.959e-16 4.976e-02 0.00 1 #&gt; area 8.456e-01 5.035e-02 16.79 &lt; 2e-16 *** #&gt; yrs -2.360e-01 5.042e-02 -4.68 2.55e-05 *** #&gt; dist -3.616e-01 5.036e-02 -7.18 4.89e-09 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3519 on 46 degrees of freedom #&gt; Multiple R-squared: 0.8838, Adjusted R-squared: 0.8762 #&gt; F-statistic: 116.6 on 3 and 46 DF, p-value: &lt; 2.2e-16 # 偏回帰係数 vs 標準(化)偏回帰係数 summary(res_lm)[&quot;coefficients&quot;] #&gt; $coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 32261.46944 3652.405183 8.832938 1.805979e-11 #&gt; area 2397.14374 142.744411 16.793258 3.002396e-21 #&gt; yrs -745.44017 159.275119 -4.680205 2.548844e-05 #&gt; dist -12.44341 1.733012 -7.180222 4.893016e-09 summary(sres_lm)[&quot;coefficients&quot;] #&gt; $coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -5.958724e-16 0.04976173 -1.197451e-14 1.000000e+00 #&gt; area 8.455702e-01 0.05035176 1.679326e+01 3.002396e-21 #&gt; yrs -2.359937e-01 0.05042380 -4.680205e+00 2.548844e-05 #&gt; dist -3.616101e-01 0.05036197 -7.180222e+00 4.893016e-09 # # 確認 y_sd &lt;- sd(rentdat$rent) x_sd &lt;- apply(rentdat[, -1], 2, sd) res_lm$coef[ -1] * x_sd / y_sd #&gt; area yrs dist #&gt; 0.8455702 -0.2359937 -0.3616101 3.2 変数の選択 データ2: ボストン市内住宅物件価格データ Boston housingデータセットは, Harrison and Rubinfeld (78) で分析に使用された. これは, ボストン地域の住宅に関するデータセットで, もともとは, アメリカ合衆国国勢調査局 (U.S. Census Service) によって収集されたものに基づいている. 今日までに, 統計学・機械学習の教育や研究で広く利用されている. データセットの各行 (レコード) は, ボストン標準大都市統計地域 (Boston Standard Metropolitan Statistical Area, SMSA) 内の1つの国勢調査区（census tract）に対応する. 各国勢調査区は複数の住宅を含む地域単位であるため, 各行は個別の住宅1軒を表すものではない. Harrison, D., &amp; Rubinfeld, D. L. (1978). Hedonic prices and the demand for clean air. Journal of Environmental Economics and Management, 5(1), 81–102. (7/7/25) 変数disの訳がミスリーディングだったため, 訂正いたします. (旧)雇用センター → (新)雇用中心地 (employment centers) - Boston Housingデータセット - crim: 地域の一人当たり犯罪率 - zn: 25,000平方フィート以上の住宅用地の割合 - indus: 地域の非小売業の土地の割合 - chas: チャールズ川のダミー変数 (1: 川沿い, 0: それ以外) - nox: 窒素酸化物濃度（1000万ppm） - rm: 住宅の平均部屋数 - age: 1940年以前に建設された持ち家の割合 - dis: ボストンの5つの雇用中心地 (employment centers) までの距離の加重平均 - rad: 放射状高速道路へのアクセス指数 - tax: 10,000米ドル当たりの固定資産税率 - ptratio: 地域の生徒数・教師数比率 - b: 人種的指標, 1000(B - 0.63)^2, (Bは地域の黒人の割合) - lstat: 低所得者層の割合 - medv: 持ち家住宅の中央値（1000ドル単位） - 506件 x 14変数 (オリジナル版) - 本セクションで使用するバージョンの出所: http://lib.stat.cmu.edu/datasets/boston 注意: Harrison and Rubinfeld (78) の原文には, “employment centers&quot;に関する明確な説明はないものの, `dis`の定義として, “Weighted distances to five employment centers in the Boston region. According to traditional theories of urban land rent gradients, housing values should be higher near employment renters. DIS is entered in logarithm form; the expected sign is negative.”(p.97) とある. また, 不動産市場の文献における“accessibility to employment centers&quot;等の用法を調べる限りにおいて, “employment centers&quot;をいわゆる“職業安定所&quot;と解釈するのは誤りで, むしろ, (ビジネスが集まり労働人口の多い) &quot;雇用の中心地&quot;, “雇用集積地&quot;等と解釈するのが適切と考えられる. # library(MASS) # Bostonデータセット # housing &lt;- Boston # 変数bではなくblack housing &lt;- read.csv(&quot;boston_housing.csv&quot;, header = T) housing %&gt;% head() #&gt; crim zn indus chas nox rm age dis rad tax ptratio b lstat #&gt; 1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 #&gt; 2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 #&gt; 3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 #&gt; 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 #&gt; 5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 #&gt; 6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 #&gt; medv #&gt; 1 24.0 #&gt; 2 21.6 #&gt; 3 34.7 #&gt; 4 33.4 #&gt; 5 36.2 #&gt; 6 28.7 chasはダミー変数 (0/1) のため, 一旦除去して変数間の相関等を調べる. ライブラリcorrplotの関数corrplot()を使うと, 相関係数のヒートマップを作成することができる. # 散布図行列 pairs(housing[, -4]) # chas (バイナリ) を除去 round(cor(housing[, -4]), 2) # chas(バイナリ)を除去 #&gt; crim zn indus nox rm age dis rad tax ptratio b #&gt; crim 1.00 -0.20 0.41 0.42 -0.22 0.35 -0.38 0.63 0.58 0.29 -0.39 #&gt; zn -0.20 1.00 -0.53 -0.52 0.31 -0.57 0.66 -0.31 -0.31 -0.39 0.18 #&gt; indus 0.41 -0.53 1.00 0.76 -0.39 0.64 -0.71 0.60 0.72 0.38 -0.36 #&gt; nox 0.42 -0.52 0.76 1.00 -0.30 0.73 -0.77 0.61 0.67 0.19 -0.38 #&gt; rm -0.22 0.31 -0.39 -0.30 1.00 -0.24 0.21 -0.21 -0.29 -0.36 0.13 #&gt; age 0.35 -0.57 0.64 0.73 -0.24 1.00 -0.75 0.46 0.51 0.26 -0.27 #&gt; dis -0.38 0.66 -0.71 -0.77 0.21 -0.75 1.00 -0.49 -0.53 -0.23 0.29 #&gt; rad 0.63 -0.31 0.60 0.61 -0.21 0.46 -0.49 1.00 0.91 0.46 -0.44 #&gt; tax 0.58 -0.31 0.72 0.67 -0.29 0.51 -0.53 0.91 1.00 0.46 -0.44 #&gt; ptratio 0.29 -0.39 0.38 0.19 -0.36 0.26 -0.23 0.46 0.46 1.00 -0.18 #&gt; b -0.39 0.18 -0.36 -0.38 0.13 -0.27 0.29 -0.44 -0.44 -0.18 1.00 #&gt; lstat 0.46 -0.41 0.60 0.59 -0.61 0.60 -0.50 0.49 0.54 0.37 -0.37 #&gt; medv -0.39 0.36 -0.48 -0.43 0.70 -0.38 0.25 -0.38 -0.47 -0.51 0.33 #&gt; lstat medv #&gt; crim 0.46 -0.39 #&gt; zn -0.41 0.36 #&gt; indus 0.60 -0.48 #&gt; nox 0.59 -0.43 #&gt; rm -0.61 0.70 #&gt; age 0.60 -0.38 #&gt; dis -0.50 0.25 #&gt; rad 0.49 -0.38 #&gt; tax 0.54 -0.47 #&gt; ptratio 0.37 -0.51 #&gt; b -0.37 0.33 #&gt; lstat 1.00 -0.74 #&gt; medv -0.74 1.00 # pairs(housing) # round(cor(housing), 2) library(corrplot) corrplot(cor(housing[, -4])) # corrplot # corrplot(cor(housing)) # corrplot 4変数に絞り込み res_lm1 = lm(medv ~ crim + rm + tax + lstat, data = housing) summary(res_lm1) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.383 -3.497 -1.149 1.825 30.716 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.414928 3.178364 -0.445 0.6564 #&gt; crim -0.061579 0.035562 -1.732 0.0840 . #&gt; rm 5.248721 0.439664 11.938 &lt;2e-16 *** #&gt; tax -0.005018 0.001922 -2.611 0.0093 ** #&gt; lstat -0.534835 0.050258 -10.642 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.458 on 501 degrees of freedom #&gt; Multiple R-squared: 0.6506, Adjusted R-squared: 0.6478 #&gt; F-statistic: 233.2 on 4 and 501 DF, p-value: &lt; 2.2e-16 anova(res_lm1) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 216.206 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 560.915 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 42.542 1.693e-10 *** #&gt; lstat 1 3373.6 3373.6 113.247 &lt; 2.2e-16 *** #&gt; Residuals 501 14924.8 29.8 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # update関数でモデル更新: 変数ptratio追加 res_lm2 &lt;- update(res_lm1, . ~ . + ptratio) summary(res_lm2) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.3602 -3.1111 -0.9237 1.6569 30.4116 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 16.7488084 4.0001180 4.187 3.34e-05 *** #&gt; crim -0.0593795 0.0339830 -1.747 0.0812 . #&gt; rm 4.6349234 0.4292367 10.798 &lt; 2e-16 *** #&gt; tax -0.0008196 0.0019328 -0.424 0.6717 #&gt; lstat -0.5280046 0.0480346 -10.992 &lt; 2e-16 *** #&gt; ptratio -0.8731668 0.1251429 -6.977 9.59e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.215 on 500 degrees of freedom #&gt; Multiple R-squared: 0.6816, Adjusted R-squared: 0.6784 #&gt; F-statistic: 214.1 on 5 and 500 DF, p-value: &lt; 2.2e-16 anova(res_lm2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.784 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 614.301 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.591 2.540e-11 *** #&gt; lstat 1 3373.6 3373.6 124.026 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.684 9.589e-12 *** #&gt; Residuals 500 13600.6 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数zn追加 res_lm3 &lt;- update(res_lm2, . ~ . + zn) summary(res_lm3) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + zn, data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4790 -3.1374 -0.8754 1.6871 30.3185 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.3073953 4.0780517 4.244 2.62e-05 *** #&gt; crim -0.0584021 0.0340274 -1.716 0.0867 . #&gt; rm 4.6460026 0.4297290 10.811 &lt; 2e-16 *** #&gt; tax -0.0008832 0.0019358 -0.456 0.6484 #&gt; lstat -0.5354553 0.0491813 -10.887 &lt; 2e-16 *** #&gt; ptratio -0.8958719 0.1291910 -6.934 1.27e-11 *** #&gt; zn -0.0081367 0.0114124 -0.713 0.4762 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.218 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6819, Adjusted R-squared: 0.6781 #&gt; F-statistic: 178.3 on 6 and 499 DF, p-value: &lt; 2.2e-16 anova(res_lm3) #&gt; Analysis of Variance Table #&gt; #&gt; Response: medv #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; crim 1 6440.8 6440.8 236.5506 &lt; 2.2e-16 *** #&gt; rm 1 16709.7 16709.7 613.6973 &lt; 2.2e-16 *** #&gt; tax 1 1267.3 1267.3 46.5455 2.601e-11 *** #&gt; lstat 1 3373.6 3373.6 123.9040 &lt; 2.2e-16 *** #&gt; ptratio 1 1324.2 1324.2 48.6356 9.826e-12 *** #&gt; zn 1 13.8 13.8 0.5083 0.4762 #&gt; Residuals 499 13586.7 27.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変数nox追加, zn除去 res_lm4 &lt;- update(res_lm3, . ~ . + nox - zn) summary(res_lm4) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + lstat + ptratio + nox, #&gt; data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.2389 -3.1372 -0.9454 1.6680 30.4687 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 17.2649269 4.2731659 4.040 6.18e-05 *** #&gt; crim -0.0596990 0.0340256 -1.755 0.080 . #&gt; rm 4.6382386 0.4297223 10.794 &lt; 2e-16 *** #&gt; tax -0.0004089 0.0022705 -0.180 0.857 #&gt; lstat -0.5216846 0.0514382 -10.142 &lt; 2e-16 *** #&gt; ptratio -0.8844707 0.1294545 -6.832 2.44e-11 *** #&gt; nox -1.0363053 2.9989281 -0.346 0.730 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.22 on 499 degrees of freedom #&gt; Multiple R-squared: 0.6817, Adjusted R-squared: 0.6779 #&gt; F-statistic: 178.1 on 6 and 499 DF, p-value: &lt; 2.2e-16 目的変数medvと説明変数lstatには, 明らかに非線形な関係性が見られる. そこで, lstatに非線形変換を施すことで, 適合度が改善できる可能性がある. pairs(housing[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]) round(cor(housing[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;tax&quot;, &quot;lstat&quot;)]), 2) #&gt; medv crim rm tax lstat #&gt; medv 1.00 -0.39 0.70 -0.47 -0.74 #&gt; crim -0.39 1.00 -0.22 0.58 0.46 #&gt; rm 0.70 -0.22 1.00 -0.29 -0.61 #&gt; tax -0.47 0.58 -0.29 1.00 0.54 #&gt; lstat -0.74 0.46 -0.61 0.54 1.00 # 変数lstatの逆数を新変数invlstatとして定義し, モデルに追加 data2 &lt;- data.frame(housing, invlstat = 1 / housing$lstat) res_lm5 &lt;- lm(medv ~ crim + rm + tax + ptratio + invlstat, data = data2) plot(housing$medv, 1 / housing$lstat) summary(res_lm5) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + tax + ptratio + invlstat, data = data2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.9062 -2.6032 -0.5276 2.1041 31.2592 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.665018 3.295520 2.022 0.0437 * #&gt; crim -0.119564 0.030121 -3.969 8.26e-05 *** #&gt; rm 3.609393 0.394880 9.140 &lt; 2e-16 *** #&gt; tax -0.002272 0.001693 -1.342 0.1802 #&gt; ptratio -0.665188 0.114156 -5.827 1.01e-08 *** #&gt; invlstat 60.465938 3.762069 16.073 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.719 on 500 degrees of freedom #&gt; Multiple R-squared: 0.7393, Adjusted R-squared: 0.7367 #&gt; F-statistic: 283.6 on 5 and 500 DF, p-value: &lt; 2.2e-16 crimもmedvと非線形な関係があるため, これを適当に非線形変換することで更に改善できる余地がある (各自で試して欲しい). 標準的なモデル選択規準であるAICやBICは, 関数AIC(), BIC()によって計算することができる. # AIC, BICの計算 AIC(res_lm5, res_lm2) #&gt; df AIC #&gt; res_lm5 7 3014.149 #&gt; res_lm2 7 3115.379 BIC(res_lm5, res_lm2) #&gt; df BIC #&gt; res_lm5 7 3043.735 #&gt; res_lm2 7 3144.965 AIC, BIC双方とも, res_lm5はres_lm2より望ましいことを示している. 関数anova()を使って, 分散分析によって (包含関係になる) モデル間の比較をすることができる. # 追加 (除去) した変数群の有意性 (例) anova(res_lm1, res_lm3, test = &quot;F&quot;) # F検定 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat #&gt; Model 2: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 501 14925 #&gt; 2 499 13587 2 1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(res_lm3, res_lm1, test = &quot;F&quot;) # 実質的に同一 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: medv ~ crim + rm + tax + lstat + ptratio + zn #&gt; Model 2: medv ~ crim + rm + tax + lstat #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 499 13587 #&gt; 2 501 14925 -2 -1338.1 24.572 6.636e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # anova(res_lm1, res_lm2, test = &quot;LRT&quot;) # 尤度比検定 ステップワイズ法による変数選択 ステップワイズ法は, 関数lm()の実行結果オブジェクトを, 関数step()に入力として与えることで実行することができる. # step(); AICによって決定 # scope: モデルサーチの範囲 (追加や削除を検討するべき変数を指定) # scope指定ない場合: # - directionのデフォルトは, 変数減少法 (後方削除) # - モデルサーチ上限 (upper) は, 初期モデル # scope指定ある場合: # - directionのデフォルトは, 変数増減法 # - scopeがリストでなく, 単一式で与えらている場合, upperモデルと解釈 (lowerは欠損) res_lm_all = lm(medv ~ ., data = housing) # → 13変数 res_lm_all_2 = lm(medv ~ 1, data = housing) # → y切片のみ (変数なし) # step(res_lm5) # 変数減少法 (scopeない場合のデフォルト) #&gt; Start: AIC=1576.18 #&gt; medv ~ crim + rm + tax + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 40.1 11175 1576.0 #&gt; &lt;none&gt; 11134 1576.2 #&gt; - crim 1 350.9 11485 1589.9 #&gt; - ptratio 1 756.1 11891 1607.4 #&gt; - rm 1 1860.5 12995 1652.4 #&gt; - invlstat 1 5752.7 16887 1784.9 #&gt; #&gt; Step: AIC=1576 #&gt; medv ~ crim + rm + ptratio + invlstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11175 1576.0 #&gt; - crim 1 643.8 11818 1602.3 #&gt; - ptratio 1 951.0 12126 1615.3 #&gt; - rm 1 1847.9 13023 1651.4 #&gt; - invlstat 1 6153.6 17328 1796.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + ptratio + invlstat, data = data2) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm ptratio invlstat #&gt; 6.6395 -0.1399 3.5960 -0.7113 61.4172 #step(res_lm5, direction = &quot;forward&quot;) # 変数増加法 (上限は初期モデル) #step(res_lm5, direction = &quot;both&quot;) # 変数増減法 (上限は初期モデル) # 採用する変数の上限・下限の指定 step(res_lm_all, scope = list(lower = ~ crim + rm)) # 下限のモデルを指定. 変数増減法 #&gt; Start: AIC=1589.64 #&gt; medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + #&gt; tax + ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - age 1 0.06 11079 1587.7 #&gt; - indus 1 2.52 11081 1587.8 #&gt; &lt;none&gt; 11079 1589.6 #&gt; - chas 1 218.97 11298 1597.5 #&gt; - tax 1 242.26 11321 1598.6 #&gt; - zn 1 257.49 11336 1599.3 #&gt; - b 1 270.63 11349 1599.8 #&gt; - rad 1 479.15 11558 1609.1 #&gt; - nox 1 487.16 11566 1609.4 #&gt; - ptratio 1 1194.23 12273 1639.4 #&gt; - dis 1 1232.41 12311 1641.0 #&gt; - lstat 1 2410.84 13490 1687.3 #&gt; #&gt; Step: AIC=1587.65 #&gt; medv ~ crim + zn + indus + chas + nox + rm + dis + rad + tax + #&gt; ptratio + b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - indus 1 2.52 11081 1585.8 #&gt; &lt;none&gt; 11079 1587.7 #&gt; - chas 1 219.91 11299 1595.6 #&gt; - tax 1 242.24 11321 1596.6 #&gt; - zn 1 260.32 11339 1597.4 #&gt; - b 1 272.26 11351 1597.9 #&gt; - rad 1 481.09 11560 1607.2 #&gt; - nox 1 520.87 11600 1608.9 #&gt; - ptratio 1 1200.23 12279 1637.7 #&gt; - dis 1 1352.26 12431 1643.9 #&gt; - lstat 1 2718.88 13798 1696.7 #&gt; #&gt; Step: AIC=1585.76 #&gt; medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + #&gt; b + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 11081 1585.8 #&gt; - chas 1 227.21 11309 1594.0 #&gt; - zn 1 257.82 11339 1595.4 #&gt; - b 1 270.82 11352 1596.0 #&gt; - tax 1 273.62 11355 1596.1 #&gt; - rad 1 500.92 11582 1606.1 #&gt; - nox 1 541.91 11623 1607.9 #&gt; - ptratio 1 1206.45 12288 1636.0 #&gt; - dis 1 1448.94 12530 1645.9 #&gt; - lstat 1 2723.48 13805 1695.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + #&gt; tax + ptratio + b + lstat, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim zn chas nox rm #&gt; 36.341145 -0.108413 0.045845 2.718716 -17.376023 3.801579 #&gt; dis rad tax ptratio b lstat #&gt; -1.492711 0.299608 -0.011778 -0.946525 0.009291 -0.522553 step(res_lm_all_2, scope = list(upper = ~ crim + rm)) # 上限のモデルを指定. 変数増減法 #&gt; Start: AIC=2246.51 #&gt; medv ~ 1 #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + rm 1 20654.4 22062 1914.2 #&gt; + crim 1 6440.8 36276 2165.8 #&gt; &lt;none&gt; 42716 2246.5 #&gt; #&gt; Step: AIC=1914.19 #&gt; medv ~ rm #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + crim 1 2496.1 19566 1855.4 #&gt; &lt;none&gt; 22062 1914.2 #&gt; - rm 1 20654.4 42716 2246.5 #&gt; #&gt; Step: AIC=1855.43 #&gt; medv ~ rm + crim #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 19566 1855.4 #&gt; - crim 1 2496.1 22062 1914.2 #&gt; - rm 1 16709.7 36276 2165.8 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ rm + crim, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) rm crim #&gt; -29.2447 8.3911 -0.2649 step(res_lm1, scope = list(upper = ~ crim + rm + tax + lstat + ptratio + b, lower = ~ crim + rm)) #&gt; Start: AIC=1722.43 #&gt; medv ~ crim + rm + tax + lstat #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + ptratio 1 1324.2 13601 1677.4 #&gt; + b 1 255.6 14669 1715.7 #&gt; &lt;none&gt; 14925 1722.4 #&gt; - tax 1 203.1 15128 1727.3 #&gt; - lstat 1 3373.6 18298 1823.5 #&gt; #&gt; Step: AIC=1677.41 #&gt; medv ~ crim + rm + tax + lstat + ptratio #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; + b 1 306.0 13295 1667.9 #&gt; - tax 1 4.9 13606 1675.6 #&gt; &lt;none&gt; 13601 1677.4 #&gt; - ptratio 1 1324.2 14925 1722.4 #&gt; - lstat 1 3286.7 16887 1784.9 #&gt; #&gt; Step: AIC=1667.9 #&gt; medv ~ crim + rm + tax + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - tax 1 3.06 13298 1666.0 #&gt; &lt;none&gt; 13295 1667.9 #&gt; - b 1 306.02 13601 1677.4 #&gt; - ptratio 1 1374.66 14669 1715.7 #&gt; - lstat 1 2849.76 16144 1764.2 #&gt; #&gt; Step: AIC=1666.01 #&gt; medv ~ crim + rm + lstat + ptratio + b #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 13298 1666.0 #&gt; + tax 1 3.06 13295 1667.9 #&gt; - b 1 307.85 13606 1675.6 #&gt; - ptratio 1 1478.71 14776 1717.4 #&gt; - lstat 1 3001.77 16299 1767.0 #&gt; #&gt; Call: #&gt; lm(formula = medv ~ crim + rm + lstat + ptratio + b, data = housing) #&gt; #&gt; Coefficients: #&gt; (Intercept) crim rm lstat ptratio b #&gt; 11.615006 -0.038921 4.788176 -0.495139 -0.877249 0.009593 # 上限・下限を同時に指定. 変数増減法 # ----------------------------------------# 多重共線性のチェックについて VIF (Variance Inflation Factor) によって, 説明変数間の多重共線性 (マルチコ) の 有無を確認することができる. VIFによる多重共線性への対応に関する慣用ルールとして, 5以上の値を持つ説明変数は要注意, 10以上の変数は除去するのが良いとされている. # VIF # install.packages(&quot;car&quot;) # or RStudio, Tools → Install packages, library(car) # &quot;Companion to Applied Regression&quot; package vif(res_lm_all) # 全13説明変数についてVIFを計算 #&gt; crim zn indus chas nox rm age dis #&gt; 1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 #&gt; rad tax ptratio b lstat #&gt; 7.484496 9.008554 1.799084 1.348521 2.941491 → 変数taxがVIF値が最大 (9.01) と10に近いことから, これを除いてVIF値を再計算してみる. vif(update(res_lm_all, . ~ . - tax)) # VIF値最大のtaxを除いてVIF値を再計算 #&gt; crim zn indus chas nox rm age dis #&gt; 1.791940 2.184240 3.226015 1.058220 4.369271 1.923075 3.098044 3.954446 #&gt; rad ptratio b lstat #&gt; 2.837494 1.788839 1.347564 2.940800 → taxの次に大きかったradのVIF値が大きく減少. 上でみたとおり, もともと, taxとradは相関が高かった (0.91). 線形回帰分析におけるマルチコを考慮した変数選択法としては, 例えば, Kariya, Kurata and Hayashi (2024) の提案した”Empirically Effective Modelling Methodology (EEM-M)“がある. 参考文献 Kariya, Kurata and Hayashi (2024). “A Modelling Framework for Regression with Collinearity”, Journal of Statistical Planning and Inference, 228 (1), Pages 95-115. EEM-Mアプリ 3.3 説明変数に質的変数を含む回帰 データセット#3: 高速道路事故データ - Hoffstedt’s Highway accident data - adt：1日の平均交通量（単位：千台) - trks：総交通量に占めるトラック交通量の割合 - lane：交通の総車線数 - acpt：1マイルあたりのアクセスポイント数 - sigs: 1マイルあたりの信号付きインターチェンジの数 - itg：1マイルあたりの高速道路型インターチェンジの数 - slim：1973年の制限速度 - lwid: 車線幅（フィート単位） - shld: 車道の外側路肩の幅（フィート単位) - htype: 道路の種類または道路の財源を示す指標: &quot;mc&quot;: メジャーコレクター (major collector), &quot;fai&quot;: 州間 (interstate) 高速道路, &quot;pa&quot;: 地域・都市間主要幹線 (principal arterial) 道路, &quot;ma&quot;; 地域・都市内主要幹線 (major arterial) 道路 - rate: 1973年の事故発生率（百万車両マイル当たり） - 注) htypeは4-水準因子 - 参考文献: Weisberg (2014), Applied Linear Regression, 4th Ed., Wiley. library(alr4) data(Highway) str(Highway) #&gt; &#39;data.frame&#39;: 39 obs. of 12 variables: #&gt; $ adt : int 69 73 49 61 28 30 46 25 43 23 ... #&gt; $ trks : int 8 8 10 13 12 6 8 9 12 7 ... #&gt; $ lane : int 8 4 4 6 4 4 4 4 4 4 ... #&gt; $ acpt : num 4.6 4.4 4.7 3.8 2.2 24.8 11 18.5 7.5 8.2 ... #&gt; $ sigs : num 0 0 0 0 0 1.84 0.7 0.38 1.39 1.21 ... #&gt; $ itg : num 1.2 1.43 1.54 0.94 0.65 0.34 0.47 0.38 0.95 0.12 ... #&gt; $ slim : int 55 60 60 65 70 55 55 55 50 50 ... #&gt; $ len : num 4.99 16.11 9.75 10.65 20.01 ... #&gt; $ lwid : int 12 12 12 12 12 12 12 12 12 12 ... #&gt; $ shld : int 10 10 10 10 10 10 8 10 4 5 ... #&gt; $ htype: Factor w/ 4 levels &quot;mc&quot;,&quot;fai&quot;,&quot;pa&quot;,..: 2 2 2 2 2 3 3 3 3 3 ... #&gt; $ rate : num 4.58 2.86 3.02 2.29 1.61 6.87 3.85 6.12 3.29 5.88 ... Highway %&gt;% head() #&gt; adt trks lane acpt sigs itg slim len lwid shld htype rate #&gt; 1 69 8 8 4.6 0.00 1.20 55 4.99 12 10 fai 4.58 #&gt; 2 73 8 4 4.4 0.00 1.43 60 16.11 12 10 fai 2.86 #&gt; 3 49 10 4 4.7 0.00 1.54 60 9.75 12 10 fai 3.02 #&gt; 4 61 13 6 3.8 0.00 0.94 65 10.65 12 10 fai 2.29 #&gt; 5 28 12 4 2.2 0.00 0.65 70 20.01 12 10 fai 1.61 #&gt; 6 30 6 4 24.8 1.84 0.34 55 5.97 12 10 pa 6.87 ライブラリcorrplotの関数corrplot.mixed()を使うと, 相関係数のヒートマップと相関係数の値を同時に表示するプロットを作成することができる. library(corrplot) cor_hw &lt;- cor(cbind(Highway$rate, Highway[, -(11:12)])) # htypeを除去 corrplot.mixed(cor_hw) # OLS回帰 res_lm &lt;- lm(rate ~ ., data = Highway) summary(res_lm) #&gt; #&gt; Call: #&gt; lm(formula = rate ~ ., data = Highway) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.99564 -0.62039 -0.05676 0.61741 2.54998 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 13.658212 6.872719 1.987 0.0579 . #&gt; adt -0.004038 0.033945 -0.119 0.9063 #&gt; trks -0.100150 0.114726 -0.873 0.3910 #&gt; lane 0.026675 0.283834 0.094 0.9259 #&gt; acpt 0.066588 0.042569 1.564 0.1303 #&gt; sigs 0.713644 0.525213 1.359 0.1864 #&gt; itg -0.475478 1.282742 -0.371 0.7140 #&gt; slim -0.123778 0.081683 -1.515 0.1422 #&gt; len -0.064751 0.033369 -1.940 0.0637 . #&gt; lwid -0.133813 0.597917 -0.224 0.8247 #&gt; shld 0.014113 0.162174 0.087 0.9313 #&gt; htypefai 0.543592 1.728270 0.315 0.7557 #&gt; htypepa -1.009777 1.105612 -0.913 0.3698 #&gt; htypema -0.548025 0.975623 -0.562 0.5793 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.198 on 25 degrees of freedom #&gt; Multiple R-squared: 0.7605, Adjusted R-squared: 0.636 #&gt; F-statistic: 6.107 on 13 and 25 DF, p-value: 5.733e-05 # 多重共線性のチェック (VIF) # install.packages(&quot;car&quot;) # or RStudio, Tools → Install packages, library(car) # &quot;Companion to Applied Regression&quot; package vif(res_lm) #&gt; GVIF Df GVIF^(1/(2*Df)) #&gt; adt 10.563911 1 3.250217 #&gt; trks 1.931277 1 1.389704 #&gt; lane 3.947949 1 1.986945 #&gt; acpt 4.164627 1 2.040742 #&gt; sigs 2.929007 1 1.711434 #&gt; itg 7.362521 1 2.713397 #&gt; slim 6.041264 1 2.457898 #&gt; len 1.706597 1 1.306368 #&gt; lwid 1.966483 1 1.402313 #&gt; shld 6.417952 1 2.533368 #&gt; htype 28.984452 3 1.752646 データセットHighwayは質的変数 (htype) を含んでいることから, VIFを拡張したGVIF (Generalized VIF) を算出している. GVIFを (変数間で比較できるように) 自由度調整した値GVIF\\(^{1/(2D_f)}\\)は, 量的変数の場合オリジナルのVIFの平方根を取ったものに対応している. そこで, この自由度調整済GVIF\\(^{1/(2D_f)}\\)は, \\(\\sqrt{5} \\approx 2.24\\)越えで要注意, \\(\\sqrt{10} \\approx 3.16\\)越えで除去を検討というのが一つの目安となる ただ, そもそもVIFの5, 10が慣用的な閾値に過ぎないことから, 2.24や3.16といった小数の値まで厳密に評価する合理性は乏しい. そこで, (使いやすくかつ覚えやすくするため) 数字を丸めて, 例えば, 「2までなら安全, 2〜5で要注意, 5越えたら除去を検討する」 等がより実践的である. → adtは要注意だが, 以下では除かないでステップワイズを実行してみる. # ステップワイズ回帰 res_step &lt;- step(res_lm, trace = 0) # 実行プロセスの非表示 summary(res_step) #&gt; #&gt; Call: #&gt; lm(formula = rate ~ acpt + sigs + slim + len, data = Highway) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.7505 -0.8659 0.1051 0.6618 2.5116 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.81443 2.60435 3.385 0.00181 ** #&gt; acpt 0.08940 0.02818 3.173 0.00319 ** #&gt; sigs 0.48538 0.34164 1.421 0.16450 #&gt; slim -0.09599 0.04255 -2.256 0.03064 * #&gt; len -0.06856 0.02524 -2.717 0.01030 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 1.116 on 34 degrees of freedom #&gt; Multiple R-squared: 0.7176, Adjusted R-squared: 0.6843 #&gt; F-statistic: 21.6 on 4 and 34 DF, p-value: 6.112e-09 ## 除かれた変数群の有意性 # anova(res_step, res_lm) # # anova(res_lm, res_step) (自主課題) ステップワイズで得られたモデルの結果を解釈してみよう. 3.4 説明変数に質的変数を含む回帰 (2) 関数lm()の結果を関数anova()に入力することで, 分散分析表を作成する. 関数anova()は各説明変数 (量的変数, 質的変数どちらについても) ごとに変動性を分割して \\(F\\) 値および \\(p\\) 値を計算する詳細な分散分析表 (ANOVA table) を作成する. また, 二つ以上の包含関係 (ネスト) のある回帰モデルの適合結果オブジェクトを同時に引数として与えることで, 追加 (あるいは除去される) 変数群の持つ有意性を一括して調べることが出来る. データセット#5: 収入データ (仮想) - income.csv - 月収 (万円) - キャリア年数 (年) - 能力試験 (点) - 業種 (A/B) dat1 &lt;- read.csv(&quot;income.csv&quot;) # キャリア年数を説明変数とする単回帰 lm1_mod0 &lt;- lm(月収 ~ キャリア年数, dat = dat1) summary(lm1_mod0) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -16.8581 -3.4759 -0.7415 4.5299 13.6303 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.8952 1.8298 10.87 &lt;2e-16 *** #&gt; キャリア年数 1.9703 0.1362 14.46 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.495 on 98 degrees of freedom #&gt; Multiple R-squared: 0.681, Adjusted R-squared: 0.6778 #&gt; F-statistic: 209.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 anova(lm1_mod0) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 209.23 &lt; 2.2e-16 *** #&gt; Residuals 98 2958.6 30.2 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 業種 (質的変数) を説明変数に追加 # 交互作用項なしモデル # lm1_mod1 &lt;- lm(月収 ~ キャリア年数 + 業種, dat = dat1) lm1_mod1 &lt;- update(lm1_mod0, . ~ . + 業種) summary(lm1_mod1) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.4436 -3.5628 -0.6921 3.5165 12.6536 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 20.7222 1.7727 11.690 &lt; 2e-16 *** #&gt; キャリア年数 1.9900 0.1306 15.233 &lt; 2e-16 *** #&gt; 業種B -3.5982 1.1499 -3.129 0.00232 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.264 on 97 degrees of freedom #&gt; Multiple R-squared: 0.7103, Adjusted R-squared: 0.7043 #&gt; F-statistic: 118.9 on 2 and 97 DF, p-value: &lt; 2.2e-16 anova(lm1_mod1) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 227.9980 &lt; 2.2e-16 *** #&gt; 業種 1 271.2 271.2 9.7906 0.002317 ** #&gt; Residuals 97 2687.4 27.7 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm1_mod1)の出力結果から, この回帰分析のベースラインは 業種Aであることが分かる. すなわち, ２つ目の回帰係数業種Bは, 業種Bに属することによる相対効果, すなわち, 業種BのAに対する目的変数月収の平均値の差分を表している. 具体的には, 回帰係数の切片項の値が \\(20.7222\\) が業種Aの切片となっていて, 一方, 業種Bは \\(20.7222-3.5982=17.1240\\) を切片に持つと読むことができる. 業種の違いによるキャリア年数の影響度 (傾き) の違い を調べるためには, 業種とキャリア年数の2つの項を持つモデルに両者の交互作用項を加え, その交互作用項の有意性 (\\(t\\)値に基づく\\(p\\)値) を確認すれば良い. # 交互作用項有りモデル lm1_mod1_2 &lt;- lm(月収 ~ キャリア年数 * 業種, dat = dat1) summary(lm1_mod1_2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 * 業種, data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.2175 -3.7691 -0.8021 3.4916 13.2028 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 19.3523 2.0795 9.306 4.6e-15 *** #&gt; キャリア年数 2.0980 0.1563 13.424 &lt; 2e-16 *** #&gt; 業種B 0.9930 3.8464 0.258 0.797 #&gt; キャリア年数:業種B -0.3537 0.2828 -1.250 0.214 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.248 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7149, Adjusted R-squared: 0.706 #&gt; F-statistic: 80.24 on 3 and 96 DF, p-value: &lt; 2.2e-16 anova(lm1_mod1_2) #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 229.3230 &lt; 2.2e-16 *** #&gt; 業種 1 271.2 271.2 9.8475 0.002259 ** #&gt; キャリア年数:業種 1 43.1 43.1 1.5637 0.214160 #&gt; Residuals 96 2644.3 27.5 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 回帰係数の結果表より, 交互作用項は有意な差があるとは言えない, すなわち, 業種の違いによる回帰係数の差は認められなかった (十分な証拠が得られなかった). 代替的に, 業種とキャリア年数の交互作用ありモデルとなしモデルについてlm()をそれぞれ走らせ, 二つの結果をanova()に同時に与えることで両者の変動性に有意な差があるかを調べても良い. anova(lm1_mod1, lm1_mod1_2) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 月収 ~ キャリア年数 + 業種 #&gt; Model 2: 月収 ~ キャリア年数 * 業種 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 97 2687.4 #&gt; 2 96 2644.3 1 43.073 1.5637 0.2142 先述の交互作用項の\\(t\\)値に基づいた\\(p\\)値と等価な結果が得られた. lm1_mod2 &lt;- update(lm1_mod1, . ~ . + 能力試験) summary(lm1_mod2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種 + 能力試験, #&gt; data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.8763 -3.9926 -0.6659 3.7814 12.0713 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 23.92898 3.52028 6.797 9e-10 *** #&gt; キャリア年数 1.98691 0.13060 15.214 &lt; 2e-16 *** #&gt; 業種B -3.77837 1.16192 -3.252 0.00158 ** #&gt; 能力試験 -0.06343 0.06017 -1.054 0.29444 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.261 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7136, Adjusted R-squared: 0.7046 #&gt; F-statistic: 79.72 on 3 and 96 DF, p-value: &lt; 2.2e-16 anova(lm1_mod2) # ANOVA表 #&gt; Analysis of Variance Table #&gt; #&gt; Response: 月収 #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; キャリア年数 1 6316.7 6316.7 228.2596 &lt; 2e-16 *** #&gt; 業種 1 271.2 271.2 9.8018 0.00231 ** #&gt; 能力試験 1 30.8 30.8 1.1113 0.29444 #&gt; Residuals 96 2656.6 27.7 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 次に, 関数anova()により, ネスト関係にある二つのモデルlm1_mod0, lm1_mod2を比較する. # モデル比較 # anova(lm1_mod0, lm1_mod1, lm1_mod2) anova(lm1_mod0, lm1_mod2) #&gt; Analysis of Variance Table #&gt; #&gt; Model 1: 月収 ~ キャリア年数 #&gt; Model 2: 月収 ~ キャリア年数 + 業種 + 能力試験 #&gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) #&gt; 1 98 2958.6 #&gt; 2 96 2656.6 2 302 5.4566 0.005695 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 二つの変数, 業種, 能力試験は一括して, 偶然 (サンプリング・エラー) とはみなせないような体系的な (有意な) 変動を持つことを示している. すなわち, 業種と能力試験は説明変数に加えておいた方が良いと判断される. さらに, モデル選択規準AICによっても, これらを説明変数に持つlm1_mod2の方が望ましいことを示している. AIC(lm1_mod0, lm1_mod2) #&gt; df AIC #&gt; lm1_mod0 3 628.5190 #&gt; lm1_mod2 5 621.7522 “コントラスト”の設定変更 関数options()のパラメータの一つであるコントラスト (contrasts) を変えることで, パラメータの持つ意味が, すなわち, 解釈が変わる. Rのデフォルトは, 処置対比 (“contr.treatment”). #options(&quot;contrasts&quot;) #options(&quot;contrasts&quot; = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # デフォルト #options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) 零和対比 (“contr.sum) に変更した場合について結果を, 上と比較してみよう. options(&quot;contrasts&quot; = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) lm1_mod2_2 &lt;- update(lm1_mod1, . ~ . + 能力試験) summary(lm1_mod2_2) #&gt; #&gt; Call: #&gt; lm(formula = 月収 ~ キャリア年数 + 業種 + 能力試験, #&gt; data = dat1) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.8763 -3.9926 -0.6659 3.7814 12.0713 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.03980 3.45054 6.387 6.02e-09 *** #&gt; キャリア年数 1.98691 0.13060 15.214 &lt; 2e-16 *** #&gt; 業種1 1.88919 0.58096 3.252 0.00158 ** #&gt; 能力試験 -0.06343 0.06017 -1.054 0.29444 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 5.261 on 96 degrees of freedom #&gt; Multiple R-squared: 0.7136, Adjusted R-squared: 0.7046 #&gt; F-statistic: 79.72 on 3 and 96 DF, p-value: &lt; 2.2e-16 ここでは, 回帰係数業種1が業種Aに対応, 一方, 業種2 (業種B) は省略されている. 零和条件から, 業種2の係数は\\(-1.88919\\)であることが分かる. すなわち, 切片項の値が \\(22.03980\\) であることから, 業種Aの切片の値は, \\(22.03980+1.88919=23.92899\\), 一方, Bは, \\(23.92899-1.88919=20.15061\\) となると読める. すなわち, 先のデフォルトの処置対比の場合の切片の値と一致していることが確認される. 3.5 多項式回帰 データセット#6: Wage 中部大西洋地域の男性労働者3000人の賃金その他のデータ. Inquidia Consulting（旧Open BI）のSteve Millerが手作業で集計. Current Population Surveyの2011年3月補足より. 出所: https://www.re3data.org/repository/r3d100011860 パッケージISLRに含まれるデータセットWageを用いて, 賃金を被説明変数, 年齢を説明変数とする回帰分析を行う. library(ISLR) head(Wage) #&gt; year age maritl race education region #&gt; 231655 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic #&gt; 86582 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic #&gt; 161300 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic #&gt; 155159 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic #&gt; 11443 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic #&gt; 376662 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic #&gt; jobclass health health_ins logwage wage #&gt; 231655 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 #&gt; 86582 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 #&gt; 161300 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 #&gt; 155159 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 #&gt; 11443 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 #&gt; 376662 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 # par(new=T) plot(Wage$age, Wage$wage) 2次の多項式回帰 # 2次 res_lm2 = lm(wage ~ poly(age, 2), data = Wage) # 2次の直交多項式 head(poly(Wage$age, 2)) # 2次の多項式 #&gt; 1 2 #&gt; [1,] -0.0386247992 0.055908727 #&gt; [2,] -0.0291326034 0.026298066 #&gt; [3,] 0.0040900817 -0.014506548 #&gt; [4,] 0.0009260164 -0.014831404 #&gt; [5,] 0.0120002448 -0.009815846 #&gt; [6,] 0.0183283753 -0.002073906 coef(summary(res_lm2)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 111.7036 0.730162 152.98470 0.000000e+00 #&gt; poly(age, 2)1 447.0679 39.992617 11.17876 1.878131e-28 #&gt; poly(age, 2)2 -478.3158 39.992617 -11.96010 3.077420e-32 plot(Wage$age, predict(res_lm2), pch = 2, col = &quot;blue&quot;) 3次の多項式回帰 # 3次 res_lm3 = lm(wage ~ poly(age, 3), data = Wage) # 3次の直交多項式 head(poly(Wage$age, 3)) # 3次の多項式 #&gt; 1 2 3 #&gt; [1,] -0.0386247992 0.055908727 -0.0717405794 #&gt; [2,] -0.0291326034 0.026298066 -0.0145499511 #&gt; [3,] 0.0040900817 -0.014506548 -0.0001331835 #&gt; [4,] 0.0009260164 -0.014831404 0.0045136682 #&gt; [5,] 0.0120002448 -0.009815846 -0.0111366263 #&gt; [6,] 0.0183283753 -0.002073906 -0.0166282799 coef(summary(res_lm3)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 111.7036 0.7290826 153.211181 0.000000e+00 #&gt; poly(age, 3)1 447.0679 39.9334995 11.195309 1.570802e-28 #&gt; poly(age, 3)2 -478.3158 39.9334995 -11.977808 2.511784e-32 #&gt; poly(age, 3)3 125.5217 39.9334995 3.143268 1.687063e-03 plot(Wage$age, predict(res_lm3), pch = 2, col = &quot;blue&quot;) 4次の多項式回帰 # 4次 res_lm4 = lm(wage ~ poly(age, 4), data = Wage) # 4次の直交多項式 head(poly(Wage$age, 4)) # 4次の多項式 #&gt; 1 2 3 4 #&gt; [1,] -0.0386247992 0.055908727 -0.0717405794 0.086729854 #&gt; [2,] -0.0291326034 0.026298066 -0.0145499511 -0.002599280 #&gt; [3,] 0.0040900817 -0.014506548 -0.0001331835 0.014480093 #&gt; [4,] 0.0009260164 -0.014831404 0.0045136682 0.012657507 #&gt; [5,] 0.0120002448 -0.009815846 -0.0111366263 0.010211456 #&gt; [6,] 0.0183283753 -0.002073906 -0.0166282799 -0.001314381 coef(summary(res_lm4)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 111.70361 0.7287409 153.283015 0.000000e+00 #&gt; poly(age, 4)1 447.06785 39.9147851 11.200558 1.484604e-28 #&gt; poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32 #&gt; poly(age, 4)3 125.52169 39.9147851 3.144742 1.678622e-03 #&gt; poly(age, 4)4 -77.91118 39.9147851 -1.951938 5.103865e-02 plot(Wage$age, predict(res_lm4), pch = 2, col = &quot;blue&quot;) # res_lm4_2 = lm(wage ~ poly(age, 4, raw = T), data = Wage) # 非直交化 head(poly(Wage$age, 4, raw = T)) # 4次の多項式 #&gt; 1 2 3 4 #&gt; [1,] 18 324 5832 104976 #&gt; [2,] 24 576 13824 331776 #&gt; [3,] 45 2025 91125 4100625 #&gt; [4,] 43 1849 79507 3418801 #&gt; [5,] 50 2500 125000 6250000 #&gt; [6,] 54 2916 157464 8503056 coef(summary(res_lm4_2)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.841542e+02 6.004038e+01 -3.067172 0.0021802539 #&gt; poly(age, 4, raw = T)1 2.124552e+01 5.886748e+00 3.609042 0.0003123618 #&gt; poly(age, 4, raw = T)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446 #&gt; poly(age, 4, raw = T)3 6.810688e-03 3.065931e-03 2.221409 0.0263977518 #&gt; poly(age, 4, raw = T)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498 モデル式を変えて結果を比較する. # 上と同一 res_lm4_3 = lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage) coef(summary(res_lm4_3)) # res_lm4_3 = lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage) coef(summary(res_lm4_3)) # 以下と比較せよ res_lm4_4 = lm(wage ~ age + age^2 + age^3 + age^4, data = Wage) coef(summary(res_lm4_4)) 3.6 関数factor()について Rの初心者向けに関数factor()の使い方に関する簡単な説明を行う. Rでのfactor関数の使い方 サンプルコードの出所: ChatGPT (GPT-4) 関数factor()は, カテゴリーデータ（文字列や整数を値に持つベクトル）を因子型（factor）に変換するために使用される. ベクトルを因子に変換 # 質的変数 (文字列ベクトル) を定義 ratings &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;) # 因子 (ファクター) 化 ratings_factor &lt;- factor(ratings) ratings_factor #&gt; [1] A B C B A C A B #&gt; Levels: A B C 因子水準（カテゴリー）の確認 # 因子水準 levels(ratings_factor) #&gt; [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; 因子ラベルの変更 # 因子ラベルを付け替え # 注) 実行前に, 水準とラベルが正しく対応していることを確認すること levels(ratings_factor) &lt;- c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Fair&quot;) levels(ratings_factor) #&gt; [1] &quot;Excellent&quot; &quot;Good&quot; &quot;Fair&quot; 因子の並べ替え # ファクターの並び替え ratings_factor &lt;- factor(ratings_factor, levels = c(&quot;Fair&quot;, &quot;Good&quot;, &quot;Excellent&quot;)) ratings_factor #&gt; [1] Excellent Good Fair Good Excellent Fair Excellent #&gt; [8] Good #&gt; Levels: Fair Good Excellent (質的な値を持つ) ベクトルをfactor()により因子に変換する際, Rのデフォルトでは水準の値に対してアルファベット順に順番 (整数) が割当てられる. 名義尺度変数であれば水準に割り当てられる順番自体には本来意味は持たないはずであるが, それでも, 例えば箱ひげ図など質的変数を使ったプロットする場合, 意図とは異なる順番に配置され不都合が起こることがある. 例えば, “level1”, “level2”, …,“level9”, “level10”のような数値を含んたベクトルに対してfactor() を適用して因子化すると, “level1”の次に”level2”ではなく, “level10”が配置されてしまったりする. aaa &lt;- c(&quot;level1&quot;, &quot;level2&quot;, &quot;level1&quot;, &quot;level10&quot;, &quot;level2&quot;, &quot;level10&quot;) factor(aaa) #&gt; [1] level1 level2 level1 level10 level2 level10 #&gt; Levels: level1 level10 level2 このような不具合を防ぐには, factor()を適用する際に, 因子水準 (並び順) を引数levelsで予め指定しておくのが良い. その際, 引数labelsも併用し, 使いやすいラベルを付与しておくと良い. aaa_factor &lt;- factor(aaa, levels = c(&quot;level1&quot;, &quot;level2&quot;, &quot;level10&quot;), labels = c(&quot;L1&quot;, &quot;L2&quot;, &quot;L10&quot;)) aaa_factor #&gt; [1] L1 L2 L1 L10 L2 L10 #&gt; Levels: L1 L2 L10 因子の利用 Rのデータフレーム (data.frame) は, 行列の形をしているが, 実際は, 長さは等しいものの 異なる”データ型”のベクトルを要素に持つリスト (list) である. 因子に変換されたデータは, データフレーム内でカテゴリー変数として保持することで, 統計解析や可視化などで利用される. # サンプルのカテゴリーデータ ratings &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;) prices &lt;- c(100, 85, 60, 79, 90, 65, 92, 84) # データフレームの作成 df &lt;- data.frame(ratings = factor(ratings), prices = prices) df #&gt; ratings prices #&gt; 1 A 100 #&gt; 2 B 85 #&gt; 3 C 60 #&gt; 4 B 79 #&gt; 5 A 90 #&gt; 6 C 65 #&gt; 7 A 92 #&gt; 8 B 84 # データフレームの要約 summary(df) #&gt; ratings prices #&gt; A:3 Min. : 60.00 #&gt; B:3 1st Qu.: 75.50 #&gt; C:2 Median : 84.50 #&gt; Mean : 81.88 #&gt; 3rd Qu.: 90.50 #&gt; Max. :100.00 # 箱ひげ図 plot(prices ~ ratings, data = df) Rにおける標準的な統計解析や可視化を行う場合には, 関数factor()を使うことで, カテゴリー変数はR内部で適正に処理されるため 一変数のままで使用することができる. すなわち, 通常, カテゴリーの水準に対応するダミー変数を作る作業は不要である. 関数lm()やglm()等においては, 文字列を値に持つカテゴリー変数は明示的にfactor()を使って因子型に 変換せずとも, Rは因子型と解釈して関数を実行するが, もしそのカテゴリー変数が整数値 (例, area &lt;- c(1, 2, 2, 1, 1)) を持つ場合には, factor()を使って 変換しないと, 意図とは異なる結果やエラーを生じることになる # カテゴリーデータが整数値で記録されている場合 ratings &lt;- c(1, 2, 3, 2, 1, 3, 1, 2) prices &lt;- c(100, 85, 60, 79, 90, 65, 92, 84) # データフレームの作成 df &lt;- data.frame(ratings = factor(ratings), prices = prices) df #&gt; ratings prices #&gt; 1 1 100 #&gt; 2 2 85 #&gt; 3 3 60 #&gt; 4 2 79 #&gt; 5 1 90 #&gt; 6 3 65 #&gt; 7 1 92 #&gt; 8 2 84 # データフレームの要約? summary(df) #&gt; ratings prices #&gt; 1:3 Min. : 60.00 #&gt; 2:3 1st Qu.: 75.50 #&gt; 3:2 Median : 84.50 #&gt; Mean : 81.88 #&gt; 3rd Qu.: 90.50 #&gt; Max. :100.00 # 箱ひげ図? plot(prices ~ ratings, data = df) # 適切な処理: 関数factor()の使用 df &lt;- data.frame(area = factor(ratings, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)), prices = prices) df #&gt; area prices #&gt; 1 A 100 #&gt; 2 B 85 #&gt; 3 C 60 #&gt; 4 B 79 #&gt; 5 A 90 #&gt; 6 C 65 #&gt; 7 A 92 #&gt; 8 B 84 # データフレームの要約 summary(df) #&gt; area prices #&gt; A:3 Min. : 60.00 #&gt; B:3 1st Qu.: 75.50 #&gt; C:2 Median : 84.50 #&gt; Mean : 81.88 #&gt; 3rd Qu.: 90.50 #&gt; Max. :100.00 # 箱ひげ図 boxplot(prices ~ ratings, data = df) "],["ロジットブロビット回帰分析.html", "4 ロジット/ブロビット回帰分析 4.1 ロジット回帰分析の基本操作 4.2 データ分析例 4.3 疑似R2の計算", " 4 ロジット/ブロビット回帰分析 4.1 ロジット回帰分析の基本操作 ロジットモデル: シミュレーションデータ 基本操作を確認するため, ロジット回帰モデルが想定するデータ生成メカニズムに従って, 人工データを生成する. その後, このデータを使って, ロジット回帰分析を行う関数であるglm()を呼び出して実行する. シミュレーションデータの生成 set.seed(1) n &lt;- 100 p &lt;- 2 a &lt;- 1.2; b &lt;- c(0.5, 1.5) X &lt;- matrix(runif(n * p, -5, 5), ncol = p) # 予測変数 (X1, X2) colnames(X) &lt;- paste0(&quot;X&quot;, 1:p) eta &lt;- a + X %*% b # 線形予測子 pi &lt;- exp(eta)/(1 + exp(eta)) # ロジスティック変換 y &lt;- rbinom(n, 1, pi) # 発生頻度 (ランダム) plot(X[, 1], y) plot(X[, 2], y) ロジット回帰分析の実行 ロジット回帰分析は, R関数glm()を用いて実行することができる. glm()は標準パッケージstatsに含まれているため, あらたなインストール作業は不要である. glm()を用いた分析の操作手順や出力結果の読み方は, おおむねlm()を踏襲すれば良い. glm()に与えるモデル式 (引数formula) のシンタックスは lm()と同様である. それに加えて, glm()では, 引数familyによってモデルの (分析データを生成していると想定する) 確率分布を指定する. ロジット回帰/プロビット回帰では, binomialを指定する. また, lm()と同様に, glm()の出力結果に対して summary()関数を適用することで, 推定結果が出力される. # ロジット回帰 res_glm &lt;- glm(y ~ X, family = binomial) summary(res_glm) #&gt; #&gt; Call: #&gt; glm(formula = y ~ X, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.5815 0.8647 2.985 0.002831 ** #&gt; XX1 1.1494 0.3434 3.347 0.000818 *** #&gt; XX2 2.7630 0.7509 3.680 0.000233 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 133.750 on 99 degrees of freedom #&gt; Residual deviance: 34.478 on 97 degrees of freedom #&gt; AIC: 40.478 #&gt; #&gt; Number of Fisher Scoring iterations: 8 summary()の出力結果の主な相違は, 個別の回帰係数の有意性を見る指標が, lm()ではt値であったところが glm()ではz値になっている点 (それに伴い, 付随するp値の表記も変化), および, 全体の適合度の指標について, lm()のように\\(R^2\\), 補正\\(R^2\\)の値が表示されておらず, その代わりとして, “Null Deviance”, “Residual Deviance”が表示されている点である. (内挿) 予測 glm()の出力結果を関数predict()に与えることで, 内挿予測 (適合値の算出) を行うことができる. 注意点としては, 引数typeを指定しない場合 (デフォルト設定) には, 未知パラメータの線形結合である “線形予測子”\\(\\eta=\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k\\)の予測値が出力される. 関数predict(): - 予測値を返す関数 - 引数typeの選択肢: &quot;link&quot;(デフォルト), &quot;response&quot;, &quot;terms&quot; - &quot;link&quot;: 線形予測子のスケールにおける予測 - &quot;response&quot;: 反応変数のスケールにおける予測 - &quot;terms&quot;: 線形予測子のスケールにおける予測, モデル式の各項の適合値を格納した行列を返す ロジット回帰において, 確率の予測を得たい場合には, type=\"response\"と指定する必要がある. # 予測 (内挿) eta_hat &lt;- predict(res_glm) # 対数オッズ p_hat &lt;- predict(res_glm, type = &quot;response&quot;) # 確率 head(data.frame(y, p_hat)) #&gt; y p_hat #&gt; 1 1 0.9846543 #&gt; 2 0 0.0500027 #&gt; 3 0 0.0507452 #&gt; 4 1 1.0000000 #&gt; 5 1 0.9448650 #&gt; 6 0 0.3178520 plot(eta_hat, p_hat) 以下のように, 線形予測子の予測値に対して, ロジスティク変換 (リンク関数\\(g(\\cdot)\\)の逆関数) を適用すると, 確率の予測値p_hatと一致することが確認される. # 確認 # logistic_f &lt;- function(eta){ # exp(eta) / (1 + exp(eta)) # } # logistic_f(eta_hat) # 線形予測子のロジスティク変換 exp(eta_hat) / (1 + exp(eta_hat)) # 線形予測子のロジスティック変換 #&gt; 1 2 3 4 5 6 #&gt; 9.846543e-01 5.000270e-02 5.074520e-02 1.000000e+00 9.448650e-01 3.178520e-01 #&gt; 7 8 9 10 11 12 #&gt; 7.255619e-02 9.786489e-01 9.999999e-01 5.680028e-01 9.999957e-01 9.948751e-01 #&gt; 13 14 15 16 17 18 #&gt; 6.840476e-01 3.443645e-01 1.734430e-02 1.848314e-05 9.999839e-01 6.129494e-02 #&gt; 19 20 21 22 23 24 #&gt; 4.300994e-01 9.999350e-01 1.000000e+00 2.997055e-01 9.800110e-01 2.155297e-05 #&gt; 25 26 27 28 29 30 #&gt; 9.990392e-01 4.996798e-01 6.281495e-02 1.057488e-03 3.392529e-01 9.674261e-01 #&gt; 31 32 33 34 35 36 #&gt; 9.883911e-01 3.490784e-04 3.277648e-05 9.488525e-01 1.000000e+00 9.992746e-01 #&gt; 37 38 39 40 41 42 #&gt; 9.995223e-01 2.305139e-01 1.000000e+00 8.548202e-01 9.999879e-01 9.991567e-01 #&gt; 43 44 45 46 47 48 #&gt; 2.007423e-01 2.957535e-02 9.999048e-01 9.900165e-01 6.973787e-06 9.998923e-01 #&gt; 49 50 51 52 53 54 #&gt; 3.461442e-03 9.999997e-01 9.958972e-01 9.997546e-01 5.412342e-02 1.615667e-01 #&gt; 55 56 57 58 59 60 #&gt; 8.780960e-02 1.960509e-05 7.839028e-01 1.310930e-04 1.548464e-01 1.613948e-03 #&gt; 61 62 63 64 65 66 #&gt; 7.990981e-01 9.999853e-01 6.514770e-01 9.997732e-01 9.999996e-01 6.913302e-02 #&gt; 67 68 69 70 71 72 #&gt; 6.024214e-05 7.496756e-01 9.817408e-01 9.174800e-01 9.870712e-01 9.999999e-01 #&gt; 73 74 75 76 77 78 #&gt; 9.999765e-01 8.861010e-02 2.704708e-01 1.000000e+00 9.999787e-01 9.996573e-01 #&gt; 79 80 81 82 83 84 #&gt; 9.998298e-01 1.000000e+00 2.045796e-02 2.912945e-02 9.999945e-01 6.607180e-01 #&gt; 85 86 87 88 89 90 #&gt; 9.999999e-01 8.082135e-05 9.999947e-01 9.882999e-01 9.999933e-01 4.497550e-01 #&gt; 91 92 93 94 95 96 #&gt; 9.956770e-01 3.843901e-03 1.100383e-03 1.000000e+00 4.496549e-01 9.997968e-01 #&gt; 97 98 99 100 #&gt; 1.668414e-04 9.999826e-01 7.548861e-01 9.999909e-01 # → p_hatと同じ結果となることを確認 係数の信頼区間 同様に, 回帰係数の信頼区間は関数confint()を使えば良い. # 信頼区間 confint(res_glm) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 1.1914472 4.659417 #&gt; XX1 0.5901329 1.974012 #&gt; XX2 1.5989191 4.620681 confint(res_glm, level = 0.90) # 90%信頼区間 #&gt; 5 % 95 % #&gt; (Intercept) 1.3787136 4.269921 #&gt; XX1 0.6664132 1.818782 #&gt; XX2 1.7488603 4.265779 4.2 データ分析例 データセット (1): 企業パフォーマンス・データ (仮想) - firmperf.txt, 8件 - 企業規模 (size): H/L - 人材投資 (hr_invest): H/L - SDGs活動 (sdg): H/L - 対象企業数 (n_tot): 社 - 優良社数 (n_pos): 社 データ読み込み perf_dat1 &lt;- read.csv(&quot;firmperf.txt&quot;, skip = 2) # 注) デフォルトはstringsAsFactors = F (文字列を因子型変数に変換せずに読み込む) # size:sdgは, 関数read.csv()でそのまま読み込むと文字型変数となる. # 読み込み時に因子型にするには, stringsAsFactors = T # 執筆現在 (2024年5月10日)のglm()の仕様では, 文字型のままでもOK colnames(perf_dat1) &lt;- c(&quot;size&quot;, &quot;hr_invest&quot;, &quot;sdg&quot;, &quot;n_tot&quot;, &quot;n_pos&quot;) attach(perf_dat1) ロジット回帰実行 ロジット回帰の実行の際, 目的変数として glm()へは異なるデータ形式を与えることもできる. まず, 目的変数のデータ形式として, “成功回数”,“失敗回数” の2列を持つ行列をglm内でモデル式formulaの左辺に与える例である. # データ形式-1 # &quot;成功回数&quot;、&quot;失敗回数&quot;の2列 perf_tbl &lt;- cbind(n_pos, n_tot - n_pos) res_glm1 &lt;- glm(perf_tbl ~ size + hr_invest + sdg, family = binomial) summary(res_glm1) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ size + hr_invest + sdg, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.63288 0.25177 -2.514 0.01195 * #&gt; sizeL -0.07844 0.26926 -0.291 0.77082 #&gt; hr_investL -0.82333 0.27607 -2.982 0.00286 ** #&gt; sdgL -0.61585 0.35132 -1.753 0.07961 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.3921 on 4 degrees of freedom #&gt; AIC: 36.198 #&gt; #&gt; Number of Fisher Scoring iterations: 4 線形回帰分析の時と同様, 二つのモデルの間に包含関係 (ネスト, “nested”) がある場合には, 分散分析を行う関数anova()を適用することで, 追加された項がまとめて有意かどうかを評価することができる. 下の例は, 切片しか持たないNullモデル (perf_tbl ~ 1) による モデルの当てはまりの悪さ, “Residual deviance” (残差逸脱度) を, hr_investとsdgの2変数を 持つモデルの”Residual deviance”と比較し, 後者の残差逸脱度がこれら2変数を加えることでどれだけ 改善したのかをみることことで, 有意性を評価している. res_glm2 &lt;- glm(perf_tbl ~ hr_invest + sdg, binomial) summary(res_glm2) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ hr_invest + sdg, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.6570 0.2380 -2.760 0.00578 ** #&gt; hr_investL -0.8235 0.2760 -2.983 0.00285 ** #&gt; sdgL -0.6100 0.3507 -1.739 0.08200 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.4775 on 5 degrees of freedom #&gt; AIC: 34.284 #&gt; #&gt; Number of Fisher Scoring iterations: 4 res_glm0 &lt;- glm(perf_tbl ~ 1, binomial) # 切片項のみ (null model) summary(res_glm0) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ 1, family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.3950 0.1205 -11.58 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.255 on 7 degrees of freedom #&gt; Residual deviance: 15.255 on 7 degrees of freedom #&gt; AIC: 43.061 #&gt; #&gt; Number of Fisher Scoring iterations: 4 # anova(res_glm2, test = &quot;Chisq&quot;) # カイ2乗検定 (test = &quot;LRT&quot;でも可) anova(res_glm0, res_glm2, test = &quot;Chisq&quot;) # 同 #&gt; Analysis of Deviance Table #&gt; #&gt; Model 1: perf_tbl ~ 1 #&gt; Model 2: perf_tbl ~ hr_invest + sdg #&gt; Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) #&gt; 1 7 15.2547 #&gt; 2 5 2.4775 2 12.777 0.001681 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 今回のケースでは, 2変数を加える (自由度2を使う) ことで, 残差逸脱度が15.2547から2.4775へと12.777減少すること, これにより, 帰無仮説 (二つの変数の回帰係数が同時にゼロ) の下で従う自由度2の\\(\\chi^2\\)分布に基づいて計算される\\(p\\)値が0.001681であること, したがって, 有意水準1%で有意である (帰無仮説が棄却される) ことを示している. 目的変数のデータ形式として, 上とは別の形式として, “成功率”を”試行回数”と共にglmに与える例を次に示す. # データ形式-2 # &quot;成功率&quot;の指定 prop_perf &lt;- n_pos / n_tot res_glm1_2 &lt;- glm(prop_perf ~ size + hr_invest + sdg, binomial, weights = n_tot) summary(res_glm1_2) #&gt; #&gt; Call: #&gt; glm(formula = prop_perf ~ size + hr_invest + sdg, family = binomial, #&gt; weights = n_tot) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.63288 0.25177 -2.514 0.01195 * #&gt; sizeL -0.07844 0.26926 -0.291 0.77082 #&gt; hr_investL -0.82333 0.27607 -2.982 0.00286 ** #&gt; sdgL -0.61585 0.35132 -1.753 0.07961 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.3921 on 4 degrees of freedom #&gt; AIC: 36.198 #&gt; #&gt; Number of Fisher Scoring iterations: 4 プロビット回帰については, glm()の引数family=binomial (link = \"probit\")と指定すれば, あとは ロジット回帰と同様に実行できる. # プロビット回帰 res_glm1_p &lt;- glm(perf_tbl ~ size + hr_invest + sdg, family = binomial(link = &quot;probit&quot;)) # probit summary(res_glm1_p) #&gt; #&gt; Call: #&gt; glm(formula = perf_tbl ~ size + hr_invest + sdg, family = binomial(link = &quot;probit&quot;)) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -0.39738 0.15235 -2.608 0.0091 ** #&gt; sizeL -0.04947 0.15331 -0.323 0.7470 #&gt; hr_investL -0.48155 0.16439 -2.929 0.0034 ** #&gt; sdgL -0.33830 0.18901 -1.790 0.0735 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 15.2547 on 7 degrees of freedom #&gt; Residual deviance: 2.4259 on 4 degrees of freedom #&gt; AIC: 36.232 #&gt; #&gt; Number of Fisher Scoring iterations: 3 (内挿) 予測 関数predict()を用いて予測値を出力するが, 上述のように引数typeを指定しない場合には, 線形予測子 (のスケールでの) 予測を行う. 具体的には, ロジット回帰では対数オッズを, プロビット回帰では標準正規分布の分位点 (分位点関数は累積分布関数の逆関数) を返す. 一方, type=\"response\"とした場合には, どちらのモデルにおいても, 反応変数のスケール, すなわち, 確率の予測値を返す. # 予測 (ロジット回帰) predict(res_glm1_2) # 対数オッズ (∵ロジット回帰) #&gt; 1 2 3 4 5 6 7 #&gt; -2.0720496 -1.2487226 -1.4562023 -1.3271595 -1.5346392 -0.6328753 -0.7113122 #&gt; 8 #&gt; -2.1504865 p_hat &lt;- predict(res_glm1_2, type = &quot;response&quot;) # 確率 predict(res_glm1_2, type = &quot;terms&quot;) # 線形予測子の項ごとの適合値 #&gt; size hr_invest sdg #&gt; 1 0.03921846 -0.4116635 -0.3079237 #&gt; 2 0.03921846 0.4116635 -0.3079237 #&gt; 3 0.03921846 -0.4116635 0.3079237 #&gt; 4 -0.03921846 0.4116635 -0.3079237 #&gt; 5 -0.03921846 -0.4116635 0.3079237 #&gt; 6 0.03921846 0.4116635 0.3079237 #&gt; 7 -0.03921846 0.4116635 0.3079237 #&gt; 8 -0.03921846 -0.4116635 -0.3079237 #&gt; attr(,&quot;constant&quot;) #&gt; [1] -1.391681 # 同 (プロビット回帰) predict(res_glm1_p) # 標準正規分布の分位点 (∵プロビット回帰) #&gt; 1 2 3 4 5 6 7 #&gt; -1.2172398 -0.7356881 -0.8789356 -0.7851545 -0.9284021 -0.3973839 -0.4468504 #&gt; 8 #&gt; -1.2667063 p_hat_p &lt;- predict(res_glm1_p, type = &quot;response&quot;) # 確率 # 予測値の比較: ロジット vs プロビット data.frame(perf_dat1, logit = p_hat, probit = p_hat_p) #&gt; size hr_invest sdg n_tot n_pos logit probit #&gt; 1 H L L 60 7 0.1118433 0.1117565 #&gt; 2 H H L 8 2 0.2229213 0.2309603 #&gt; 3 H L H 186 36 0.1890489 0.1897181 #&gt; 4 L H L 3 0 0.2096296 0.2161815 #&gt; 5 L L H 86 14 0.1773159 0.1765995 #&gt; 6 H H H 50 16 0.3468589 0.3455422 #&gt; 7 L H H 22 9 0.3293090 0.3274915 #&gt; 8 L L L 18 2 0.1042858 0.1026302 # 参考) 説明変数が因子型(factor)でない場合, 以前はエラー発生 # → 文字型変数は因子型への変換が必要だった plot(factor(hr_invest), predict(res_glm2, type = &quot;response&quot;)) plot(factor(sdg), predict(res_glm2, type = &quot;response&quot;)) # 説明変数が数値型変数ならば、logistic曲線を描く 係数の信頼区間 # 信頼区間 confint(res_glm1_2) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -1.1392044 -0.14805877 #&gt; sizeL -0.6194614 0.43967083 #&gt; hr_investL -1.3592877 -0.27372310 #&gt; sdgL -1.3520115 0.03750972 confint(res_glm1_2, level = 0.90) # 90%信頼区間 #&gt; 5 % 95 % #&gt; (Intercept) -1.0558140 -0.22506829 #&gt; sizeL -0.5304547 0.35748113 #&gt; hr_investL -1.2736384 -0.36344375 #&gt; sdgL -1.2264225 -0.06339516 detach() (自主課題) “firmperf.txt”に対するロジット回帰分析の結果を解釈してみよう. データセット (2): 個人ローン・デフォルト・データ (仮想) - default.csv, 100件 - デフォルト (1/0) - ローン残高 (万円) - 収入 (万円) - 職種 (A/B) データ読み込み default &lt;- read.csv(&quot;default.csv&quot;) attach(default) ロジット回帰 # ロジット回帰 res_glm &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial) summary(res_glm) #&gt; #&gt; Call: #&gt; glm(formula = デフォルト ~ ローン残高 + 収入 + 職種, #&gt; family = binomial) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -1.336e+01 3.505e+00 -3.811 0.000138 *** #&gt; ローン残高 4.708e-03 1.224e-03 3.846 0.000120 *** #&gt; 収入 -9.227e-04 2.345e-03 -0.394 0.693925 #&gt; 職種B 9.713e-01 1.445e+00 0.672 0.501434 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 102.791 on 99 degrees of freedom #&gt; Residual deviance: 46.971 on 96 degrees of freedom #&gt; AIC: 54.971 #&gt; #&gt; Number of Fisher Scoring iterations: 8 plot(ローン残高, predict(res_glm, type = &quot;response&quot;)) plot(収入, predict(res_glm, type = &quot;response&quot;)) # plot(職種, predict(res_glm, type = &quot;response&quot;)) プロビット回帰 # プロビット回帰 res_glm_p &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial(link = &quot;probit&quot;)) summary(res_glm_p) #&gt; #&gt; Call: #&gt; glm(formula = デフォルト ~ ローン残高 + 収入 + 職種, #&gt; family = binomial(link = &quot;probit&quot;)) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) -7.0233825 1.6903093 -4.155 3.25e-05 *** #&gt; ローン残高 0.0024920 0.0005893 4.229 2.35e-05 *** #&gt; 収入 -0.0006478 0.0013082 -0.495 0.620 #&gt; 職種B 0.6549554 0.8035424 0.815 0.415 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 102.79 on 99 degrees of freedom #&gt; Residual deviance: 47.81 on 96 degrees of freedom #&gt; AIC: 55.81 #&gt; #&gt; Number of Fisher Scoring iterations: 8 ## 結果の比較: glm vs lm # res_glm_normal &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = gaussian) # summary(res_glm_normal) # res_lm &lt;- lm(デフォルト ~ ローン残高 + 収入 + 職種) # anova(res_lm) (外挿) 予測 外挿予測を行いたい場合には, 関数predict()に対して, glm()の出力と共に, 予測を行いたい予測変数の値の組をデータフレームとして, 引数newdataに与えれば良い. # 予測 (新しいデータセットに対して) newdat &lt;- data.frame(ローン残高 = c(100, 500, 1000, 10000), 収入 = 30000, 職種 = &quot;A&quot;) predict(res_glm, newdata = newdat, type = &quot;response&quot;) #&gt; 1 2 3 4 #&gt; 2.220446e-16 2.220446e-16 2.220446e-16 9.976404e-01 係数の信頼区間 # 信頼区間 confint(res_glm) # 95%信頼区間 (デフォルト) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -21.713657994 -7.721969240 #&gt; ローン残高 0.002743950 0.007642011 #&gt; 収入 -0.005498298 0.003877354 #&gt; 職種B -1.917922786 3.862413416 detach() 便利なツール: パッケージepiDisplay # (参考) 便利なツール library(epiDisplay) logistic.display(res_glm, simplified = TRUE) #&gt; #&gt; OR lower95ci upper95ci Pr(&gt;|Z|) #&gt; ローン残高 1.0047194 1.0023118 1.007133 0.0001199072 #&gt; 収入 0.9990777 0.9944971 1.003679 0.6939246907 #&gt; 職種B 2.6413382 0.1555813 44.842586 0.5014338592 (自主課題) “default.csv”に対するロジット回帰分析の結果を解釈してみよう. 4.3 疑似R2の計算 線形回帰の場合と異なり, ロジット回帰では, \\(R^2\\)は計算されない. しかし, ロジット回帰 (より一般に, 一般化線形モデル) においては, 疑似的に\\(R^2\\)を計算する方法が 複数提案されている. # ロジット回帰 res_glm &lt;- glm(デフォルト ~ ローン残高 + 収入 + 職種, family = binomial, data = default) # summary(res_glm) 以下では, 3つのパッケージDescTools, pscl, performanceを使用した例を紹介する. パッケージDescToolsの利用 DescTools::PseudoR2() - usage: PseudoR2(x, which = NULL) - which: 計算したい疑似R2. 選択肢: &quot;McFadden&quot;(デフォルト), &quot;McFaddenAdj&quot;, &quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;AldrichNelson&quot;, &quot;VeallZimmermann&quot;, &quot;Efron&quot;, &quot;McKelveyZavoina&quot;, &quot;Tjur&quot;, &quot;all&quot;. # 疑似R2の計算 library(DescTools) PseudoR2(res_glm) # McFadden (デフォルト) #&gt; McFadden #&gt; 0.5430468 PseudoR2(res_glm, which = &quot;CoxSnell&quot;) # Cox-Snell #&gt; CoxSnell #&gt; 0.4277647 PseudoR2(res_glm, which = &quot;Nagelkerke&quot;) # Nagelkerke #&gt; Nagelkerke #&gt; 0.6660436 PseudoR2(res_glm, which = &quot;all&quot;) #&gt; McFadden McFaddenAdj CoxSnell Nagelkerke AldrichNelson #&gt; 0.5430468 0.4652192 0.4277647 0.6660436 0.3582359 #&gt; VeallZimmermann Efron McKelveyZavoina Tjur AIC #&gt; 0.7067438 0.5736644 0.8515845 0.5589555 54.9708332 #&gt; BIC logLik logLik0 G2 #&gt; 65.3915139 -23.4854166 -51.3956671 55.8205010 疑似\\(R^2\\)は, 方法により相当異なることが確認される. パッケージpsclの利用 pscl::pR2() - 以下を出力: - llh: The log-likelihood from the fitted model - llhNull: The log-likelihood from the intercept-only restricted model - G2: Minus two times the difference in the log-likelihoods - McFadden: McFadden&#39;s pseudo r-squared - r2ML: Maximum likelihood pseudo r-squared - r2CU: Cragg and Uhler&#39;s pseudo r-squared # install.packages(&quot;pscl&quot;) library(pscl) # 疑似R2の計算 pscl::pR2(res_glm) #&gt; fitting null model for pseudo-r2 #&gt; llh llhNull G2 McFadden r2ML r2CU #&gt; -23.4854166 -51.3956671 55.8205010 0.5430468 0.4277647 0.6660436 パッケージperformanceの利用 performance::r2() - モデルに応じて適切な疑似R2を選んで出力: - Logistic models: Tjur&#39;s R2 - General linear models: Nagelkerke&#39;s R2 - Multinomial Logit: McFadden&#39;s R2 - Models with zero-inflation: R2 for zero-inflated models - Mixed models: Nakagawa&#39;s R2 - Bayesian models: R2 bayes # install.packages(&quot;performance&quot;) library(performance) # 疑似R2の計算 performance::r2(res_glm) #&gt; # R2 for Logistic Regression #&gt; Tjur&#39;s R2: 0.559 "],["一般化加法モデル-gam.html", "5 一般化加法モデル (GAM) 5.1 パッケージgamの利用 5.2 パッケージmgcvの利用 5.3 パッケージprophetの利用 5.4 【追加分析】Boston housingデータセットの変数間の関係性", " 5 一般化加法モデル (GAM) 本章では, GAMを実行するパッケージとして, (参考書ISLRが使用している) gam, およびmgcvを紹介する. しかし, 後述のように, 二つのライブラリを同時に読み込むと プログラムの一部が正常に動作しなくなる可能性があるため, 一つのプログラムの中ではどちらか一方のみの使用が安全である. 具体的には, mgcvのみを使用するのが良い. mgcvの方が理論的により適切な計算を行っている (後述). さらに, mgcvの方が, より多様な機能を有している. 例えば, パッケージmgcvの方の関数gam()は, 予測変数間の交互作用をモデルに 取り入れられるなど柔軟性に富む. パッケージmgcvの方の関数s()は, 多様な平滑化項を指定することができる. 5.1 パッケージgamの利用 本小節の出所: ISLR, Ch.7 (pp.287–) library(gam) 関数gam(): - 加法予測子のシンボリック表現や誤差分布を指定の上でGAMをフィット - バックフィッティング・アルゴリズムを採用 (異なる平滑化やフィッティングの方法を混ぜる) - 局所回帰や平滑化スプラインをサポート - usage: gam(formula, family = gaussian, data, weights, subset, na.action, start, etastart, mustart, control = gam.control(...), model = TRUE, method, x = FALSE, y = TRUE, ...) library(ISLR) # Wage head(Wage) #&gt; year age maritl race education region #&gt; 231655 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic #&gt; 86582 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic #&gt; 161300 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic #&gt; 155159 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic #&gt; 11443 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic #&gt; 376662 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic #&gt; jobclass health health_ins logwage wage #&gt; 231655 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 #&gt; 86582 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 #&gt; 161300 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 #&gt; 155159 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 #&gt; 11443 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 #&gt; 376662 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 Wage: 東海岸賃金データ (Mid-Atlantic Wage Data) - 東海岸地域 (Mid-Atlantic region) 3000人の男性労働者の賃金その他のデータ. - year 賃金情報の記録年 - age 労働者の年齢 - maritl 婚姻状況 (因子): 1.未婚, 2. 既婚, 3. 配偶者死別, 4. 離婚 5. 別居 - race 人種 (因子): 1. 白人, 2. 黒人, 3. アジア人, 4. その他 - education 教育水準 (因子): 1. 高校未卒業, 2. 高卒, 3. 大学中退 (課程の一部を修了), 4. 学部卒, 5. 院卒 - region 国の地域（東海岸のみ） - jobclass 職種 (因子): 1. 産業, 2. 情報 - health 健康状態 (因子): 1. 良好以下, 2. 非常に良好以上 - health_ins 健康保険の有無 (因子): 1. あり, 2. なし - logwage 労働者の賃金の対数 - wage 労働者の賃金 (名目) 多項式回帰 ※以下の多項式回帰部分は, 既に配布したものと重複. fit_lm &lt;- lm(wage ~ poly(age, 4), data = Wage) # 4次の直交多項式 head(poly(Wage$age, 4)) # 4次の多項式 #&gt; 1 2 3 4 #&gt; [1,] -0.0386247992 0.055908727 -0.0717405794 0.086729854 #&gt; [2,] -0.0291326034 0.026298066 -0.0145499511 -0.002599280 #&gt; [3,] 0.0040900817 -0.014506548 -0.0001331835 0.014480093 #&gt; [4,] 0.0009260164 -0.014831404 0.0045136682 0.012657507 #&gt; [5,] 0.0120002448 -0.009815846 -0.0111366263 0.010211456 #&gt; [6,] 0.0183283753 -0.002073906 -0.0166282799 -0.001314381 coef(summary(fit_lm)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 111.70361 0.7287409 153.283015 0.000000e+00 #&gt; poly(age, 4)1 447.06785 39.9147851 11.200558 1.484604e-28 #&gt; poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32 #&gt; poly(age, 4)3 125.52169 39.9147851 3.144742 1.678622e-03 #&gt; poly(age, 4)4 -77.91118 39.9147851 -1.951938 5.103865e-02 fit_lm &lt;- lm(wage ~ poly(age, 4, raw = T), data = Wage) # 非直交化 head(poly(Wage$age, 4, raw = T)) # 4次の多項式 #&gt; 1 2 3 4 #&gt; [1,] 18 324 5832 104976 #&gt; [2,] 24 576 13824 331776 #&gt; [3,] 45 2025 91125 4100625 #&gt; [4,] 43 1849 79507 3418801 #&gt; [5,] 50 2500 125000 6250000 #&gt; [6,] 54 2916 157464 8503056 coef(summary(fit_lm)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -1.841542e+02 6.004038e+01 -3.067172 0.0021802539 #&gt; poly(age, 4, raw = T)1 2.124552e+01 5.886748e+00 3.609042 0.0003123618 #&gt; poly(age, 4, raw = T)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446 #&gt; poly(age, 4, raw = T)3 6.810688e-03 3.065931e-03 2.221409 0.0263977518 #&gt; poly(age, 4, raw = T)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498 # 上と同一 fit_lm &lt;- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage) coef(summary(fit_lm)) fit_lm &lt;- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage) coef(summary(fit_lm)) # 以下と比較せよ fit_lm2 &lt;- lm(wage ~ age + age^2 + age^3 + age^4, data = Wage) coef(summary(fit_lm2)) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 81.7047354 2.84624224 28.70618 2.543375e-160 #&gt; age 0.7072759 0.06475113 10.92299 2.900778e-27 スプライン適合 agelims &lt;- range(Wage$age) age_grid &lt;- seq(from = agelims[1], to = agelims[2]) library(splines) # B-スプライン fit_sp &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage) # bs: B-スプライン pred_sp &lt;- predict(fit_sp, newdata = list(age = age_grid), se = T) plot(Wage$age, Wage$wage, col = &quot;grey&quot;) lines(age_grid, pred_sp$fit, lw = 2) lines(age_grid, pred_sp$fit + 2 * pred_sp$se, lty = &quot;dashed&quot;) lines(age_grid, pred_sp$fit - 2 * pred_sp$se, lty = &quot;dashed&quot;) # 自然スプライン fit_sp2 &lt;- lm(wage ~ ns(age, df = 4), data = Wage) # ns: 自然スプライン(4次) pred_sp2 &lt;- predict(fit_sp2, newdata = list(age = age_grid), se = T) plot(Wage$age, Wage$wage, col = &quot;grey&quot;) lines(age_grid, pred_sp2$fit, lw = 2) lines(age_grid, pred_sp2$fit + 2 * pred_sp2$se, lty = &quot;dashed&quot;) lines(age_grid, pred_sp2$fit - 2 * pred_sp2$se, lty = &quot;dashed&quot;) # 平滑化スプライン plot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = &quot;darkgrey&quot;) fit_sp3 &lt;- smooth.spline(Wage$age, Wage$wage, df = 16) # 平滑化スプライン(自由度 = 16) fit_sp4 &lt;- smooth.spline(Wage$age, Wage$wage, cv = T) # 同 (交差検証により自由度選択) fit_sp4$df #&gt; [1] 6.794596 pred_sp2 &lt;- predict(fit_sp2, newdata = list(age = age_grid), se = T) lines(fit_sp3, col = &quot;orange&quot;, lw = 2) lines(fit_sp4, col = &quot;lightblue&quot;, lw = 2) legend(&quot;topright&quot;, legend = c(&quot;16 DF&quot;, &quot;6.8 DF&quot;), col = c(&quot;orange&quot;, &quot;lightblue&quot;), lty = 1, lwd = 2, cex = .8) GAM # lmの実行 fit_gam1 &lt;- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage) # ns: natural cubic splineの(Bスプライン)基底行列生成 summary(fit_gam1) #&gt; #&gt; Call: #&gt; lm(formula = wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -120.513 -19.608 -3.583 14.112 214.535 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 46.949 4.704 9.980 &lt; 2e-16 *** #&gt; ns(year, 4)1 8.625 3.466 2.488 0.01289 * #&gt; ns(year, 4)2 3.762 2.959 1.271 0.20369 #&gt; ns(year, 4)3 8.127 4.211 1.930 0.05375 . #&gt; ns(year, 4)4 6.806 2.397 2.840 0.00455 ** #&gt; ns(age, 5)1 45.170 4.193 10.771 &lt; 2e-16 *** #&gt; ns(age, 5)2 38.450 5.076 7.575 4.78e-14 *** #&gt; ns(age, 5)3 34.239 4.383 7.813 7.69e-15 *** #&gt; ns(age, 5)4 48.678 10.572 4.605 4.31e-06 *** #&gt; ns(age, 5)5 6.557 8.367 0.784 0.43328 #&gt; education2. HS Grad 10.983 2.430 4.520 6.43e-06 *** #&gt; education3. Some College 23.473 2.562 9.163 &lt; 2e-16 *** #&gt; education4. College Grad 38.314 2.547 15.042 &lt; 2e-16 *** #&gt; education5. Advanced Degree 62.554 2.761 22.654 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 35.16 on 2986 degrees of freedom #&gt; Multiple R-squared: 0.293, Adjusted R-squared: 0.2899 #&gt; F-statistic: 95.2 on 13 and 2986 DF, p-value: &lt; 2.2e-16 par(mfrow = c(1, 2)) plot(fit_gam1, se = TRUE, col = &quot;orange&quot;) # GAM fit_gam3 &lt;- gam(wage ~ s(year, df = 4) + s(age, df = 5) + education, data = Wage) # s: 平滑化スプライン #head(gam::s(Wage$age, 5)) summary(fit_gam3) # --&gt; ANOVA #&gt; #&gt; Call: gam(formula = wage ~ s(year, df = 4) + s(age, df = 5) + education, #&gt; data = Wage) #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -119.43 -19.70 -3.33 14.17 213.48 #&gt; #&gt; (Dispersion Parameter for gaussian family taken to be 1235.69) #&gt; #&gt; Null Deviance: 5222086 on 2999 degrees of freedom #&gt; Residual Deviance: 3689770 on 2986 degrees of freedom #&gt; AIC: 29887.75 #&gt; #&gt; Number of Local Scoring Iterations: NA #&gt; #&gt; Anova for Parametric Effects #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; s(year, df = 4) 1 27162 27162 21.981 2.877e-06 *** #&gt; s(age, df = 5) 1 195338 195338 158.081 &lt; 2.2e-16 *** #&gt; education 4 1069726 267432 216.423 &lt; 2.2e-16 *** #&gt; Residuals 2986 3689770 1236 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Anova for Nonparametric Effects #&gt; Npar Df Npar F Pr(F) #&gt; (Intercept) #&gt; s(year, df = 4) 3 1.086 0.3537 #&gt; s(age, df = 5) 4 32.380 &lt;2e-16 *** #&gt; education #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 par(mfrow = c(1, 3)) plot(fit_gam3, se = TRUE, col = &quot;lightblue&quot;) ところで, gam()内で実行されている“平滑化スプライン”は, モデルの複雑性を示す“自由度” (関数s()の2つ目の引数で指定) の大きさで包含関係を実質的に表せることができる. すなわち, モデル間に包含関係 (ネスト, “nested”) がある場合には, 線形モデルの場合と同様に, 関数anova()を適用し, 両者のデータへの当てはまりの悪さ (乖離度) の差の有意性を調べることで, モデルを複雑にすることの妥当性について, 定量的に評価することができる. なお, モデル間に包含関係がない場合には, これまでと同様, AICやBICの大きさを比較することでモデルの選択をすれば良い (パッケージgamの使用したモデル選択については, 以下の注意事項を参照). fit_gam1 &lt;- gam(wage ~ s(age, df = 5) + education, data = Wage) fit_gam2 &lt;- gam(wage ~ year + s(age, df = 5) + education, data = Wage) anova(fit_gam1, fit_gam2, fit_gam3, test = &quot;F&quot;) #&gt; Analysis of Deviance Table #&gt; #&gt; Model 1: wage ~ s(age, df = 5) + education #&gt; Model 2: wage ~ year + s(age, df = 5) + education #&gt; Model 3: wage ~ s(year, df = 4) + s(age, df = 5) + education #&gt; Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) #&gt; 1 2990 3711731 #&gt; 2 2989 3693842 1 17889.2 14.4771 0.0001447 *** #&gt; 3 2986 3689770 3 4071.1 1.0982 0.3485661 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 AIC(fit_gam1, fit_gam2, fit_gam3) #&gt; df AIC #&gt; fit_gam1 7 29897.55 #&gt; fit_gam2 8 29885.06 #&gt; fit_gam3 8 29887.75 これより, モデル1と2の間には有意水準0.1%で差が見られるが, 2と3の間には (有意水準10%でも) 差は見られない. AICの大きさで比較した場合には, モデル2が最小である. すなわち, この3つのモデルの中では, ageのみに平滑化スプライン (自由度5) を適用したモデル2が妥当と言えそうである. # ISLRの修正: preplot.Gam → preplot tmp &lt;- preplot(fit_gam2, terms = labels(fit_gam2)) age_sm &lt;- cbind(tmp$`s(age, df = 5)`$x, tmp$`s(age, df = 5)`$y) age_sm_uniq &lt;- unique(age_sm[order(age_sm[, 1]), ]) plot(age_sm_uniq, type = &quot;l&quot;, xlab = &quot;age&quot;, ylab = &quot;s(age, df = 5)&quot;) パッケージgamの使用に関する注意点 ところで, 関数gam()は, R言語の前身であるS言語から継承されてきた古い関数であり, 関数内部における適合計算の欠点が指摘されている. その後, GAMの適合アルゴリズムは今日までに種々提案されており 次節で解説するパッケージmgcvは, 欠点を改善した それらの一つをRパッケージ化したものである. また, パッケージgamの関数gam()はクラス属性”gam”を持つオブジェクトを出力するが, これはクラス“glm”を継承していることから, 変数選択/モデル選択の際に使用する 関数anova()やAIC()の内部では, gam()の出力はクラス“glm”のオブジェクトとして処理され, それ自体が不正確性を もたらす可能性がある. したがって, gamのgam()の推定結果や, その出力オブジェクトを使ったモデルの比較や選択は信頼性にやや欠ける点に注意が必要である. 補足: **gam**内の関数`s()`は外生的に与えられる引数`df`によって固定された自由度で平滑化スプライン適合する. 一方, **mgcv**の方の`s()`はデータから平滑化パラメータを 自動的に推定しつつ, 平滑化スプライン適合する (“実質的自由度&quot; (ISLR, p.278) が推定される). そもそも, 計算アルゴリズムがより適切である**mgcv**の使用が望ましいが, さらに, もし異なる滑らかさ (平滑化構造) を持つモデル同士を比較する場合には, **gam**は公平なモデル比較をしているとは言えないため, **mgcv**の使用が良い. 5.2 パッケージmgcvの利用 交互作用項を含んだGAMを扱いたい場合は, mgcvパッケージに含まれるgam()を使用する. ※ 上述のように, 交互作用項が含まれない場合にも, 推定方法が理論的により妥当なmgcvのgam()を使用するのが望ましい. library(mgcv) ライブラリmgcv, 関数gam() - Generalized additive models with integrated smoothness estimation - usage: gam(formula, family = gaussian(), data = list(), weights = NULL, subset = NULL, na.action, offset = NULL, method = &quot;GCV.Cp&quot;, optimizer = c(&quot;outer&quot;, &quot;newton&quot;), control = list(), scale = 0, select = FALSE, knots = NULL, sp = NULL, min.sp = NULL, H = NULL, gamma = 1, fit = TRUE, paraPen = NULL, G = NULL, in.out, drop.unused.levels = TRUE, drop.intercept = NULL, discrete = FALSE, ...) - 参考サイト: https://cran.r-project.org/web/packages/mgcv/mgcv.pdf ところで, mgcvには, 先に見たgamと同一名の関数gam()が 存在することから, コンフリクトが生じている (“名前空間の衝突”と呼ぶ). Rでは, より後からlibrary()やrequire()でロードしたパッケージの関数が優先されるルールがあること. すなわち, 以下のコードでは, 特に何も指定しない限りでは, mgcv内のgam()が優先されることになる. 関数のコンフリクトを確認する方法としては, # conflicts() # 一覧 find(&quot;gam&quot;) #&gt; [1] &quot;package:mgcv&quot; &quot;package:gam&quot; # or find(&quot;gam&quot;) #&gt; [1] &quot;package:mgcv&quot; &quot;package:gam&quot; # or getAnywhere(gam) #&gt; 2 differing objects matching &#39;gam&#39; were found #&gt; in the following places #&gt; package:mgcv #&gt; package:gam #&gt; namespace:mgcv #&gt; namespace:gam #&gt; Use [] to view one of them 関数の優先順位を入れ替える方法 (パッケージgamをmgcvよりも優先する場合) としては, 再度library()を実行する: # library(mgcv) library(gam) detach()でパッケージを外す: detach(&quot;package:mgcv&quot;, unload = TRUE) 都度「パッケージ名::関数名」を使う: gam::gam(wage ~ ., data = Wage) がある. 以下では, 以前, 線形回帰の章でも使用したデータセットBostonについて分析を行う. GAMを実行する前に, lm()を走らせてみる. library(MASS) # Bostonデータセット fit_lm &lt;- lm(medv ~ tax + crim + age + ptratio + nox + ptratio + dis, data = Boston) summary(fit_lm) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ tax + crim + age + ptratio + nox + ptratio + #&gt; dis, data = Boston) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.111 -4.661 -1.301 2.431 32.135 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.609e+01 4.480e+00 19.217 &lt; 2e-16 *** #&gt; tax 2.793e-04 3.023e-03 0.092 0.92641 #&gt; crim -1.754e-01 4.448e-02 -3.944 9.15e-05 *** #&gt; age -5.282e-02 1.800e-02 -2.935 0.00349 ** #&gt; ptratio -1.794e+00 1.669e-01 -10.751 &lt; 2e-16 *** #&gt; nox -3.610e+01 5.188e+00 -6.960 1.08e-11 *** #&gt; dis -1.654e+00 2.549e-01 -6.488 2.10e-10 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 6.946 on 499 degrees of freedom #&gt; Multiple R-squared: 0.4365, Adjusted R-squared: 0.4297 #&gt; F-statistic: 64.42 on 6 and 499 DF, p-value: &lt; 2.2e-16 まず, mgcvのgam()を実行する. この関数は, gamのgam()より優先されるため何も指定しなくても そのまま呼び出されるが, コードの可読性を高めるため, ここでは敢えてmgcv::gam()と書くことにする. fit_gam &lt;- mgcv::gam(medv ~ s(tax) + s(crim) + s(age) + s (ptratio) + s(nox) + s(dis), data = Boston) plot(fit_gam, residuals = T, pch = 1, se = T, cex.lab = 1.3) # --&gt; 各変数の非線形性 なお, mgcv::gam()内の平滑化スプライン実行関数s()は mgcv内の関数である. # 交互作用を含めたGAM fit_gam &lt;- mgcv::gam(medv ~ s(ptratio) + s(rm) + s(ptratio, rm), data = Boston) vis.gam(fit_gam, color = &quot;topo&quot;, theta = 50) # 3次元プロット (デフォルト) vis.gam(fit_gam, color = &quot;topo&quot;, plot.type = &quot;contour&quot;) # 輪郭プロット # --&gt; ptratioとrmのheatmap summary(fit_gam) #&gt; #&gt; Family: gaussian #&gt; Link function: identity #&gt; #&gt; Formula: #&gt; medv ~ s(ptratio) + s(rm) + s(ptratio, rm) #&gt; #&gt; Parametric coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.5328 0.2277 98.94 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Approximate significance of smooth terms: #&gt; edf Ref.df F p-value #&gt; s(ptratio) 3.596 4.016 0.466 0.747 #&gt; s(rm) 7.200 8.174 6.460 &lt;2e-16 *** #&gt; s(ptratio,rm) 10.595 27.000 1.668 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; R-sq.(adj) = 0.69 Deviance explained = 70.3% #&gt; GCV = 27.457 Scale est. = 26.242 n = 506 # 交互作用を含めたGAM-2 fit_gam &lt;- mgcv::gam(medv ~ s(lstat) + s(rm) + s(lstat, rm), data = Boston) vis.gam(fit_gam, color = &quot;topo&quot;, theta = 50) # 3次元プロット (デフォルト) vis.gam(fit_gam, color = &quot;topo&quot;, plot.type = &quot;contour&quot;) # 輪郭プロット summary(fit_gam) #&gt; #&gt; Family: gaussian #&gt; Link function: identity #&gt; #&gt; Formula: #&gt; medv ~ s(lstat) + s(rm) + s(lstat, rm) #&gt; #&gt; Parametric coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.5328 0.1845 122.1 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Approximate significance of smooth terms: #&gt; edf Ref.df F p-value #&gt; s(lstat) 2.312 2.372 1.586 0.248 #&gt; s(rm) 8.386 8.870 6.466 &lt; 2e-16 *** #&gt; s(lstat,rm) 17.427 27.000 1.373 2.93e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; R-sq.(adj) = 0.796 Deviance explained = 80.8% #&gt; GCV = 18.273 Scale est. = 17.221 n = 506 # 交互作用を含めたGAM-3 fit_gam &lt;- mgcv::gam(medv ~ s(crim) + s(rm) + s(crim, rm), data = Boston) vis.gam(fit_gam, color = &quot;topo&quot;, theta = 50) # 3次元プロット (デフォルト) vis.gam(fit_gam, color = &quot;topo&quot;, plot.type = &quot;contour&quot;) # 輪郭プロット summary(fit_gam) #&gt; #&gt; Family: gaussian #&gt; Link function: identity #&gt; #&gt; Formula: #&gt; medv ~ s(crim) + s(rm) + s(crim, rm) #&gt; #&gt; Parametric coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.5328 0.2144 105.1 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Approximate significance of smooth terms: #&gt; edf Ref.df F p-value #&gt; s(crim) 1.724 2.270 0.039 0.968546 #&gt; s(rm) 6.805 8.026 3.848 0.000187 *** #&gt; s(crim,rm) 27.000 27.000 2.869 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; R-sq.(adj) = 0.725 Deviance explained = 74.4% #&gt; GCV = 25.063 Scale est. = 23.254 n = 506 # 交互作用を含めたGAM-4 fit_gam &lt;- mgcv::gam(medv ~ s(dis) + s(rm) + s(dis, rm), data = Boston) vis.gam(fit_gam, color = &quot;topo&quot;, theta = 50) vis.gam(fit_gam, color = &quot;topo&quot;, plot.type = &quot;contour&quot;) summary(fit_gam) #&gt; #&gt; Family: gaussian #&gt; Link function: identity #&gt; #&gt; Formula: #&gt; medv ~ s(dis) + s(rm) + s(dis, rm) #&gt; #&gt; Parametric coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.5328 0.2321 97.07 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Approximate significance of smooth terms: #&gt; edf Ref.df F p-value #&gt; s(dis) 6.054 7.471 1.577 0.128824 #&gt; s(rm) 8.233 8.765 3.315 0.000455 *** #&gt; s(dis,rm) 27.000 27.000 2.370 1.99e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; R-sq.(adj) = 0.678 Deviance explained = 70.4% #&gt; GCV = 29.749 Scale est. = 27.263 n = 506 線形モデルでは捉えることのできなかた変数ペア間の非線形な関係性を表現している. 注意点としては, データが少ない, あるいは存在しない領域まで面を描いてしまう可能性のあることから, 誤った解釈を加えることのないようにしなければならない. fit_gam0 &lt;- mgcv::gam(medv ~ s(ptratio) + s(rm), data = Boston) # 交互作用項を持たないモデル summary(fit_gam0) #&gt; #&gt; Family: gaussian #&gt; Link function: identity #&gt; #&gt; Formula: #&gt; medv ~ s(ptratio) + s(rm) #&gt; #&gt; Parametric coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.5328 0.2363 95.34 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Approximate significance of smooth terms: #&gt; edf Ref.df F p-value #&gt; s(ptratio) 6.312 7.357 16.21 &lt;2e-16 *** #&gt; s(rm) 8.047 8.759 61.38 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; R-sq.(adj) = 0.666 Deviance explained = 67.5% #&gt; GCV = 29.149 Scale est. = 28.264 n = 506 anova(fit_gam, fit_gam0) # 注) test = &quot;F&quot;/&quot;Chisq&quot;は機能しない (← 不要) #&gt; Analysis of Deviance Table #&gt; #&gt; Model 1: medv ~ s(dis) + s(rm) + s(dis, rm) #&gt; Model 2: medv ~ s(ptratio) + s(rm) #&gt; Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) #&gt; 1 461.76 12642 #&gt; 2 488.88 13867 -27.121 -1225.3 1.6571 0.02125 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 AIC(fit_gam, fit_gam0) #&gt; df AIC #&gt; fit_gam 43.28713 3150.978 #&gt; fit_gam0 16.35950 3143.930 (自主課題) 以上のGAMの実行結果を解釈してみよう. 他の変数ペアについても調べてみよう. 5.3 パッケージprophetの利用 時系列予測を目的とした関数および補助関数のパッケージ. prophet: - 非線形のトレンドに年次・週次・日次の季節性, さらに 休日効果を加えた加法モデルに基づいて時系列データを予測 - 強い季節性があり, 数シーズンの過去データを持つ時系列データに対して良く機能 - 欠損値やトレンドのシフトに対して頑強. 通常, 外れ値をうまく処理 - 出所: https://facebook.github.io/prophet/docs/quick_start.html#r-api #install.packages(&quot;prophet&quot;) library(prophet) library(zoo) # index, yearmon air &lt;- AirPassengers # Pan Am, # international passenger bokking (in 1000s) per month # 1949--1960 (Brown, 1963) class(air) #&gt; [1] &quot;ts&quot; air #&gt; Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec #&gt; 1949 112 118 132 129 121 135 148 148 136 119 104 118 #&gt; 1950 115 126 141 135 125 149 170 170 158 133 114 140 #&gt; 1951 145 150 178 163 172 178 199 199 184 162 146 166 #&gt; 1952 171 180 193 181 183 218 230 242 209 191 172 194 #&gt; 1953 196 196 236 235 229 243 264 272 237 211 180 201 #&gt; 1954 204 188 235 227 234 264 302 293 259 229 203 229 #&gt; 1955 242 233 267 269 270 315 364 347 312 274 237 278 #&gt; 1956 284 277 317 313 318 374 413 405 355 306 271 306 #&gt; 1957 315 301 356 348 355 422 465 467 404 347 305 336 #&gt; 1958 340 318 362 348 363 435 491 505 404 359 310 337 #&gt; 1959 360 342 406 396 420 472 548 559 463 407 362 405 #&gt; 1960 417 391 419 461 472 535 622 606 508 461 390 432 # ここでは, 生データのまま使用 (対数変換せず) # 年月の取り出し tt &lt;- as.Date(yearmon(index(air))) # prophetモデルの生成 air_df &lt;- data.frame(ds = tt, y = air) air_ppht &lt;- prophet(air_df) # 予測年月の生成 dates_ft &lt;- make_future_dataframe(air_ppht, periods = 12, freq = &quot;month&quot;) tail(dates_ft) #&gt; ds #&gt; 151 1961-07-01 #&gt; 152 1961-08-01 #&gt; 153 1961-09-01 #&gt; 154 1961-10-01 #&gt; 155 1961-11-01 #&gt; 156 1961-12-01 # 予測値の生成 air_forecast &lt;- predict(air_ppht, dates_ft) tail(air_forecast[c(&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;)]) #&gt; ds yhat yhat_lower yhat_upper #&gt; 151 1961-07-01 576.3980 546.4439 605.7250 #&gt; 152 1961-08-01 576.8090 548.1246 604.5037 #&gt; 153 1961-09-01 528.3037 499.4033 557.7264 #&gt; 154 1961-10-01 493.0463 465.2631 520.4769 #&gt; 155 1961-11-01 459.2066 431.9837 488.5751 #&gt; 156 1961-12-01 488.5528 460.9132 518.6747 str(air_forecast) #&gt; &#39;data.frame&#39;: 156 obs. of 16 variables: #&gt; $ ds : POSIXct, format: &quot;1949-01-01&quot; &quot;1949-02-01&quot; ... #&gt; $ trend : num 106 108 110 113 115 ... #&gt; $ additive_terms : num -21.951 -30.725 -0.498 -5.184 -3.774 ... #&gt; $ additive_terms_lower : num -21.951 -30.725 -0.498 -5.184 -3.774 ... #&gt; $ additive_terms_upper : num -21.951 -30.725 -0.498 -5.184 -3.774 ... #&gt; $ yearly : num -21.951 -30.725 -0.498 -5.184 -3.774 ... #&gt; $ yearly_lower : num -21.951 -30.725 -0.498 -5.184 -3.774 ... #&gt; $ yearly_upper : num -21.951 -30.725 -0.498 -5.184 -3.774 ... #&gt; $ multiplicative_terms : num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ multiplicative_terms_lower: num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ multiplicative_terms_upper: num 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ yhat_lower : num 56.6 50.3 81.2 77.9 81.4 ... #&gt; $ yhat_upper : num 112 105 139 137 140 ... #&gt; $ trend_lower : num 106 108 110 113 115 ... #&gt; $ trend_upper : num 106 108 110 113 115 ... #&gt; $ yhat : num 84.3 77.7 109.9 107.4 110.9 ... plot(air_ppht, air_forecast) # prophetによる予測の各成分のプロット prophet_plot_components(air_ppht, air_forecast) prophetの主な引数 - growth: &quot;linear&quot;(デフォルト), &quot;logisitc&quot;, &quot;flat&quot; - changepoints (変化点): 日付ベクトルをユーザー指定 or 潜在的な変化点の自動選択(デフォルト) - n.changepoints (変化点の数): 25 (デフォルト) - yearly.seasonality (年次季節性への適合): &quot;auto&quot;(デフォルト), T, F, 生成するFourier項の数 - weekly.seasonality (週次季節性への適合): 同上 - daily.seasonality (週次季節性への適合): 同上 - holidays (休日の指定): なし(デフォルト - seasonality.mode (季節性の入り方): &quot;additive&quot;(加法的)(デフォルト), &quot;multiplictive&quot;(乗法的) 等 5.4 【追加分析】Boston housingデータセットの変数間の関係性 ※ 以下のRコードの出所: ChatGPT (GPT-4o) (一部修正) library(ggplot2) # データ確認 data(Boston) # chasを因子型に変換（形状の制御に便利） Boston$chas &lt;- as.factor(Boston$chas) # プロット作成 ggplot(Boston, aes(x = dis, y = rm, size = medv, shape = chas, color = rad)) + geom_point(alpha = 0.7) + scale_size_continuous(name = &quot;住宅価格中央値 (medv)&quot;, range = c(1, 10)) + scale_shape_manual(values = c(16, 17), labels = c(&quot;川沿いでない&quot;, &quot;川沿い&quot;)) + scale_color_gradient(low = &quot;navy&quot;, high = &quot;orange&quot;) + labs( title = &quot;住宅の平均部屋数と雇用中心地距離・価格・立地・高速道路アクセスの関係&quot;, x = &quot;雇用中心地への距離 (dis)&quot;, y = &quot;住宅の平均部屋数 (rm)&quot;, shape = &quot;チャールズ川沿い&quot;, color = &quot;高速道路アクセス (rad)&quot; ) + theme_minimal() # プロット作成 ggplot(Boston, aes(x = dis, y = medv, size = rm, shape = chas, color = rad)) + geom_point(alpha = 0.7) + scale_size_continuous(name = &quot;平均部屋数 (rm)&quot;) + scale_shape_manual(values = c(16, 17), labels = c(&quot;川沿いでない&quot;, &quot;川沿い&quot;)) + scale_color_gradient(low = &quot;navy&quot;, high = &quot;orange&quot;) + labs( title = &quot;住宅価格と雇用中心地距離・部屋数・立地・高速道路アクセスの関係&quot;, x = &quot;雇用中心地への距離 (dis)&quot;, y = &quot;住宅価格中央値 (medv, $1000単位)&quot;, shape = &quot;チャールズ川沿い&quot;, color = &quot;高速道路アクセス (rad)&quot; ) + theme_minimal() "],["正則化回帰-lasso-ridge回帰-elastic-net.html", "6 正則化回帰 (lasso, ridge回帰, elastic net) 6.1 検証用セット法による回帰モデルの予測精度評価 6.2 lasso 6.3 ridge回帰 6.4 一般化線形モデル (GLM) + 正則化 6.5 補足: GLMに対する変数選択の代替的方法", " 6 正則化回帰 (lasso, ridge回帰, elastic net) 本章では, Rパッケージglmnetを使用する. 同パッケージは, Lasso（Least Absolute Shrinkage and Selection Operator） やその拡張 (Elastic Net等) の理論研究や応用を推進した Jerome Friedman, Trevor Hastie, Robert Tibshirani らによって開発され, 維持されている. glmnetに関する有用情報マニュアル, 解説ページ: - https://glmnet.stanford.edu/articles/glmnet.html. - https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf ※ 本セクションにおける分析フローは, 主としてISLR, Ch.6を参考にした. 6.1 検証用セット法による回帰モデルの予測精度評価 データセット1: ワイン品質データ - winequality-white.csv - fixed acidity: 酢酸濃度 - volitle acidity: 揮発酸濃度 - citric acidity: クエン酸濃度 - chlorides: 塩化物 - sulfur dioxide: 二酸化硫黄 - sulphate: 硫酸塩 - fixed acidity: 酒石酸含有量（g/dm3) - volatile acidity: 酢酸含有量（g/dm3) - citric acid: クエン酸含有量（g/dm3) - residual sugar: 残留糖分含有量（g/dm3） - chlorides: 塩化ナトリウム含有量（g/dm3) - free sulfur dioxide: 遊離亜硫酸含有量（mg/dm3） - total sulfur dioxide: 総亜硫酸含有量（mg/dm3） - density: 密度（g/dm3) - pH: pH - sulphates: 硫酸カリウム含有量（g/dm3） - alcohol: アルコール度数（% vol.） - quality: ワインの品質 (0 (very bad) -- 10 (excellent)) wine &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) head(wine) #&gt; fixed.acidity volatile.acidity citric.acid residual.sugar chlorides #&gt; 1 7.0 0.27 0.36 20.7 0.045 #&gt; 2 6.3 0.30 0.34 1.6 0.049 #&gt; 3 8.1 0.28 0.40 6.9 0.050 #&gt; 4 7.2 0.23 0.32 8.5 0.058 #&gt; 5 7.2 0.23 0.32 8.5 0.058 #&gt; 6 8.1 0.28 0.40 6.9 0.050 #&gt; free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol #&gt; 1 45 170 1.0010 3.00 0.45 8.8 #&gt; 2 14 132 0.9940 3.30 0.49 9.5 #&gt; 3 30 97 0.9951 3.26 0.44 10.1 #&gt; 4 47 186 0.9956 3.19 0.40 9.9 #&gt; 5 47 186 0.9956 3.19 0.40 9.9 #&gt; 6 30 97 0.9951 3.26 0.44 10.1 #&gt; quality #&gt; 1 6 #&gt; 2 6 #&gt; 3 6 #&gt; 4 6 #&gt; 5 6 #&gt; 6 6 検証用セット法 (validation set approach) データセットを学習用と検証用にランダムに分割 学習用データセットで学習 (線形回帰モデルへの適合) 検証用データセットで予測 予測MSE (平均2乗誤差) を計算 # データセットを学習用と検証用にランダムに分割 set.seed(1) train &lt;- sample(1:nrow(wine), 3000) # wine_train &lt;- wine[train,] # 学習用データセット, 3000 # wine_test &lt;- wine[-train,] # テスト用データセット, 1898 # 学習用データセットで学習 (線形回帰モデルへの適合) lm_mod &lt;- lm(quality ~ ., data = wine[train, ]) # 検証用データセットで予測 lm_pred &lt;- predict(lm_mod, newdata = wine[-train, -12]) # 予測MSE (平均2乗誤差) の計算 mean((lm_pred - wine[-train, 12])^2) #&gt; [1] 0.5657141 # 回帰結果 summary(lm_mod) #&gt; #&gt; Call: #&gt; lm(formula = quality ~ ., data = wine[train, ]) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.2676 -0.4885 -0.0412 0.4639 2.9119 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.594e+02 2.202e+01 7.236 5.84e-13 *** #&gt; fixed.acidity 6.054e-02 2.524e-02 2.399 0.0165 * #&gt; volatile.acidity -1.834e+00 1.446e-01 -12.684 &lt; 2e-16 *** #&gt; citric.acid -2.951e-02 1.208e-01 -0.244 0.8070 #&gt; residual.sugar 8.795e-02 9.123e-03 9.640 &lt; 2e-16 *** #&gt; chlorides -1.260e-01 6.522e-01 -0.193 0.8468 #&gt; free.sulfur.dioxide 4.523e-03 1.082e-03 4.181 2.98e-05 *** #&gt; total.sulfur.dioxide -1.199e-04 4.791e-04 -0.250 0.8024 #&gt; density -1.596e+02 2.235e+01 -7.141 1.16e-12 *** #&gt; pH 6.627e-01 1.303e-01 5.085 3.90e-07 *** #&gt; sulphates 6.331e-01 1.285e-01 4.925 8.90e-07 *** #&gt; alcohol 2.002e-01 2.858e-02 7.006 3.03e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.7518 on 2988 degrees of freedom #&gt; Multiple R-squared: 0.2953, Adjusted R-squared: 0.2927 #&gt; F-statistic: 113.8 on 11 and 2988 DF, p-value: &lt; 2.2e-16 6.2 lasso 関数glmnet()の基本操作 glmnet(): 罰則付き最尤法: GLM推定 + elastic netによる正則化 - usage: glmnet(x, y, family = c(&quot;gaussian&quot;, &quot;binomial&quot;, &quot;poisson&quot;, &quot;multinomial&quot;, &quot;cox&quot;, &quot;mgaussian&quot;), ...) - 最重要パラメータ - alpha: elastic net混合パラメータ (デフォルト=1) - lambda: 正則化パラメータ (デフォルトは自動的に100個設定) library(glmnet) 簡便のため, glmnetに入力するため, 一旦, 特徴量行列 x と 目的変数ベクトル y を別々のオブジェクトとして用意しておく. x &lt;- as.matrix(wine[, 1:11]) y &lt;- as.matrix(wine[, 12]) lassoを実行するには, glmnet()において, 引数alpha = 1と設定する. \\(\\alpha\\)は, lasso, ridge回帰 (後述) を包括するクラスである elastic netのパラメータであり, 両者の正規化項の結合ウェイトを示す (\\(\\alpha=1\\)がlasso, \\(\\alpha=0\\)がridge回帰に対応). 特定のlambdaの値二つに対して実行し, 結果を比較してみる. # LASSO (alpha = 1) # lambda = L1-penalty項 lasso_mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = 0.1) coef(lasso_mod) #&gt; 12 x 1 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; s0 #&gt; (Intercept) 3.5648570 #&gt; fixed.acidity . #&gt; volatile.acidity -0.9333891 #&gt; citric.acid . #&gt; residual.sugar . #&gt; chlorides . #&gt; free.sulfur.dioxide . #&gt; total.sulfur.dioxide . #&gt; density . #&gt; pH . #&gt; sulphates . #&gt; alcohol 0.2443784 #lasso_mod$beta # y切片無し lasso_mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = 0.01) # lambda小(penalty小) --&gt; 係数 = 0少 coef(lasso_mod) #&gt; 12 x 1 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; s0 #&gt; (Intercept) 74.381719017 #&gt; fixed.acidity -0.002299923 #&gt; volatile.acidity -1.812108243 #&gt; citric.acid . #&gt; residual.sugar 0.050598423 #&gt; chlorides -0.386260201 #&gt; free.sulfur.dioxide 0.004109954 #&gt; total.sulfur.dioxide . #&gt; density -73.059537698 #&gt; pH 0.311903370 #&gt; sulphates 0.412510523 #&gt; alcohol 0.284548889 最初の例 (lambda=0.1), すなわち, 正則化項の係数が相対的に大きい (罰則大) 時, 回帰係数の殆どの値がゼロ (変数選択されない). 他方, 二番目の例 (lambda=0.01), すなわち, 正則化項の係数が相対的に小さい時, 多くの回帰係数の値が非ゼロとなっている. lambdaのグリッド指定によるglmnetの一括実行 正則化パラメータ\\(\\lambda\\)は, 数値ベクトル として関数glmnet()に与えることもできる. すると, 各lambdaの値に対してglmnet()が個別に実行され, それらが一括して戻ってくる. 以下では, \\(10^1\\)から\\(10^{-3}\\)まで100個の点からなるグリッドを取り, 関数glmnet()に与える例である. # lambdaのグリッド指定 # grid &lt;- 10^seq(10, -2, length = 100) # lambda = 10^10 -- 10^(-2) grid &lt;- 10^seq(1, -3, length = 100) # lambda = 10^1 -- 10^(-3) # 範囲設定されたlambaの各値に対してglmnetを実行 lasso_mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = grid) # 結果オブジェクトの大きさ (次元) dim(coef(lasso_mod)) # 12 rows --&gt; 説明変数+y切片, 100 columns --&gt; lambdaの値 #&gt; [1] 12 100 dim(lasso_mod$beta) # 11 rows --&gt; interceptがない #&gt; [1] 11 100 戻ってきたオブジェクトから, 個々のlambda値に対する実行結果を取り出すことができる. 以下, 50番目. 90番目のグリッド点における結果を取り出す例を示す. # 50番目のlambdaの値 lasso_mod$lambda[50] # --&gt; 0.1047616 #&gt; [1] 0.1047616 # 回帰係数推定値 coef(lasso_mod)[, 50] #&gt; (Intercept) fixed.acidity volatile.acidity #&gt; 3.5943708 0.0000000 -0.8843771 #&gt; citric.acid residual.sugar chlorides #&gt; 0.0000000 0.0000000 0.0000000 #&gt; free.sulfur.dioxide total.sulfur.dioxide density #&gt; 0.0000000 0.0000000 0.0000000 #&gt; pH sulphates alcohol #&gt; 0.0000000 0.0000000 0.2402657 lasso_mod$beta[, 50] # 代替的方法 #&gt; fixed.acidity volatile.acidity citric.acid #&gt; 0.0000000 -0.8843771 0.0000000 #&gt; residual.sugar chlorides free.sulfur.dioxide #&gt; 0.0000000 0.0000000 0.0000000 #&gt; total.sulfur.dioxide density pH #&gt; 0.0000000 0.0000000 0.0000000 #&gt; sulphates alcohol #&gt; 0.0000000 0.2402657 # 回帰係数推定値のノルムの大きさ # sqrt(sum(coef(lasso_mod)[-1, 50]^2)) # --&gt; L2-norm sum(abs(coef(lasso_mod)[-1, 50])) # --&gt; L1-norm #&gt; [1] 1.124643 # 90番目のlambdaの値の例 lasso_mod$lambda[90] # --&gt; 0.002535364 #&gt; [1] 0.002535364 # 回帰係数推定値 coef(lasso_mod)[, 90] #&gt; (Intercept) fixed.acidity volatile.acidity #&gt; 1.285160e+02 2.870680e-02 -1.839954e+00 #&gt; citric.acid residual.sugar chlorides #&gt; -8.511098e-03 7.470412e-02 -2.656102e-01 #&gt; free.sulfur.dioxide total.sulfur.dioxide density #&gt; 4.331940e-03 -3.401243e-05 -1.280749e+02 #&gt; pH sulphates alcohol #&gt; 5.201851e-01 5.621013e-01 2.323178e-01 # 回帰係数推定値のノルムの大きさ #sqrt(sum(coef(lasso_mod)[-1, 90]^2)) # --&gt; L2-norm sum(abs(coef(lasso_mod)[-1, 90])) # --&gt; L1-norm #&gt; [1] 131.6114 lambda値大 (罰則大) → 回帰係数推定値の大きさ (L1ノルムで評価) 小, となっている 学習結果を使い, グリッド上にない新しいlambda値 (例, 0.05) に対する回帰係数の予測をすることもできる. # 新しいlambda値 (例, 0.05) に対する回帰係数の予測 predict(lasso_mod, s = 0.05, type = &quot;coefficients&quot;) #&gt; 12 x 1 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; s0 #&gt; (Intercept) 3.037559871 #&gt; fixed.acidity -0.021059990 #&gt; volatile.acidity -1.453951512 #&gt; citric.acid . #&gt; residual.sugar 0.007980186 #&gt; chlorides . #&gt; free.sulfur.dioxide 0.002465338 #&gt; total.sulfur.dioxide . #&gt; density . #&gt; pH . #&gt; sulphates . #&gt; alcohol 0.309041958 指定したグリッド範囲において, 回帰係数推定値の変化を示す 正則化パス (回帰係数推定値 vs log(lambda), vs L1ノルム) はplot()により描くことができる. plot(lasso_mod) # (デフォルト) 回帰係数推定値 vs log(lambda) plot(lasso_mod, xvar = &quot;norm&quot;) # 回帰係数推定値 vs L1ノルム # テストデータを使った予測, および予測MSEの計算 lasso_pred &lt;- predict(lasso_mod, s = 0.05, newx = x[-train, ]) # lambda = 0.05 mean((lasso_pred - y[-train])^2) # test MSE #&gt; [1] 0.5771119 交差検証によるパラメータ・チューニング 以上のようなlambdaのグリッドに対して実行したglmnetの結果より, 関数cv.glmnet()は, 交差検証 (cross-validation) を使って, 正則化パラメータ\\(\\lambda\\)の”最適な”値を見つけることができる. - cv.glmnet(): k-重交差検証を実行. 最適なλの値を返す. # 交差検証 (Cross-validation) によるlambda値のチューニング set.seed(2) cv_out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1) # デフォルト: 10-fold CV (nfoldsで指定) cv_out &lt;- cv.glmnet(x[train, ], y[train], lambda = grid, nfolds = 5, alpha = 1) # lambdaは上のgrid使用, 5-fold CV # オプション # lambda: Optional user - supplied lambda sequence; default is NULL, and glmnet chooses its own sequence plot(cv_out) “最適な”\\(\\lambda\\)の決め方の一つは, CV誤差 (予測MSE推定値) が最小となる \\(\\lambda\\)値であり, これはcv.glmnet()の出力オブジェクト内の 要素$lambda.minに格納されている. この$lambda.minをpredict()の引数sに与えることで, 対応するlassoのモデルを使った外挿予測をすることができる. # CV誤差 (予測MSE推定値) が最小 (CV_min) となるlambda (lmbd_min &lt;- cv_out$lambda.min) #&gt; [1] 0.007054802 # このlambdaを持つモデルによる予測, 予測MSE lasso_pred &lt;- predict(lasso_mod, s = lmbd_min, newx = x[-train, ]) mean((lasso_pred - y[-train])^2) # 予測MSE #&gt; [1] 0.5647419 代替的な\\(\\lambda\\)の決め方として慣用的によく用いられる方法に, “1標準誤差ルール (1 standard error rule)”がある. これは, CV誤差 (予測MSE推定値) の最小値 (\\(CV_{min}\\)), その時の標準誤差\\(se_{min}\\)に対して, \\(\\lambda\\)における予測MSE \\(CV_{\\lambda}\\)が \\(CV_{\\lambda} \\le CV_{min} + se_{min}\\) となる\\(\\lambda\\)値の中で 最大の\\(\\lambda\\)を選択するものである. CV誤差最小となる\\(\\lambda\\)よりも大きな\\(\\lambda\\)を 取ることで, 選択される変数の数が減り, よりシンプルなモデルが最終モデルとして選ばれるようになる. # 1標準誤差ルール (1 standard error rule) によるlambda: # CV誤差が CV &lt;= CV_min + se_min となる最大のlambda (lmbd_1se &lt;- cv_out$lambda.1se) #&gt; [1] 0.03125716 # このlambdaを持つモデルによる予測, 予測MSE lasso_pred2 &lt;- predict(lasso_mod, s = lmbd_1se, newx = x[-train, ]) mean((lasso_pred2 - y[-train])^2) #&gt; [1] 0.5702662 以上の, cv.glmnet()による出力の確認. # 上記2つの&quot;最適な&quot;lambdaのインデックス cv_out$index # min, 1seの順 #&gt; Lambda #&gt; min 79 #&gt; 1se 63 # cv_out$cvm # (予測MSEの) CV推定値 # cv_out$cvsd # cvmの標準誤差 (cvmse_1se &lt;- cv_out$cvm[cv_out$index[&quot;min&quot;, ]] + cv_out$cvsd[cv_out$index[&quot;min&quot;, ]]) #&gt; [1] 0.5872448 which.max(cv_out$cvm &lt;= cvmse_1se) #&gt; [1] 63 # 最適なlambda値の対数 → CV誤差曲線 (上図) 内の2本の点線の位置 log(cv_out$lambda.min) #&gt; [1] -4.954047 log(cv_out$lambda.1se) #&gt; [1] -3.465507 CV誤差曲線 (上図) において, 2本の縦線が引かれているが, 左側の線はCV誤差最小点の位置log(cv_out$lambda.min), 右側の線は”1標準誤差ルール”による点の位置 log(cv_out$lambda.1se)を表している. 6.3 ridge回帰 ridge回帰は, 関数glmnet()において, 引数alpha=0を指定すれば良い. その他の実行方法や結果の見方等は, 上記lassoのケースと全く同様である. # Ridge regression (alpha = 0) ridge_mod &lt;- glmnet(x[train, ], y[train], alpha = 0, lambda = 0.1) ridge_mod$beta #&gt; 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; s0 #&gt; fixed.acidity -2.633821e-02 #&gt; volatile.acidity -1.639141e+00 #&gt; citric.acid -5.476896e-03 #&gt; residual.sugar 3.694013e-02 #&gt; chlorides -1.399251e+00 #&gt; free.sulfur.dioxide 4.920892e-03 #&gt; total.sulfur.dioxide -5.358797e-04 #&gt; density -5.165966e+01 #&gt; pH 2.494285e-01 #&gt; sulphates 4.307148e-01 #&gt; alcohol 2.645255e-01 ridge_mod &lt;- glmnet(x[train, ], y[train], alpha = 0, lambda = 0.01) ridge_mod$beta #&gt; 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; s0 #&gt; fixed.acidity 2.667047e-02 #&gt; volatile.acidity -1.834882e+00 #&gt; citric.acid -3.723824e-02 #&gt; residual.sugar 7.023424e-02 #&gt; chlorides -4.222285e-01 #&gt; free.sulfur.dioxide 4.800634e-03 #&gt; total.sulfur.dioxide -2.510749e-04 #&gt; density -1.172285e+02 #&gt; pH 5.021875e-01 #&gt; sulphates 5.659802e-01 #&gt; alcohol 2.410154e-01 先のlassoの時と異なり, 明らかに, どの回帰係数も非ゼロの値にとどまっている. また, 理論が示す通り, \\(\\lambda=0.1\\)の時の方が, 各係数の絶対値がゼロに近付いことが確認される 最後に, 上と同様, lambdaの範囲を指定し, 正則化パスを描いてみよう. # lambdaのグリッド指定 grid &lt;- 10^seq(1, -3, length = 100) # lambda = 10^1 -- 10^(-3) # 範囲設定されたlambaの各値に対してglmnetを実行 ridge_mod &lt;- glmnet(x[train, ], y[train], alpha = 0, lambda = grid) plot(ridge_mod) # (デフォルト) 回帰係数推定値 vs log(lambda) # plot(ridge_mod, xvar = &quot;norm&quot;) # 回帰係数推定値 vs L1ノルム (注: L2ノルムでない) 6.4 一般化線形モデル (GLM) + 正則化 関数glment()はその名が示すように, 線形回帰にとどまらず, 一般化線形モデル (GLM) に対しても, elastic net (lasso, ridge回帰) を実行することができる. 引数familyをデフォルト (gaussian) 以外の 適切な確率分布を指定する. 選択可能なクラスとして, family = c(&quot;gaussian&quot;, &quot;binomial&quot;, &quot;poisson&quot;, &quot;multinomial&quot;, &quot;cox&quot;, &quot;mgaussian&quot;) また. glmnet()は使用上, 通常の統計解析実行用のR関数, 例えばlm()やglm()等とは異なるクセのあることに注意が必要である. glmnet()実行上の注意点 - 欠損値(NA)の事前処理が必要; 除去 or 補間 - 通常のy~x モデル式の入力ではなく, matrixクラス x, vectorクラス yを引数に取る - 説明変数は数値変数のみ (xは行列) → 質的変数/factorの数量化が必要 (ダミー変数導入) データセット2: german creditデータ german_credit_modified.csv - 予測変数 (20変数): StatusAccount, DurationMonth, CreditHistory, Purpose, CreditAmount, SavingsAccount, EmploymentSince, InstallmentRate, StatusAndSex, Guarantors, ResidenceSince, Property, Age, InstallmentPlans, Housing, NCredits, Job, NPeopleMain, Phone, ForeignWorker - 目的変数: Customer [good/bad] - オリジナルデータの出所: (http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)[http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/] # 例. credit_dat &lt;- read.csv(&quot;german_credit_modified.csv&quot;, skip = 3, header = T) str(credit_dat) #&gt; &#39;data.frame&#39;: 1000 obs. of 21 variables: #&gt; $ StatusAccount : chr &quot;lt_0DM&quot; &quot;lt_200DM&quot; &quot;none&quot; &quot;lt_0DM&quot; ... #&gt; $ DurationMonth : int 6 48 12 42 24 36 24 36 12 30 ... #&gt; $ CreditHistory : chr &quot;E&quot; &quot;C&quot; &quot;E&quot; &quot;C&quot; ... #&gt; $ Purpose : chr &quot;TV_etc&quot; &quot;TV_etc&quot; &quot;education&quot; &quot;TV_etc&quot; ... #&gt; $ CreditAmount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... #&gt; $ SavingsAccount : chr NA &quot;lt_100DM&quot; &quot;lt_100DM&quot; &quot;lt_100DM&quot; ... #&gt; $ EmploymentSince : chr &quot;ge_7yrs&quot; &quot;lt_4yrs&quot; &quot;lt_7yrs&quot; &quot;lt_7yrs&quot; ... #&gt; $ InstallmentRate : int 4 2 2 2 3 2 3 2 2 4 ... #&gt; $ StatusAndSex : chr &quot;M_single&quot; &quot;F_divorced&quot; &quot;M_single&quot; &quot;M_single&quot; ... #&gt; $ Guarantors : chr &quot;none&quot; &quot;none&quot; &quot;none&quot; &quot;guarantor&quot; ... #&gt; $ ResidenceSince : int 4 2 3 4 4 4 4 2 4 2 ... #&gt; $ Property : chr &quot;real_estate&quot; &quot;real_estate&quot; &quot;real_estate&quot; &quot;life_insurance&quot; ... #&gt; $ Age : int 67 22 49 45 53 35 53 35 61 28 ... #&gt; $ InstallmentPlans: chr &quot;none&quot; &quot;none&quot; &quot;none&quot; &quot;none&quot; ... #&gt; $ Housing : chr &quot;own&quot; &quot;own&quot; &quot;own&quot; &quot;free&quot; ... #&gt; $ NCredits : int 2 1 1 1 2 1 1 1 1 2 ... #&gt; $ Job : chr &quot;skilled&quot; &quot;skilled&quot; &quot;unskilled&quot; &quot;skilled&quot; ... #&gt; $ NPeopleMain : int 1 1 2 2 2 2 1 1 1 1 ... #&gt; $ Phone : chr &quot;yes&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... #&gt; $ ForeignWorker : chr &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; &quot;yes&quot; ... #&gt; $ Customer : chr &quot;good&quot; &quot;bad&quot; &quot;good&quot; &quot;good&quot; ... 上記のようにglmnet()を実行する前に, 必要なデータ前処理を行う. 欠損値の確認および欠損値の処理 例えば, パッケージAmeliaにある関数missmap()を使うと, 欠損値のある変数やレコードを可視化することができる. ここでは, 欠損値は補完せずに, 欠損値を持つレコード自体を除去することにする. パッケージtidyverse (より正確には, その中に含まれるパッケージdplyr) 内にある filter()関数を使用した例を示すが, 代替的に, complete.cases()を使用しても良い. # 欠損値の可視化 library(Amelia) missmap(credit_dat, col = c(&#39;black&#39;, &#39;lightblue&#39;), y.at = 1, y.labels = &#39;&#39;, legend = TRUE) # Amelia # 欠損値除去 library(tidyverse) credit_dat2 &lt;- credit_dat %&gt;% filter(!is.na(SavingsAccount) &amp; !is.na(Property)) # 欠損値除去 (代替的方法) # idx_comp &lt;- complete.cases(credit_dat) # credit_dat2 &lt;- credit_dat[idx_comp, ] 質的変数の数値化 (ダミー変数化) # model.matrix()により, 説明変数行列 (design matrix) の生成, かつ # 質的変数 → ダミー変数化 x &lt;- model.matrix(Customer ~ ., credit_dat2)[, -1] # (intercept) を除去 (&amp; NAを含む行は除かれる) y &lt;- credit_dat2$Customer lasso + ロジット回帰の実行 library(glmnet) res_glmnet &lt;- glmnet(x, y, family = &quot;binomial&quot;, alpha = 1, lambda = 0.01) res_glmnet$beta # 推定回帰係数 #&gt; 42 x 1 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; s0 #&gt; StatusAccountlt_0DM -0.685602433 #&gt; StatusAccountlt_200DM -0.412434306 #&gt; StatusAccountnone 0.782355735 #&gt; DurationMonth -0.042679931 #&gt; CreditHistoryB -0.095412680 #&gt; CreditHistoryC . #&gt; CreditHistoryD . #&gt; CreditHistoryE 0.501848921 #&gt; Purposebusiness . #&gt; Purposecar -0.154132447 #&gt; Purposeeducation -0.811628841 #&gt; Purposeothers . #&gt; Purposerepairs . #&gt; CreditAmount . #&gt; SavingsAccountlt_1000DM . #&gt; SavingsAccountlt_100DM -0.479910279 #&gt; SavingsAccountlt_500DM . #&gt; EmploymentSincelt_1yr -0.303152940 #&gt; EmploymentSincelt_4yrs . #&gt; EmploymentSincelt_7yrs 0.298352171 #&gt; EmploymentSinceunemployed -0.542343752 #&gt; InstallmentRate -0.096852163 #&gt; StatusAndSexM_divorced -0.191933913 #&gt; StatusAndSexM_married 0.075476637 #&gt; StatusAndSexM_single 0.095810739 #&gt; Guarantorsguarantor 0.819400005 #&gt; Guarantorsnone 0.005573323 #&gt; ResidenceSince . #&gt; Propertylife_insurance . #&gt; Propertyreal_estate 0.076516039 #&gt; Age 0.015995247 #&gt; InstallmentPlansnone 0.525251308 #&gt; InstallmentPlansstore . #&gt; Housingown . #&gt; Housingrent -0.430797835 #&gt; NCredits -0.102761386 #&gt; Jobskilled . #&gt; Jobunemployed . #&gt; Jobunskilled . #&gt; NPeopleMain . #&gt; Phoneyes 0.083489661 #&gt; ForeignWorkeryes -0.303151094 6.5 補足: GLMに対する変数選択の代替的方法 最後に, 正則化回帰ではないが, Rを使った, 一般化線形モデル (GLM) に対する変数選択の代替的方法について紹介する. ここでは, 上でも使用した白ワインデータを使い, ワインの品質を目的変数として デフォルトである線形回帰 (family = “gaussian”) をそのまま使用する. - glm() + step()の利用 - step(): Choose a model by AIC in a Stepwise Algorithm - ← AIC基準を使い, ステップワイズによりモデル選択 - glm()では, NAはデフォルトで除去されるが, step()では行数が変化したとしてエラー - → 欠損値(NA)の事前処理が必要; 除去 or 補間 # 例. ワインデータ # wine &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) res_glm &lt;- glm(quality ~ ., data = wine[train, ]) res_step &lt;- step(res_glm) #&gt; Start: AIC=6816.15 #&gt; quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + #&gt; chlorides + free.sulfur.dioxide + total.sulfur.dioxide + #&gt; density + pH + sulphates + alcohol #&gt; #&gt; Df Deviance AIC #&gt; - chlorides 1 1689.0 6814.2 #&gt; - citric.acid 1 1689.0 6814.2 #&gt; - total.sulfur.dioxide 1 1689.0 6814.2 #&gt; &lt;none&gt; 1689.0 6816.1 #&gt; - fixed.acidity 1 1692.2 6819.9 #&gt; - free.sulfur.dioxide 1 1698.8 6831.6 #&gt; - sulphates 1 1702.7 6838.4 #&gt; - pH 1 1703.6 6840.0 #&gt; - alcohol 1 1716.7 6863.0 #&gt; - density 1 1717.8 6864.9 #&gt; - residual.sugar 1 1741.5 6906.0 #&gt; - volatile.acidity 1 1779.9 6971.5 #&gt; #&gt; Step: AIC=6814.19 #&gt; quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + #&gt; free.sulfur.dioxide + total.sulfur.dioxide + density + pH + #&gt; sulphates + alcohol #&gt; #&gt; Df Deviance AIC #&gt; - total.sulfur.dioxide 1 1689.0 6812.2 #&gt; - citric.acid 1 1689.0 6812.3 #&gt; &lt;none&gt; 1689.0 6814.2 #&gt; - fixed.acidity 1 1692.4 6818.3 #&gt; - free.sulfur.dioxide 1 1698.8 6829.7 #&gt; - sulphates 1 1702.8 6836.5 #&gt; - pH 1 1704.2 6839.0 #&gt; - alcohol 1 1716.8 6861.2 #&gt; - density 1 1718.7 6864.6 #&gt; - residual.sugar 1 1743.8 6908.0 #&gt; - volatile.acidity 1 1781.7 6972.5 #&gt; #&gt; Step: AIC=6812.25 #&gt; quality ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + #&gt; free.sulfur.dioxide + density + pH + sulphates + alcohol #&gt; #&gt; Df Deviance AIC #&gt; - citric.acid 1 1689.1 6810.3 #&gt; &lt;none&gt; 1689.0 6812.2 #&gt; - fixed.acidity 1 1692.5 6816.3 #&gt; - sulphates 1 1702.8 6834.6 #&gt; - free.sulfur.dioxide 1 1702.8 6834.6 #&gt; - pH 1 1704.2 6837.1 #&gt; - alcohol 1 1716.8 6859.3 #&gt; - density 1 1719.8 6864.5 #&gt; - residual.sugar 1 1744.5 6907.2 #&gt; - volatile.acidity 1 1786.9 6979.3 #&gt; #&gt; Step: AIC=6810.33 #&gt; quality ~ fixed.acidity + volatile.acidity + residual.sugar + #&gt; free.sulfur.dioxide + density + pH + sulphates + alcohol #&gt; #&gt; Df Deviance AIC #&gt; &lt;none&gt; 1689.1 6810.3 #&gt; - fixed.acidity 1 1692.5 6814.3 #&gt; - sulphates 1 1702.8 6832.6 #&gt; - free.sulfur.dioxide 1 1702.8 6832.7 #&gt; - pH 1 1704.6 6835.8 #&gt; - alcohol 1 1717.0 6857.5 #&gt; - density 1 1720.5 6863.6 #&gt; - residual.sugar 1 1745.2 6906.4 #&gt; - volatile.acidity 1 1789.3 6981.3 summary(res_step) #&gt; #&gt; Call: #&gt; glm(formula = quality ~ fixed.acidity + volatile.acidity + residual.sugar + #&gt; free.sulfur.dioxide + density + pH + sulphates + alcohol, #&gt; data = wine[train, ]) #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.615e+02 2.139e+01 7.549 5.77e-14 *** #&gt; fixed.acidity 6.060e-02 2.473e-02 2.450 0.0143 * #&gt; volatile.acidity -1.839e+00 1.380e-01 -13.323 &lt; 2e-16 *** #&gt; residual.sugar 8.868e-02 8.897e-03 9.968 &lt; 2e-16 *** #&gt; free.sulfur.dioxide 4.336e-03 8.785e-04 4.936 8.42e-07 *** #&gt; density -1.617e+02 2.168e+01 -7.459 1.14e-13 *** #&gt; pH 6.706e-01 1.279e-01 5.242 1.69e-07 *** #&gt; sulphates 6.297e-01 1.278e-01 4.926 8.83e-07 *** #&gt; alcohol 1.996e-01 2.839e-02 7.030 2.54e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for gaussian family taken to be 0.5647177) #&gt; #&gt; Null deviance: 2396.8 on 2999 degrees of freedom #&gt; Residual deviance: 1689.1 on 2991 degrees of freedom #&gt; AIC: 6810.3 #&gt; #&gt; Number of Fisher Scoring iterations: 2 "],["判別分析.html", "7 判別分析 7.1 線形判別 &amp; 2次判別: パッケージMASSの利用 7.2 kNN法: パッケージclassの利用 7.3 他パッケージの利用: パッケージklaR", " 7 判別分析 判別分析における標準的的手法である線形判別 (lda), 2次判別 (qda)は, 標準パッケージMASSに含まれている関数lda(), qda(), 一方, kNN (k近傍法) はパッケージclassに含まれている関数knn()を 使用して実行することができる. 7.1 線形判別 &amp; 2次判別: パッケージMASSの利用 以下, 7.1, 7.2のコード作成の参考: ISLR, p.155- データセット#1: 米国S&amp;P500日次リターンデータ Smarket: S&amp;P500日次%リターン5年分 - Year 観測値の記録年 (2001--2005) - Lag1 前日の%リターン - Lag2 2日前の%リターン - Lag3 3日前の%リターン - Lag4 4日前の%リターン - Lag5 5日前の%リターン - Volume 取引量 (日次取引株式数, 単位十億枚) - Today 当日の%リターン - Direction 相場の方向性 (Down/Up, 2-水準因子) library(ISLR) names(Smarket) #&gt; [1] &quot;Year&quot; &quot;Lag1&quot; &quot;Lag2&quot; &quot;Lag3&quot; &quot;Lag4&quot; &quot;Lag5&quot; #&gt; [7] &quot;Volume&quot; &quot;Today&quot; &quot;Direction&quot; dim(Smarket) #&gt; [1] 1250 9 summary(Smarket) #&gt; Year Lag1 Lag2 Lag3 #&gt; Min. :2001 Min. :-4.922000 Min. :-4.922000 Min. :-4.922000 #&gt; 1st Qu.:2002 1st Qu.:-0.639500 1st Qu.:-0.639500 1st Qu.:-0.640000 #&gt; Median :2003 Median : 0.039000 Median : 0.039000 Median : 0.038500 #&gt; Mean :2003 Mean : 0.003834 Mean : 0.003919 Mean : 0.001716 #&gt; 3rd Qu.:2004 3rd Qu.: 0.596750 3rd Qu.: 0.596750 3rd Qu.: 0.596750 #&gt; Max. :2005 Max. : 5.733000 Max. : 5.733000 Max. : 5.733000 #&gt; Lag4 Lag5 Volume Today #&gt; Min. :-4.922000 Min. :-4.92200 Min. :0.3561 Min. :-4.922000 #&gt; 1st Qu.:-0.640000 1st Qu.:-0.64000 1st Qu.:1.2574 1st Qu.:-0.639500 #&gt; Median : 0.038500 Median : 0.03850 Median :1.4229 Median : 0.038500 #&gt; Mean : 0.001636 Mean : 0.00561 Mean :1.4783 Mean : 0.003138 #&gt; 3rd Qu.: 0.596750 3rd Qu.: 0.59700 3rd Qu.:1.6417 3rd Qu.: 0.596750 #&gt; Max. : 5.733000 Max. : 5.73300 Max. :3.1525 Max. : 5.733000 #&gt; Direction #&gt; Down:602 #&gt; Up :648 #&gt; #&gt; #&gt; #&gt; # cor(Smarket) # -&gt; Error cor(Smarket[, -9]) #&gt; Year Lag1 Lag2 Lag3 Lag4 #&gt; Year 1.00000000 0.029699649 0.030596422 0.033194581 0.035688718 #&gt; Lag1 0.02969965 1.000000000 -0.026294328 -0.010803402 -0.002985911 #&gt; Lag2 0.03059642 -0.026294328 1.000000000 -0.025896670 -0.010853533 #&gt; Lag3 0.03319458 -0.010803402 -0.025896670 1.000000000 -0.024051036 #&gt; Lag4 0.03568872 -0.002985911 -0.010853533 -0.024051036 1.000000000 #&gt; Lag5 0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641 #&gt; Volume 0.53900647 0.040909908 -0.043383215 -0.041823686 -0.048414246 #&gt; Today 0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527 #&gt; Lag5 Volume Today #&gt; Year 0.029787995 0.53900647 0.030095229 #&gt; Lag1 -0.005674606 0.04090991 -0.026155045 #&gt; Lag2 -0.003557949 -0.04338321 -0.010250033 #&gt; Lag3 -0.018808338 -0.04182369 -0.002447647 #&gt; Lag4 -0.027083641 -0.04841425 -0.006899527 #&gt; Lag5 1.000000000 -0.02200231 -0.034860083 #&gt; Volume -0.022002315 1.00000000 0.014591823 #&gt; Today -0.034860083 0.01459182 1.000000000 まず, データセットを学習データとテストデータに2分割する (Validation set approach). 具体的には, 時間軸に沿って, 2004以前を学習用, 2005年をテスト用に使用する. # データセット分割: Validation set approach # 時間軸に沿って2分割 (2004以前 → 学習, 2005年 → テスト) idx_train &lt;- (1:nrow(Smarket))[Smarket$Year &lt; 2005] Smarket_2005 &lt;- Smarket[-idx_train, ] dim(Smarket_2005) #&gt; [1] 252 9 Direction_2005 &lt;- Smarket[-idx_train, &quot;Direction&quot;] # 代替的方法-1 (ISLR) train &lt;- (Smarket$Year &lt; 2005) Smarket_2005 &lt;- Smarket[!train, ] Direction_2005 &lt;- Smarket[!train, &quot;Direction&quot;] # 代替的方法-2 library(tidyverse) idx_train &lt;- Smarket %&gt;% filter(Year &lt; 2005) %&gt;% row_number() Smarket_2005 &lt;- Smarket %&gt;% filter(Year == 2005) Direction_2005 &lt;- Smarket %&gt;% select(Direction) %&gt;% slice(-idx_train) %&gt;% `[[`(1) 線形判別 (LDA) モデル適合 線形判別の実行は, パッケージMASSの関数lda()により行うことができる. library(MASS) # 学習データによるモデル適合 fit_lda &lt;- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = idx_train) fit_lda #&gt; Call: #&gt; lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = idx_train) #&gt; #&gt; Prior probabilities of groups: #&gt; Down Up #&gt; 0.491984 0.508016 #&gt; #&gt; Group means: #&gt; Lag1 Lag2 #&gt; Down 0.04279022 0.03389409 #&gt; Up -0.03954635 -0.03132544 #&gt; #&gt; Coefficients of linear discriminants: #&gt; LD1 #&gt; Lag1 -0.6420190 #&gt; Lag2 -0.5135293 fit_ldaの出力結果には, 各グループ(Down, Up)ごとの, 予測変数 (Lag1, Lag2) のグループ平均の値や, 線形判別関数の係数に関する情報などが格納されている. 判別 (外挿予測) 線形判別 (lda) の出力オブジェクト (適合結果) を 関数predictに入力することで, 判別結果 (class), およびその判断に使用した 事後確率 (posterior) が出力される. 判別分析の使用目的は, まさにこれらの出力 (特に前者) を得ることである. # テストデータの判別 (外挿予測) pred_lda &lt;- predict(fit_lda, Smarket_2005) names(pred_lda) #&gt; [1] &quot;class&quot; &quot;posterior&quot; &quot;x&quot; # head(pred_lda$class) #&gt; [1] Up Up Up Up Up Up #&gt; Levels: Down Up head(pred_lda$posterior) #&gt; Down Up #&gt; 999 0.4901792 0.5098208 #&gt; 1000 0.4792185 0.5207815 #&gt; 1001 0.4668185 0.5331815 #&gt; 1002 0.4740011 0.5259989 #&gt; 1003 0.4927877 0.5072123 #&gt; 1004 0.4938562 0.5061438 # → 出力: $class, $posterior, $x 精度評価 学習済モデルのパフォーマンス評価は, テストデータを使って判別させ (外挿予測), マスクしておいた教師信号 (正解) と比較することにより行う. 分類精度の評価は, 混同行列や正解率で確認することができる. # 混同行列 table(pred_lda$class, Direction_2005) # 正解率 mean(pred_lda$class == Direction_2005) #&gt; Direction_2005 #&gt; Down Up #&gt; Down 35 35 #&gt; Up 76 106 #&gt; [1] 0.5595238 # 閾値0.5の場合の方向予測 # 2005年における (下落発生の) 事後確率0.5以上の予測件数 sum(pred_lda$posterior[, 1] &gt;= 0.5) # 同0.5未満の予測確率 sum(pred_lda$posterior[, 1] &lt; 0.5) # モデルによる事後確率の出力値, 市場が下落する確率に対応 pred_lda$posterior[1:20, 1] pred_lda$class[1:20] # 2005年における事後確率0.9超の予測件数 sum(pred_lda$posterior[, 1] &gt; 0.9) # 同事後確率の最大値 max(pred_lda$posterior[, 1]) 2次判別 (QDA) モデル適合 線形判別の実行は, パッケージMASSの関数qda()により行うことができる. 操作はlda()と同様である. # 学習データによるモデル適合 fit_qda &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = idx_train) fit_qda #&gt; Call: #&gt; qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = idx_train) #&gt; #&gt; Prior probabilities of groups: #&gt; Down Up #&gt; 0.491984 0.508016 #&gt; #&gt; Group means: #&gt; Lag1 Lag2 #&gt; Down 0.04279022 0.03389409 #&gt; Up -0.03954635 -0.03132544 判別 (外挿予測) # テストデータの判別 (外挿予測) pred_qda &lt;- predict(fit_qda, Smarket_2005) names(pred_qda) #&gt; [1] &quot;class&quot; &quot;posterior&quot; head(pred_qda$class) #&gt; [1] Up Up Up Up Up Up #&gt; Levels: Down Up head(pred_qda$posterior) #&gt; Down Up #&gt; 999 0.4873243 0.5126757 #&gt; 1000 0.4759011 0.5240989 #&gt; 1001 0.4636911 0.5363089 #&gt; 1002 0.4739253 0.5260747 #&gt; 1003 0.4903426 0.5096574 #&gt; 1004 0.4913561 0.5086439 # → 出力: $class, $posterior 精度評価 # 混同行列 table(pred_qda$class, Direction_2005) # 正解率 mean(pred_qda$class == Direction_2005) #&gt; Direction_2005 #&gt; Down Up #&gt; Down 30 20 #&gt; Up 81 121 #&gt; [1] 0.5992063 上でも軽く触れたが, 学習済モデルのパラメータ推定値は lda()やqda()の出力オブジェクトに格納され, これらから 線形判別関数や2次判別関数や, 判別境界の形状を知ることができる. # fit_lda$scaling # 線形判別関数の係数 # hist(predict(fit_lda)$x) # 判別得点 # fit_lda$means # グループ平均 # fit_lda$prior # 事前確率 ここで用いたSmarketは時系列データゆえ, Validation set approachを採用するにあたって時間的に古いものを学習用データ, 新しいものをテスト用データに指定した. 一般の時系列ではないデータセットの場合には, データセットの分割は通常ランダムに行うことになるが, それでも 学習済モデルの判別パフォーマンスはデータセットの分割の仕方に依存してしまう. そこで, そのようなデータセット分割への依存性を軽減し, モデルによる外挿予測の精度評価の信頼性を高めるために交互検証 (クロス・バリデーション) を行うことが推奨される. 関数lda()やqda()は引数CVを持っていて, これをTRUEと設定することで, LOOCVを実行することができる (デフォルトはFALSE). (Smarketは時系列データセットのため, LOOCVの使用は適切でない. ここでは省略する.) LOOCV (Lead-One-Out Cross-Validation) - 交差検証法の一種. 1個抜き交差検証 - データセットの各データ点を1つずつテスト用データとして取り出し, 残りのサンプルを学習用データとして使用して, モデルの適合および外挿予測を行う. - このプロセスをデータセットの全てのデータ点に対して繰り返し行うことで, モデルの性能を評価する. 7.2 kNN法: パッケージclassの利用 kNN法は, 標準ライブラリclass内の関数knn()によって実行できる. kNN法では, クラス数\\(k\\)を与える必要がある. knn()へのデータセットの与え方は, 上記lda(), qda()とは異なり, 学習に用いる予測変数, テストに用いる予測変数, 学習に用いる目的変数 (ラベル)を順に与える必要がある. データセットの分割 # kNN法 library(class) # データセットの分割 # train_x &lt;- cbind(Smarket$Lag1, Smarket$Lag2)[idx_train, ] # test_x &lt;- cbind(Smarket$Lag1, Smarket$Lag2)[-idx_train, ] train_x &lt;- Smarket[idx_train, c(&quot;Lag1&quot;, &quot;Lag2&quot;)] test_x &lt;- Smarket[-idx_train, c(&quot;Lag1&quot;, &quot;Lag2&quot;)] train_direction &lt;- Smarket$Direction[idx_train] 判別 (外挿予測) ここでは\\(k=1\\)とした例を示す. # K = 1 # 学習データをもとに, テストデータを分類予測 # set.seed(1) pred_knn &lt;- knn(train_x, test_x, train_direction, k =1) 精度評価 # 混同行列 (tbl_knn &lt;- table(pred_knn, Direction_2005)) #&gt; Direction_2005 #&gt; pred_knn Down Up #&gt; Down 43 58 #&gt; Up 68 83 # library(caret) # confusionMatrix(tbl_knn, mode = &quot;prec_recall&quot;) # 正解率 mean(pred_knn == Direction_2005) #&gt; [1] 0.5 交差検証 (LOOCV) classには, 関数knn.cv()が用意されている. これを用いれば, 上記のようにデータセット全体を学習用とテスト用にあらかじめ 2分割してknn()に与えるのではなく, データセット全体を与えることで LOOCV (leave-oen-out cross validation) を実行することができる. 上でも触れたが, 時系列データセットに対して, LOOCVその他の交差検証を実施する際には十分注意が必要である. すなわち, 将来データを学習して過去のデータを予測するようなデータの使い方は, 外挿予測 (=将来予測) を目的とする場合には適切ではない. そのように計算された外挿予測値を使って テスト予測誤差を評価することの意味・意義について 問わないといけない. ここでは, あくまで操作の参考として, knn.cv()の使用や\\(k\\)の選択を行うRコード例を示す. # Cross-validation # LOOCV (leave-oen-out cross validation) # → 全データセット (テストデータ1つずつ) の分類結果を返す x &lt;- Smarket[, c(&quot;Lag1&quot;, &quot;Lag2&quot;)] y &lt;- Smarket$Direction pred_knn_cv &lt;- knn.cv(x, y, k = 3, prob = T) head(pred_knn_cv) #&gt; [1] Up Up Down Up Up Down #&gt; Levels: Down Up summary(pred_knn_cv) #&gt; Down Up #&gt; 566 684 LOOCVに対して全データセットを与えて学習・ 予測させたため, 精度評価のための 予測値と真の値 (教師信号) の比較は全データセットに対して 行われねばならない. # 混同行列 (tbl_knn_cv &lt;- table(pred_knn_cv, y)) #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 269 297 #&gt; Up 333 351 # 正解率 mean(pred_knn_cv == y) #&gt; [1] 0.496 正解率は50%を下回り, 予測がうまくいってないことを示している. 一般に, kNN法は\\(k\\)の大きさに結果が左右されることから, 適切な\\(k\\)の選択は重要である. for (k_tmp in 1:10){ print(paste0(&quot;k=&quot;, k_tmp)) pred_knn_cv &lt;- knn.cv(x, y, k = k_tmp) # 混同行列 print(tbl_knn_cv &lt;- table(pred_knn_cv, y)) # 正解率 print(mean(pred_knn_cv == y)) } #&gt; [1] &quot;k=1&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 280 310 #&gt; Up 322 338 #&gt; [1] 0.4944 #&gt; [1] &quot;k=2&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 287 309 #&gt; Up 315 339 #&gt; [1] 0.5008 #&gt; [1] &quot;k=3&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 269 297 #&gt; Up 333 351 #&gt; [1] 0.496 #&gt; [1] &quot;k=4&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 273 299 #&gt; Up 329 349 #&gt; [1] 0.4976 #&gt; [1] &quot;k=5&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 273 300 #&gt; Up 329 348 #&gt; [1] 0.4968 #&gt; [1] &quot;k=6&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 286 305 #&gt; Up 316 343 #&gt; [1] 0.5032 #&gt; [1] &quot;k=7&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 268 305 #&gt; Up 334 343 #&gt; [1] 0.4888 #&gt; [1] &quot;k=8&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 270 315 #&gt; Up 332 333 #&gt; [1] 0.4824 #&gt; [1] &quot;k=9&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 270 296 #&gt; Up 332 352 #&gt; [1] 0.4976 #&gt; [1] &quot;k=10&quot; #&gt; y #&gt; pred_knn_cv Down Up #&gt; Down 255 300 #&gt; Up 347 348 #&gt; [1] 0.4824 \\(k\\)を変えても, 正解率は大きく変わらず, いずれも50%を下回った. 本データセットが時系列データであることを考えれば, 不自然とは言えない. 7.3 他パッケージの利用: パッケージklaR データセット#2: ビジネススクールの入学許可データ - admission.csv - GPA - GMAT - De (意思決定): &quot;admit&quot;, &quot;border&quot;, &quot;notadmit&quot; - source: https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html url &lt;- &#39;http://www.biz.uiowa.edu/faculty/jledolter/DataMining/admission.csv&#39; admdat &lt;- read.csv(url, stringsAsFactors = T) # 注: stringsAsFactors = F (デフォルト) → エラー # admdat &lt;- read.csv(&quot;admission.csv&quot;, skip = 2, stringsAsFactors = T) head(admdat) # admdat = data.frame(admdat) plot(admdat$GPA, admdat$GMAT, col = admdat$De) #&gt; GPA GMAT De #&gt; 1 2.96 596 admit #&gt; 2 3.14 473 admit #&gt; 3 3.22 482 admit #&gt; 4 3.29 527 admit #&gt; 5 3.69 505 admit #&gt; 6 3.46 693 admit 関数partimat()は, 判別分析やそれ以外の多様なクラス分類手法をサポートしている. 引数methodで指定: “lda”, “qda”, “rpart”, “naiveBayes”, “rda”, “sknn”, “svmlight” # &quot;klaR&quot;パッケージの利用 #install.packages(&#39;klaR&#39;) library(klaR) # 線形判別 partimat(De ~ ., data = admdat, method = &quot;lda&quot;) # 2次判別 partimat(De ~ ., data = admdat, method = &quot;qda&quot;) # kNN法 partimat(De ~ ., data = admdat, method = &quot;sknn&quot;) # Simple kNN法 (デフォルトk = 3) 変数間のスケールを統一 (標準化) した後にkNN法を再実行してみる. # 変数標準化後 admdat2 &lt;- data.frame(GPA = scale(admdat$GPA), GMAT = scale(admdat$GMAT), De = admdat$De) partimat(De ~ ., data = admdat2, method = &quot;sknn&quot;) # Simple kNN法 (デフォルトk = 3) パッケージklaR内には, ナイーブベイズ法を実行する関数NaiveBayes()も用意されている. # ナイーブベイズ法 fit_NB &lt;- NaiveBayes(De ~ ., data = admdat) # usekernel = F(デフォルト) → 正規分布 # predict(fit_NB) # # → 出力: $class, $posterior (tbl_nb &lt;- table(predict(fit_NB)$class, admdat$De)) #&gt; #&gt; admit border notadmit #&gt; admit 28 1 0 #&gt; border 3 23 2 #&gt; notadmit 0 2 26 関数partimat()において, method = \"naiveBayes\"と指定してもよい. # または partimat(De ~ ., data = admdat, method = &quot;naiveBayes&quot;) # ナイーブベイズ法 "],["決定木分析.html", "8 決定木分析 8.1 導入: 回帰木 vs 線形回帰 8.2 回帰木 8.3 分類木", " 8 決定木分析 決定木 (decision tree) は, 学習データに含まれる説明変数空間を, 目的変数との関係に基づいて (目的変数がなるべく均質になるように) 階層的に分割する手法である. この分割は, 目的関数（回帰問題であればMSEなど, 分類問題であればGini不純度等）を最適化するように, 一回につき1変数, 学習データに基づいて逐次行われる. 逐次分割のルールは, 木構造で表現される. ルートノード： 木の最上部. データセット全体を表現 (内部) ノード： データセットの分割条件 (質問や条件)を表現. データセットが分割される リーフノード （末端ノード）: 分類されたデータセットの最終的な出力を表現. これ以上の分割は行われない 「決定木はデータセットの分割ルールを自動生成する」 という説明がなされることがあるが, 不正確であることに注意が必要である. 正確には, 生成されるのは「説明変数空間の分割規則」であり, 分割後の各領域に対応した予測値（分類ラベルや回帰適合値） が付与されているに過ぎない. 決定木分析を行う標準的な関数として, パッケージrpartの関数rpart()がある. 回帰木, 分類木どちらにも適用可能である. 8.1 導入: 回帰木 vs 線形回帰 データセット1: 自動車の制動距離 決定木の特徴を理解するため, 金明哲(2017)『Rによるデータサイエンス』でも紹介されている 例を取り上げる. Rに標準的に含まれているデータセットcarsを使い, ブレーキをかけて停止するまでの移動距離を目的変数, 自動車の速度を説明変数とする場合の, 線形回帰と決定木 (回帰木) の適合の結果を比較する. - cars - speed: 自動車の速度 (mph) - dist: ブレーキをかけて自動車が停止するまでの距離 (feet) - Rの標準データセット #cars head(cars); tail(cars) #&gt; speed dist #&gt; 1 4 2 #&gt; 2 4 10 #&gt; 3 7 4 #&gt; 4 7 22 #&gt; 5 8 16 #&gt; 6 9 10 #&gt; speed dist #&gt; 45 23 54 #&gt; 46 24 70 #&gt; 47 24 92 #&gt; 48 24 93 #&gt; 49 24 120 #&gt; 50 25 85 #cars[&quot;speed&quot;] plot(cars) cor(cars[&quot;speed&quot;], cars[&quot;dist&quot;]) #&gt; dist #&gt; speed 0.8068949 単回帰分析 すでに上で解説した通りの操作を行う. # 単回帰分析 cars_lm &lt;- lm(dist ~ speed, data = cars) summary(cars_lm) #&gt; #&gt; Call: #&gt; lm(formula = dist ~ speed, data = cars) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -29.069 -9.525 -2.272 9.215 43.201 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -17.5791 6.7584 -2.601 0.0123 * #&gt; speed 3.9324 0.4155 9.464 1.49e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 15.38 on 48 degrees of freedom #&gt; Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 #&gt; F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 # 回帰係数の取り出し cars_lm$coef #&gt; (Intercept) speed #&gt; -17.579095 3.932409 coefficients(cars_lm) #&gt; (Intercept) speed #&gt; -17.579095 3.932409 # 回帰直線の図示 plot(cars) abline(cars_lm) # モデル診断（残差プロット等) # plot(cars_lm) # 4枚表示: 残差vs Y適合値, 残差 vs Q - Qプロット, # 残差平方根 vs Y適合値, 残差 vs 影響力(てこ値とCook距離) 予測 # 学習データに対する適合値 (内挿予測) cars_pred &lt;- predict(cars_lm) # 残差 cars_resd &lt;- residuals(cars_lm) # 予測値 vs 残差 plot(cars_pred, cars_resd); abline(h = 0, lty = 2) data.frame(cars, cars_pred, cars_resd) #&gt; speed dist cars_pred cars_resd #&gt; 1 4 2 -1.849460 3.849460 #&gt; 2 4 10 -1.849460 11.849460 #&gt; 3 7 4 9.947766 -5.947766 #&gt; 4 7 22 9.947766 12.052234 #&gt; 5 8 16 13.880175 2.119825 #&gt; 6 9 10 17.812584 -7.812584 #&gt; 7 10 18 21.744993 -3.744993 #&gt; 8 10 26 21.744993 4.255007 #&gt; 9 10 34 21.744993 12.255007 #&gt; 10 11 17 25.677401 -8.677401 #&gt; 11 11 28 25.677401 2.322599 #&gt; 12 12 14 29.609810 -15.609810 #&gt; 13 12 20 29.609810 -9.609810 #&gt; 14 12 24 29.609810 -5.609810 #&gt; 15 12 28 29.609810 -1.609810 #&gt; 16 13 26 33.542219 -7.542219 #&gt; 17 13 34 33.542219 0.457781 #&gt; 18 13 34 33.542219 0.457781 #&gt; 19 13 46 33.542219 12.457781 #&gt; 20 14 26 37.474628 -11.474628 #&gt; 21 14 36 37.474628 -1.474628 #&gt; 22 14 60 37.474628 22.525372 #&gt; 23 14 80 37.474628 42.525372 #&gt; 24 15 20 41.407036 -21.407036 #&gt; 25 15 26 41.407036 -15.407036 #&gt; 26 15 54 41.407036 12.592964 #&gt; 27 16 32 45.339445 -13.339445 #&gt; 28 16 40 45.339445 -5.339445 #&gt; 29 17 32 49.271854 -17.271854 #&gt; 30 17 40 49.271854 -9.271854 #&gt; 31 17 50 49.271854 0.728146 #&gt; 32 18 42 53.204263 -11.204263 #&gt; 33 18 56 53.204263 2.795737 #&gt; 34 18 76 53.204263 22.795737 #&gt; 35 18 84 53.204263 30.795737 #&gt; 36 19 36 57.136672 -21.136672 #&gt; 37 19 46 57.136672 -11.136672 #&gt; 38 19 68 57.136672 10.863328 #&gt; 39 20 32 61.069080 -29.069080 #&gt; 40 20 48 61.069080 -13.069080 #&gt; 41 20 52 61.069080 -9.069080 #&gt; 42 20 56 61.069080 -5.069080 #&gt; 43 20 64 61.069080 2.930920 #&gt; 44 22 66 68.933898 -2.933898 #&gt; 45 23 54 72.866307 -18.866307 #&gt; 46 24 70 76.798715 -6.798715 #&gt; 47 24 92 76.798715 15.201285 #&gt; 48 24 93 76.798715 16.201285 #&gt; 49 24 120 76.798715 43.201285 #&gt; 50 25 85 80.731124 4.268876 # テストデータ(未学習データ)に対する予測 (外挿予測) testdat &lt;- data.frame(speed = c(5, 6, 21)) head(predict(cars_lm, newdata = testdat)) #&gt; 1 2 3 #&gt; 2.082949 6.015358 65.001489 8.1.1 基本操作: 回帰木 シンタックスはそのままで, lm()の代わりにrpart()と書き換えれば, 回帰木が適合される. library(rpart) cars_rp &lt;- rpart(dist ~ speed, data = cars) summary(cars_rp) # ==&gt; 葉3枚 #&gt; Call: #&gt; rpart(formula = dist ~ speed, data = cars) #&gt; n= 50 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.4676398 0 1.0000000 1.0141912 0.2162518 #&gt; 2 0.1104944 1 0.5323602 0.6416901 0.1465035 #&gt; 3 0.0100000 2 0.4218658 0.4841866 0.1230232 #&gt; #&gt; Variable importance #&gt; speed #&gt; 100 #&gt; #&gt; Node number 1: 50 observations, complexity param=0.4676398 #&gt; mean=42.98, MSE=650.7796 #&gt; left son=2 (31 obs) right son=3 (19 obs) #&gt; Primary splits: #&gt; speed &lt; 17.5 to the left, improve=0.4676398, (0 missing) #&gt; #&gt; Node number 2: 31 observations, complexity param=0.1104944 #&gt; mean=29.32258, MSE=267.9605 #&gt; left son=4 (15 obs) right son=5 (16 obs) #&gt; Primary splits: #&gt; speed &lt; 12.5 to the left, improve=0.4328244, (0 missing) #&gt; #&gt; Node number 3: 19 observations #&gt; mean=65.26316, MSE=474.5097 #&gt; #&gt; Node number 4: 15 observations #&gt; mean=18.2, MSE=78.42667 #&gt; #&gt; Node number 5: 16 observations #&gt; mean=39.75, MSE=220.9375 plot(cars_rp, uniform = T, margin = 0.05) text(cars_rp, all = T, use.n = T) 決定木の主要な出力は, 分割ルールである. 今回は, 量的説明変数が一つ (speed) のみのケースであった. 適合の結果, speedの領域を 【分割1】speedが17.5未満か/以上か 【分割2】(speedが17.5未満の領域に対して) speedが12.5未満か/以上か のように2回分割して, 分割を終了した. 結果的に, 木の深さが2, リーフノード (末端ノード) が3個の小さな木である. 予測 予測値は線形回帰等と同様, 関数predict()にrpart()の結果オブジェクトを与えることで得られる. # 学習用データに対する適合値 (内挿予測) cars_rp_pred &lt;- predict(cars_rp) cars_rp_fitted &lt;- data.frame(cars$speed, cars_rp_pred) plot(cars$speed, cars$dist) lines(cars_rp_fitted, type = &quot;s&quot;) 図は, 得られた (説明変数空間の) 分割ルールでは, 横軸 (説明変数speed) は3つの領域に分割され (12.5, 17.5が境界点), それぞれの領域における予測値は, 一定である (左から順に, 18.2, 39.75, 65.26). この目的変数distに対する説明変数speedの区分一定 (piecewise constant) な形状は, 予測値を具体的に出力することでも確認できる. # 観測値, 線形回帰の適合値, 回帰木の適合値 data.frame(cars, cars_pred, cars_rp_fitted) #&gt; speed dist cars_pred cars.speed cars_rp_pred #&gt; 1 4 2 -1.849460 4 18.20000 #&gt; 2 4 10 -1.849460 4 18.20000 #&gt; 3 7 4 9.947766 7 18.20000 #&gt; 4 7 22 9.947766 7 18.20000 #&gt; 5 8 16 13.880175 8 18.20000 #&gt; 6 9 10 17.812584 9 18.20000 #&gt; 7 10 18 21.744993 10 18.20000 #&gt; 8 10 26 21.744993 10 18.20000 #&gt; 9 10 34 21.744993 10 18.20000 #&gt; 10 11 17 25.677401 11 18.20000 #&gt; 11 11 28 25.677401 11 18.20000 #&gt; 12 12 14 29.609810 12 18.20000 #&gt; 13 12 20 29.609810 12 18.20000 #&gt; 14 12 24 29.609810 12 18.20000 #&gt; 15 12 28 29.609810 12 18.20000 #&gt; 16 13 26 33.542219 13 39.75000 #&gt; 17 13 34 33.542219 13 39.75000 #&gt; 18 13 34 33.542219 13 39.75000 #&gt; 19 13 46 33.542219 13 39.75000 #&gt; 20 14 26 37.474628 14 39.75000 #&gt; 21 14 36 37.474628 14 39.75000 #&gt; 22 14 60 37.474628 14 39.75000 #&gt; 23 14 80 37.474628 14 39.75000 #&gt; 24 15 20 41.407036 15 39.75000 #&gt; 25 15 26 41.407036 15 39.75000 #&gt; 26 15 54 41.407036 15 39.75000 #&gt; 27 16 32 45.339445 16 39.75000 #&gt; 28 16 40 45.339445 16 39.75000 #&gt; 29 17 32 49.271854 17 39.75000 #&gt; 30 17 40 49.271854 17 39.75000 #&gt; 31 17 50 49.271854 17 39.75000 #&gt; 32 18 42 53.204263 18 65.26316 #&gt; 33 18 56 53.204263 18 65.26316 #&gt; 34 18 76 53.204263 18 65.26316 #&gt; 35 18 84 53.204263 18 65.26316 #&gt; 36 19 36 57.136672 19 65.26316 #&gt; 37 19 46 57.136672 19 65.26316 #&gt; 38 19 68 57.136672 19 65.26316 #&gt; 39 20 32 61.069080 20 65.26316 #&gt; 40 20 48 61.069080 20 65.26316 #&gt; 41 20 52 61.069080 20 65.26316 #&gt; 42 20 56 61.069080 20 65.26316 #&gt; 43 20 64 61.069080 20 65.26316 #&gt; 44 22 66 68.933898 22 65.26316 #&gt; 45 23 54 72.866307 23 65.26316 #&gt; 46 24 70 76.798715 24 65.26316 #&gt; 47 24 92 76.798715 24 65.26316 #&gt; 48 24 93 76.798715 24 65.26316 #&gt; 49 24 120 76.798715 24 65.26316 #&gt; 50 25 85 80.731124 25 65.26316 分割ルールを, 未学習のデータセットに対して適用してみる. speedの値が, それぞれ, 5,6,21 (mph) の時の予測値は 以下の通りである. # テストデータに対する予測値 (外挿予測) predict(cars_rp, newdata = testdat) #&gt; 1 2 3 #&gt; 18.20000 18.20000 65.26316 予測値は, 順に18.2, 18.2, 65.26である. スピードが5でも6でも同じ静止距離 (18.2) と予測している. すなわち, 制止までの距離が速度の変化に対して連続的に変化するような今回の事例においては, 区分一定な予測を行う回帰木の使用は明らかに不適切である. 8.2 回帰木 データセット2: 米國株価指数データ (再掲) - Smarket: S&amp;P500日次%リターン5年分 - Year 観測値の記録年 (2001--2005) - Lag1 前日の%リターン - Lag2 2日前の%リターン - Lag3 3日前の%リターン - Lag4 4日前の%リターン - Lag5 5日前の%リターン - Volume 取引量 (日次取引株式数, 単位十億枚) - Today 当日の%リターン - Direction 相場の方向性 (Down/Up, 2-水準因子) 8.2, 8.3は, 決定木の動作を確認し, その特性を理解することを目的とする. 決定木の機能である, データセットの分類規則の生成にフォーカスし, データセットは学習用・予測用に2分割せず全てを使って学習させる. ここでは, 容易でないタスク (株価予測) を決定木で行うが, 予測力についてはいまは議論しない. library(ISLR) head(Smarket) #&gt; Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction #&gt; 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up #&gt; 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up #&gt; 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down #&gt; 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up #&gt; 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up #&gt; 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up 回帰木の構築 Smarketを使って, 回帰木を構築する. 回帰木では当日の%リターンTodayを目的変数とする. Smarketには, 当日の相場の上下の方向性を示す 量的変数Directionが含まれているが, これはTodayの予測変数に 使えないため除去する. 記録年であるYearも当日のリターン予測には (常識的に考えて) 使えないので除く. Volumeは. Todayと同じタイミングで得られる情報であるため, ここでは予測変数からは除いておくが, ラグ変数にするなどの工夫を すれば使っても良い. library(tidyverse) sp500 &lt;- Smarket %&gt;% dplyr::select(-Year, -Direction, -Volume) # Volumeは当日情報のため, 除いておく # → 日付をずらせば加えても良い # sp500$Vol_lag1 &lt;- dplyr::lead(Smarket$Volume, n = 1) 回帰木の適合 sp_rp &lt;- rpart(Today ~ . , data = sp500) sp_rp #&gt; n= 1250 #&gt; #&gt; node), split, n, deviance, yval #&gt; * denotes terminal node #&gt; #&gt; 1) root 1250 1612.77800 0.00313840 #&gt; 2) Lag5&gt;=-2.892 1236 1528.47500 -0.01281958 * #&gt; 3) Lag5&lt; -2.892 14 56.19987 1.41200000 * # sp_rp &lt;- rpart(Today ~ . , data = sp500, control = rpart.control(cp = 0.008)) sp_rp #&gt; n= 1250 #&gt; #&gt; node), split, n, deviance, yval #&gt; * denotes terminal node #&gt; #&gt; 1) root 1250 1612.77800 0.00313840 #&gt; 2) Lag5&gt;=-2.892 1236 1528.47500 -0.01281958 * #&gt; 3) Lag5&lt; -2.892 14 56.19987 1.41200000 * # sp_rp &lt;- rpart(Today ~ . , data = sp500, control = rpart.control(cp = 0.0079)) #rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, # maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, # surrogatestyle = 0, maxdepth = 30, ...) # method: &quot;anova&quot;, &quot;poisson&quot;, &quot;class&quot; or &quot;exp&quot; sp_rp #&gt; n= 1250 #&gt; #&gt; node), split, n, deviance, yval #&gt; * denotes terminal node #&gt; #&gt; 1) root 1250 1612.77800 0.00313840 #&gt; 2) Lag5&gt;=-2.892 1236 1528.47500 -0.01281958 #&gt; 4) Lag1&gt;=0.2225 502 549.98260 -0.12242230 * #&gt; 5) Lag1&lt; 0.2225 734 968.33720 0.06214033 #&gt; 10) Lag2&gt;=-0.291 496 567.84910 -0.02306048 * #&gt; 11) Lag2&lt; -0.291 238 389.38390 0.23970170 #&gt; 22) Lag5&lt; -1.7375 15 49.41417 -0.80153330 * #&gt; 23) Lag5&gt;=-1.7375 223 322.61330 0.30973990 * #&gt; 3) Lag5&lt; -2.892 14 56.19987 1.41200000 * 得られた回帰木の可視化 決定木は, 説明変数の空間を分割するルールを生成する手法であり, 解釈容易性が大きな強みである. 解釈容易性を助けるため, 生成されたルールを表す木を表示するための関数が用意されている. # 可視化 library(rpart.plot) rpart.plot(sp_rp, digit = 3) rpart.plot(sp_rp, digit = 4, fallen.leaves = T, type = 3, extra = 101) CP表 (Complexity Parameter Table) # CP値 vs 交差検証 (CV) 予測誤差 printcp(sp_rp) #&gt; #&gt; Regression tree: #&gt; rpart(formula = Today ~ ., data = sp500, control = rpart.control(cp = 0.0079)) #&gt; #&gt; Variables actually used in tree construction: #&gt; [1] Lag1 Lag2 Lag5 #&gt; #&gt; Root node error: 1612.8/1250 = 1.2902 #&gt; #&gt; n= 1250 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.0174254 0 1.00000 1.0022 0.059367 #&gt; 2 0.0079811 1 0.98257 1.0274 0.061692 #&gt; 3 0.0079000 4 0.95863 1.1299 0.072674 plotcp(sp_rp) plotcp()は, 木の複雑度 (CP) (横軸) に対する 交差検証 (CV) 予測誤差の大きさ (縦軸) をプロットした図であり, 通常, 閾値 (点線で表される水平線. フルに成長した剪定前の木の持つ誤差の大きさ) を下回る最初のcpの値を 選択するのが望ましいとされる. 上で得られた図は, cp (横軸) が増えると, 交差検証誤差の大きさ (縦軸) が上がる, すなわち, 木を複雑にすると予測誤差が悪化するを示している. 木のサイズが1または2の時に点線を下回っているのみで, 1から2になると若干だが誤差は大きくなっており, データセットを分割することの説得性がこの図からは得らない. 株価予測の難しさを反映していると考えられる. # 手動による剪定例 # cp (complex parameter) の大きさでコントロール prn_rp &lt;- prune(sp_rp, cp = 0.008) plot(prn_rp, uniform = T, margin = 0.05) text(prn_rp, all = T, use.n = T) パフォーマンス評価 (内挿予測) 自主課題 上で得られた決定木を解釈してみよう. 8.3 分類木 同じ Smarketを使って, 分類木を構築する. 回帰木では当日の%リターンTodayを目的変数としていたが, ここでは, 量的変数Directionを目的変数に設定する. Directionは, Todayを使って作られていることから, Todayを予測変数に使わないようにYear, Volumeと共に除く. sp500_2 &lt;- Smarket %&gt;% dplyr::select(-Year, -Today, -Volume) # sp500_2$Vol_lag1 &lt;- dplyr::lead(Smarket$Volume, n = 1) 分類木の適合 sp2_rp &lt;- rpart(Direction ~ . , data = sp500_2) sp2_rp #&gt; n= 1250 #&gt; #&gt; node), split, n, loss, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 1250 602 Up (0.4816000 0.5184000) #&gt; 2) Lag1&gt;=0.0555 610 286 Down (0.5311475 0.4688525) #&gt; 4) Lag1&lt; 0.1375 50 14 Down (0.7200000 0.2800000) * #&gt; 5) Lag1&gt;=0.1375 560 272 Down (0.5142857 0.4857143) #&gt; 10) Lag2&gt;=0.635 118 45 Down (0.6186441 0.3813559) * #&gt; 11) Lag2&lt; 0.635 442 215 Up (0.4864253 0.5135747) #&gt; 22) Lag4&gt;=1.1645 53 18 Down (0.6603774 0.3396226) * #&gt; 23) Lag4&lt; 1.1645 389 180 Up (0.4627249 0.5372751) #&gt; 46) Lag4&lt; 0.833 363 174 Up (0.4793388 0.5206612) #&gt; 92) Lag2&lt; -0.5965 134 60 Down (0.5522388 0.4477612) * #&gt; 93) Lag2&gt;=-0.5965 229 100 Up (0.4366812 0.5633188) * #&gt; 47) Lag4&gt;=0.833 26 6 Up (0.2307692 0.7692308) * #&gt; 3) Lag1&lt; 0.0555 640 278 Up (0.4343750 0.5656250) #&gt; 6) Lag5&lt; -0.8225 128 57 Down (0.5546875 0.4453125) #&gt; 12) Lag4&lt; 1.3205 111 44 Down (0.6036036 0.3963964) * #&gt; 13) Lag4&gt;=1.3205 17 4 Up (0.2352941 0.7647059) * #&gt; 7) Lag5&gt;=-0.8225 512 207 Up (0.4042969 0.5957031) #&gt; 14) Lag2&gt;=0.7265 101 49 Down (0.5148515 0.4851485) #&gt; 28) Lag2&lt; 1.097 39 11 Down (0.7179487 0.2820513) * #&gt; 29) Lag2&gt;=1.097 62 24 Up (0.3870968 0.6129032) * #&gt; 15) Lag2&lt; 0.7265 411 155 Up (0.3771290 0.6228710) * 得られた分類木の可視化 # 決定木の可視化 rpart.plot(sp2_rp, digit = 3) rpart.plot(sp2_rp, digit = 4, fallen.leaves = T, type = 3, extra = 101) CP表 (Complexity Parameter Table) # CP値 vs 交差検証 (CV) 予測誤差 printcp(sp2_rp) #&gt; #&gt; Classification tree: #&gt; rpart(formula = Direction ~ ., data = sp500_2) #&gt; #&gt; Variables actually used in tree construction: #&gt; [1] Lag1 Lag2 Lag4 Lag5 #&gt; #&gt; Root node error: 602/1250 = 0.4816 #&gt; #&gt; n= 1250 #&gt; #&gt; CP nsplit rel error xerror xstd #&gt; 1 0.063123 0 1.00000 1.00000 0.029345 #&gt; 2 0.023256 1 0.93688 0.99169 0.029335 #&gt; 3 0.016058 2 0.91362 1.00664 0.029351 #&gt; 4 0.014950 5 0.86545 1.00000 0.029345 #&gt; 5 0.014120 6 0.85050 1.00997 0.029354 #&gt; 6 0.011628 8 0.82226 1.00831 0.029353 #&gt; 7 0.010000 10 0.79900 0.99336 0.029337 plotcp(sp2_rp) 上の図は, cp (横軸) が増えても, 交差検証予測誤差の大きさ (縦軸) が点線の回りで上下するだけで 大きな変化が見られない, すなわち, 木の複雑度を変えても適合が改善しないことを示している. やはり, 株価予測 (実質的にランダム) を行っていることに よる帰結だと考えられる. パフォーマンス評価 (内挿予測) # クラス分類の内挿予測 pcls_sp_rp &lt;- predict(sp2_rp, sp500_2, type = &quot;class&quot;) # クラス分類の予測結果出力 # 正解率 mean(pcls_sp_rp == sp500_2$Direction) #&gt; [1] 0.6152 # 混同行列 tbl_rp &lt;- table(pcls_sp_rp, sp500_2$Direction) 自主課題 上で得られた決定木 (分類規則) を解釈してみよう. パッケージcaretの関数confusionMatrix()を使って, 混同行列より各種評価指標を計算する. 2種類の表示モードで結果を示す: 適合率 (precision) - 再現率 (recall) 表示 感度(sensitivity) - 特異度 (specificity) 表示 library(caret) # 適合率 (precision) - 再現率 (recall) 表示 confusionMatrix(tbl_rp, mode = &quot;prec_recall&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; pcls_sp_rp Down Up #&gt; Down 313 192 #&gt; Up 289 456 #&gt; #&gt; Accuracy : 0.6152 #&gt; 95% CI : (0.5876, 0.6423) #&gt; No Information Rate : 0.5184 #&gt; P-Value [Acc &gt; NIR] : 3.483e-12 #&gt; #&gt; Kappa : 0.2249 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 1.202e-05 #&gt; #&gt; Precision : 0.6198 #&gt; Recall : 0.5199 #&gt; F1 : 0.5655 #&gt; Prevalence : 0.4816 #&gt; Detection Rate : 0.2504 #&gt; Detection Prevalence : 0.4040 #&gt; Balanced Accuracy : 0.6118 #&gt; #&gt; &#39;Positive&#39; Class : Down #&gt; # 感度(sensitivity) - 特異度 (specificity) 表示 confusionMatrix(tbl_rp) # mode = &quot;sens_spec&quot; (デフォルト) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; pcls_sp_rp Down Up #&gt; Down 313 192 #&gt; Up 289 456 #&gt; #&gt; Accuracy : 0.6152 #&gt; 95% CI : (0.5876, 0.6423) #&gt; No Information Rate : 0.5184 #&gt; P-Value [Acc &gt; NIR] : 3.483e-12 #&gt; #&gt; Kappa : 0.2249 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 1.202e-05 #&gt; #&gt; Sensitivity : 0.5199 #&gt; Specificity : 0.7037 #&gt; Pos Pred Value : 0.6198 #&gt; Neg Pred Value : 0.6121 #&gt; Prevalence : 0.4816 #&gt; Detection Rate : 0.2504 #&gt; Detection Prevalence : 0.4040 #&gt; Balanced Accuracy : 0.6118 #&gt; #&gt; &#39;Positive&#39; Class : Down #&gt; パッケージcaretは, 多種多様な機械学習アルゴリズムを統一的な環境で 実行し比較できる環境を提供する. サポートベクターマシン (SVM), 勾配ブースティング, ランダムフォレスト, ニューラルネット, …, など多様な手法・アルゴリズムをサポートする. 以上のSmarketを用いた決定木分析での留意点として, 以下が挙げられる. 連続変数を分割する点の妥当性 (ロバストか?) → 離散変数 (特に2値) に変換するか? Volumeのラグ変数を加えるか? (そのままでは不可) "],["アンサンブル学習.html", "9 アンサンブル学習 9.1 決定木 (分類木) 9.2 ランダムフォレスト分類木 9.3 一般化ブースト回帰", " 9 アンサンブル学習 個々には必ずしも精度の高くない予測モデル (“弱い学習器”) の結果を集約することで, より精度の高い予測モデル (“強い学習器”) を構築する手法をアンサンブル学習と呼ぶ. これは, “集団の叡智”を計算機内に実装したもので, 分類問題 → 個々の回答の多数決 回帰問題 → 個々の予測値の平均値 によって, 個々の予測値を集約する. 近年の計算機パワーの向上と伴に, 研究・実用化とも急拡大している. 主な技法として, バギング (bagging), ブースティング (Boosting), スタッキング (Stacking) がある. 本章では, 決定木を“弱い学習器”とするアンサンブル学習のアルゴリズムとして, 機械学習分野で多用されている, ランダムフォレスト (Random Forest) と 勾配ブースティング木 (Gradient Boosting/Boosted Tree) について取り上げる. 以下では, ランダムフォレスト, 勾配ブースティング木ともに分類問題のみを扱うが, これらはいずれも回帰問題にも対応することができる. 9.1 決定木 (分類木) 前章では, パッケージrpartの関数rpart()を使用したが, ここでは, 別のパッケージtreeの関数tree()を使用する. データセット3: 個人ローン信用データ (再掲) データセットは, german creditデータを使用する. 目的変数としては, 個人の2水準 {Bad, Good} の信用状態を示す 変数であるCustomerを選択し, これを予測するモデルを 決定木により構築する. データ入手先: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data ※ 授業用に一部修正 credit_df &lt;- read.csv(&quot;german_credit_modified.csv&quot;, header = T, skip = 3, quote = &quot;&quot;, stringsAsFactors = T) # 目的変数(Customer)はfactor型にしないとエラー発生: stringsAsFactors = Tを指定のこと str(credit_df) #&gt; &#39;data.frame&#39;: 1000 obs. of 21 variables: #&gt; $ StatusAccount : Factor w/ 4 levels &quot;ge_200DM&quot;,&quot;lt_0DM&quot;,..: 2 3 4 2 2 4 4 3 4 3 ... #&gt; $ DurationMonth : int 6 48 12 42 24 36 24 36 12 30 ... #&gt; $ CreditHistory : Factor w/ 5 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 5 3 5 3 4 3 3 3 3 5 ... #&gt; $ Purpose : Factor w/ 6 levels &quot;TV_etc&quot;,&quot;business&quot;,..: 1 1 4 1 3 4 1 3 1 3 ... #&gt; $ CreditAmount : int 1169 5951 2096 7882 4870 9055 2835 6948 3059 5234 ... #&gt; $ SavingsAccount : Factor w/ 4 levels &quot;ge_1000DM&quot;,&quot;lt_1000DM&quot;,..: NA 3 3 3 3 NA 2 3 1 3 ... #&gt; $ EmploymentSince : Factor w/ 5 levels &quot;ge_7yrs&quot;,&quot;lt_1yr&quot;,..: 1 3 4 4 3 3 1 3 4 5 ... #&gt; $ InstallmentRate : int 4 2 2 2 3 2 3 2 2 4 ... #&gt; $ StatusAndSex : Factor w/ 4 levels &quot;F_divorced&quot;,&quot;M_divorced&quot;,..: 4 1 4 4 4 4 4 4 2 3 ... #&gt; $ Guarantors : Factor w/ 3 levels &quot;co-applicant&quot;,..: 3 3 3 2 3 3 3 3 3 3 ... #&gt; $ ResidenceSince : int 4 2 3 4 4 4 4 2 4 2 ... #&gt; $ Property : Factor w/ 3 levels &quot;car_etc&quot;,&quot;life_insurance&quot;,..: 3 3 3 2 NA NA 2 1 3 1 ... #&gt; $ Age : int 67 22 49 45 53 35 53 35 61 28 ... #&gt; $ InstallmentPlans: Factor w/ 3 levels &quot;bank&quot;,&quot;none&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ Housing : Factor w/ 3 levels &quot;free&quot;,&quot;own&quot;,&quot;rent&quot;: 2 2 2 1 1 1 2 3 2 2 ... #&gt; $ NCredits : int 2 1 1 1 2 1 1 1 1 2 ... #&gt; $ Job : Factor w/ 4 levels &quot;management&quot;,&quot;skilled&quot;,..: 2 2 4 2 2 4 2 1 4 1 ... #&gt; $ NPeopleMain : int 1 1 2 2 2 2 1 1 1 1 ... #&gt; $ Phone : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 2 1 2 1 1 ... #&gt; $ ForeignWorker : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ Customer : Factor w/ 2 levels &quot;bad&quot;,&quot;good&quot;: 2 1 2 2 1 2 2 2 2 1 ... 欠損値の有無を確認し, 必要な処理を行う. 今回の分析では, 欠損値は補間せずにレコードごと除去することにする. library(Amelia) missmap(credit_df, col = c(&#39;yellow&#39;, &#39;black&#39;), y.at = 1, y.labels = &#39;&#39;, legend = TRUE) # Amelia # 欠損値処理 → 今回は除去 ok &lt;- complete.cases(credit_df) # 欠損値を持つ項目のないcase (データ点)はT, さもなくばFを返す credit_df_noNA &lt;- credit_df[ok, ] なお, 以下で使用するrandomForest()やgbm()に対しては不要であるが, 関数によっては, データフレーム内にある factor型を持つ質的変数はそのままでは受け付けずに, 一旦ダミー変数に変換する必要のあるものもある. # model.matrixは, 質的変数 → ダミー化 x &lt;- model.matrix(Customer ~ ., credit_df_noNA)[, -1] # factorのdummy変数化 + (intercept)を除去 y &lt;- credit_df_noNA$Customer # 以下のtree関数, randomForest関数, gbm関数は, factor型変数の使用可 (ダミー化は不要) 学習データ/ テストデータの分割 set.seed(1) train &lt;- sample(1:nrow(credit_df_noNA), 500) # x.train &lt;- x[train, ] # y.train &lt;- y[train] 決定木の構築 パッケージtreeを読み込み, 関数tree()を用いて分類木を構築する. # パッケージtree, 関数tree()の使用 library(tree) # tree, cv.tree, prune.tree fit_tree &lt;- tree(Customer ~ ., data = credit_df_noNA, subset = train) # split = &quot;gini&quot; (デフォルト&quot;deviance)) summary(fit_tree) #&gt; #&gt; Classification tree: #&gt; tree(formula = Customer ~ ., data = credit_df_noNA, subset = train) #&gt; Variables actually used in tree construction: #&gt; [1] &quot;StatusAccount&quot; &quot;DurationMonth&quot; &quot;Guarantors&quot; &quot;Property&quot; #&gt; [5] &quot;Age&quot; &quot;SavingsAccount&quot; &quot;CreditHistory&quot; &quot;EmploymentSince&quot; #&gt; [9] &quot;Purpose&quot; &quot;InstallmentPlans&quot; &quot;InstallmentRate&quot; #&gt; Number of terminal nodes: 14 #&gt; Residual mean deviance: 0.9171 = 445.7 / 486 #&gt; Misclassification error rate: 0.228 = 114 / 500 fit_tree #&gt; node), split, n, deviance, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 500 619.100 good ( 0.31000 0.69000 ) #&gt; 2) StatusAccount: lt_0DM,lt_200DM 272 373.300 good ( 0.44118 0.55882 ) #&gt; 4) DurationMonth &lt; 15.5 120 146.600 good ( 0.30000 0.70000 ) #&gt; 8) Guarantors: co-applicant,none 104 134.200 good ( 0.34615 0.65385 ) #&gt; 16) Property: car_etc,life_insurance 61 84.150 good ( 0.45902 0.54098 ) * #&gt; 17) Property: real_estate 43 41.320 good ( 0.18605 0.81395 ) * #&gt; 9) Guarantors: guarantor 16 0.000 good ( 0.00000 1.00000 ) * #&gt; 5) DurationMonth &gt; 15.5 152 209.000 bad ( 0.55263 0.44737 ) #&gt; 10) Age &lt; 62 147 200.800 bad ( 0.57143 0.42857 ) #&gt; 20) SavingsAccount: lt_1000DM,lt_100DM 123 163.600 bad ( 0.61789 0.38211 ) * #&gt; 21) SavingsAccount: ge_1000DM,lt_500DM 24 30.550 good ( 0.33333 0.66667 ) #&gt; 42) CreditHistory: A,C 15 20.730 bad ( 0.53333 0.46667 ) #&gt; 84) Age &lt; 27 6 0.000 bad ( 1.00000 0.00000 ) * #&gt; 85) Age &gt; 27 9 9.535 good ( 0.22222 0.77778 ) * #&gt; 43) CreditHistory: B,D,E 9 0.000 good ( 0.00000 1.00000 ) * #&gt; 11) Age &gt; 62 5 0.000 good ( 0.00000 1.00000 ) * #&gt; 3) StatusAccount: ge_200DM,none 228 195.500 good ( 0.15351 0.84649 ) #&gt; 6) EmploymentSince: lt_1yr,unemployed 48 61.110 good ( 0.33333 0.66667 ) #&gt; 12) Purpose: business,repairs 8 6.028 bad ( 0.87500 0.12500 ) * #&gt; 13) Purpose: TV_etc,car,education 40 42.650 good ( 0.22500 0.77500 ) * #&gt; 7) EmploymentSince: ge_7yrs,lt_4yrs,lt_7yrs 180 121.400 good ( 0.10556 0.89444 ) #&gt; 14) InstallmentPlans: bank,store 33 38.670 good ( 0.27273 0.72727 ) * #&gt; 15) InstallmentPlans: none 147 73.060 good ( 0.06803 0.93197 ) #&gt; 30) CreditHistory: C,D 79 56.030 good ( 0.11392 0.88608 ) #&gt; 60) InstallmentRate &lt; 3.5 49 16.710 good ( 0.04082 0.95918 ) * #&gt; 61) InstallmentRate &gt; 3.5 30 32.600 good ( 0.23333 0.76667 ) * #&gt; 31) CreditHistory: A,B,E 68 10.420 good ( 0.01471 0.98529 ) * # 木の表示 plot(fit_tree) text(fit_tree, pretty = 0) 木の成長のコントロール (停止条件の設定) tree()によって生成する木の大きさは, tree.control()によってコントロールすることが出来る. 指定した条件に引っ掛かりそれ以上分割するノードが なくなった時点でアルゴリズムが終了する. tree.control(): 木の成長をコントロールするパラメータ (特に, mincutとminsize) の設定 - usage: tree.control(nobs, mincut = 5, minsize = 10, mindev = 0.01) - 主なパラメータ: - nobs 学習セットの観測数 - mincut 各ノードを分割するために必要な最小観測数. 最大で minsize/2 まで. 値が小さいほど, より細かい分割が行われる (デフォルト=5) - minsize 末端ノードのサイズの最小値. 値が大きいと, ツリーは浅くなる (デフォルト=10) - mindev 各ノードのノード内逸脱度 (within-node deviance)が, ルートノードの逸脱度に対して少なくともこの値の割合以上である場合にのみ, そのノードが分割されることを示す. 例. mindev=0.01の場合, ノードが分割されるためにはそのノードの逸脱度がルートノードの逸脱度の1%以上である必要があある. これにより, すでに十分小さい逸脱度を持つノードが無駄に分割されるのを防ぐ. # 決定木のコントロールパラメータの設定 control_params &lt;- tree.control(nobs = nrow(credit_df_noNA), mincut = 3, minsize = 6, mindev = 0.01) # 決定木の構築 fit_tree2 &lt;- tree(Customer ~ ., data = credit_df_noNA, control = control_params) # 決定木のプロット plot(fit_tree2) text(fit_tree2, pretty = 0) # 決定木の詳細を表示 summary(fit_tree2) #&gt; #&gt; Classification tree: #&gt; tree(formula = Customer ~ ., data = credit_df_noNA, control = control_params) #&gt; Variables actually used in tree construction: #&gt; [1] &quot;StatusAccount&quot; &quot;DurationMonth&quot; &quot;CreditHistory&quot; &quot;Purpose&quot; #&gt; [5] &quot;EmploymentSince&quot; &quot;CreditAmount&quot; &quot;Guarantors&quot; #&gt; Number of terminal nodes: 10 #&gt; Residual mean deviance: 0.9739 = 667.1 / 685 #&gt; Misclassification error rate: 0.2245 = 156 / 695 木の剪定 関数cv.tree()を使えば, 交差検証 (CV) による最適な木の大きさを見つけることができる. # 木の剪定がパフォーマンスを改善するか? # 交差検証により木 (の複雑度パラメータcp) を選択する場合 # (cv.treeから, prune.treeを呼び出して実行) set.seed(1) cvfit_tree &lt;- cv.tree(fit_tree) # デフォルト: method = &quot;deviance&quot;, 10-folds CV plot(cvfit_tree$size, cvfit_tree$dev, type = &#39;b&#39;) # index of tree with minimum error cvfit_tree$size[which.min(cvfit_tree$dev)] # 末端ノードの数 #&gt; [1] 2 → デフォルトのmethod = “deviance”では, size = 2が最良 (単純過ぎ?). cvfit_tree &lt;- cv.tree(fit_tree, method = &quot;misclass&quot;) # cvfit_tree$size[which.min(cvfit_tree$dev)] # 末端ノードの数 #&gt; [1] 5 → method = “misclass”を選ぶと, size = 3が最良となる. 関数prune.tree()を使い, 手動で木を剪定することもできる. prune.tree()の主なパラメータ: - k cp (cost-complexity parameter) パラメータ - best (希望する) 木のサイズ (末端ノード数). (kを指定する代わりに指定可) - method ノードの非均一性の指標. デフォルト=&quot;deviance&quot;. 回帰木は&quot;deviance&quot;のみ. 分類木では&quot;misclass&quot; (誤分類の数) も選択可能 # prune.tree関数による剪定 #prn_tree &lt;- prune.tree(fit_tree, k = 11) # k: cost-complexity parameter, または, bestの指定 prn_tree &lt;- prune.tree(fit_tree, method = &quot;misclass&quot;, best = 8) # best: 末端ノード数 summary(prn_tree) #&gt; #&gt; Classification tree: #&gt; snip.tree(tree = fit_tree, nodes = c(4L, 7L)) #&gt; Variables actually used in tree construction: #&gt; [1] &quot;StatusAccount&quot; &quot;DurationMonth&quot; &quot;Age&quot; &quot;SavingsAccount&quot; #&gt; [5] &quot;CreditHistory&quot; &quot;EmploymentSince&quot; &quot;Purpose&quot; #&gt; Number of terminal nodes: 9 #&gt; Residual mean deviance: 0.9976 = 489.8 / 491 #&gt; Misclassification error rate: 0.228 = 114 / 500 plot(prn_tree) # &quot;deviance&quot;/&quot;msiclass&quot;で結果が異なる. cvとも異なる text(prn_tree, pretty = 0) テスト用データセットを用いて外挿予測 ここでは, 適合された木 (剪定なし) を用いて, テスト用データに対する予測を行う. デフォルトではpredict()は確率を出力してしまうため, ここではを引数type=classを指定する. #yhat_tree &lt;- predict(fit_tree, newdata = credit_df_noNA[-train, ]) # --&gt; 確率 yhat_tree &lt;- predict(fit_tree, newdata = credit_df_noNA[-train, ], type = &quot;class&quot;) # --&gt; good/bad 混同行列によるパフォーマンス評価 混同行列を作成し, パッケージcaretに含まれる関数confusionMatrix()を使用して, 各種パフォーマンス指標を計算する. この関数の出力オブジェクトから, accuracyや, F1 (“F尺度”) 等の特定の指標の値を取り出すことが出来る. (tbl_tree &lt;- table(yhat_tree, credit_df_noNA$Customer[-train])) #&gt; #&gt; yhat_tree bad good #&gt; bad 24 22 #&gt; good 29 120 library(caret) # confusionMatrix confusionMatrix(tbl_tree, mode = &quot;prec_recall&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; yhat_tree bad good #&gt; bad 24 22 #&gt; good 29 120 #&gt; #&gt; Accuracy : 0.7385 #&gt; 95% CI : (0.6709, 0.7986) #&gt; No Information Rate : 0.7282 #&gt; P-Value [Acc &gt; NIR] : 0.4091 #&gt; #&gt; Kappa : 0.3108 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.4008 #&gt; #&gt; Precision : 0.5217 #&gt; Recall : 0.4528 #&gt; F1 : 0.4848 #&gt; Prevalence : 0.2718 #&gt; Detection Rate : 0.1231 #&gt; Detection Prevalence : 0.2359 #&gt; Balanced Accuracy : 0.6490 #&gt; #&gt; &#39;Positive&#39; Class : bad #&gt; (confusionMatrix(tbl_tree)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.7384615 (confusionMatrix(tbl_tree, mode = &quot;prec_recall&quot;)$byClass[&quot;F1&quot;]) #&gt; F1 #&gt; 0.4848485 → 木の大きさを変えることで, 改善の余地がある. 自主課題 ハイパーパラメータを変えて, パフォーマンスを比較してみよう. 9.2 ランダムフォレスト分類木 パッケージrandomForestの関数randomForest()を使用する. randomForest(): 重要なパラメータ - mtry: 各分割においてランダムに抽出される変数の数 (デフォルト: 分類はsqrt(p), 回帰はp/3) - nodesize: 末端ノードの最小サイズ (大 → 小さな木) - maxnodes: 末端ノードの最大個数 - ntree: 生成する木の数 (各レコードが複数回学習に使われるように, 小さくし過ぎない) ランダムフォレストの実行 library(randomForest) set.seed(1) fit_rf &lt;- randomForest(Customer ~ ., data = credit_df_noNA, subset = train, mtry = 6, importance = TRUE) テスト用データを用いて外挿予測 yhat_rf &lt;- predict(fit_rf, newdata = credit_df_noNA[-train, ]) # summary(fit_rf) #&gt; Length Class Mode #&gt; call 6 -none- call #&gt; type 1 -none- character #&gt; predicted 500 factor numeric #&gt; err.rate 1500 -none- numeric #&gt; confusion 6 -none- numeric #&gt; votes 1000 matrix numeric #&gt; oob.times 500 -none- numeric #&gt; classes 2 -none- character #&gt; importance 80 -none- numeric #&gt; importanceSD 60 -none- numeric #&gt; localImportance 0 -none- NULL #&gt; proximity 0 -none- NULL #&gt; ntree 1 -none- numeric #&gt; mtry 1 -none- numeric #&gt; forest 14 -none- list #&gt; y 500 factor numeric #&gt; test 0 -none- NULL #&gt; inbag 0 -none- NULL #&gt; terms 3 terms call 混同行列を作成し, accuracyや, F1 (“F尺度”) の数値を取り出す. (tbl_rf &lt;- table(yhat_rf, credit_df_noNA$Customer[-train])) #&gt; #&gt; yhat_rf bad good #&gt; bad 23 14 #&gt; good 30 128 (confusionMatrix(tbl_rf)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.774359 (confusionMatrix(tbl_rf, mode = &quot;prec_recall&quot;)$byClass[&quot;F1&quot;]) #&gt; F1 #&gt; 0.5111111 → パラメータ・チューニング(mtry, nodesize/maxnodes等)より, 改善の余地有り 自主課題 ハイパーパラメータを変えて, パフォーマンスを比較してみよう. 各変数の重要度 # 各変数の重要度 importance(fit_rf) #&gt; bad good MeanDecreaseAccuracy MeanDecreaseGini #&gt; StatusAccount 18.2406750 16.4946426 23.4101092 22.7732973 #&gt; DurationMonth 8.6191619 13.7539806 16.8054128 23.4872569 #&gt; CreditHistory 2.5123173 4.9885214 5.6602350 11.4384420 #&gt; Purpose 3.9812876 2.0171391 3.7749554 11.7082009 #&gt; CreditAmount 2.3306101 8.7839987 9.0502101 30.1623368 #&gt; SavingsAccount 5.9954796 -0.1269385 3.5280400 6.7990910 #&gt; EmploymentSince 1.8684738 4.4793237 4.8717406 14.4204958 #&gt; InstallmentRate 1.0644703 4.4672097 4.1244434 8.9624679 #&gt; StatusAndSex 1.7758577 0.4215884 1.4385134 8.8589038 #&gt; Guarantors 4.8469105 8.1675310 9.0020391 5.6785251 #&gt; ResidenceSince 1.0304818 0.4797195 1.0480306 8.7191197 #&gt; Property -1.4386017 7.7430478 5.3764627 8.0519616 #&gt; Age 4.9327720 5.0869132 7.2074345 24.1410112 #&gt; InstallmentPlans 1.2617704 1.7176788 2.0252865 5.9231219 #&gt; Housing -0.3777576 2.1536104 1.4887341 3.6019637 #&gt; NCredits -0.1365011 3.8979401 3.3878630 4.7944978 #&gt; Job 0.9970704 3.4870177 3.6369344 7.0219383 #&gt; NPeopleMain -1.3613446 0.5956468 -0.1577331 2.4535740 #&gt; Phone 1.8034420 -0.3854100 0.7655450 3.2441279 #&gt; ForeignWorker 0.2520411 -1.6396991 -1.1523368 0.7819814 varImpPlot(fit_rf) -&gt; 重要な変数として, Credit Amount, Age, DurationMonth, Status Account, EmploymentSince, … ROC曲線の可視化, およびAUC指標の計算 # ROC曲線, AUC library(ROCR) # performance yprob_rf &lt;- predict(fit_rf, newdata = credit_df_noNA[-train, ], type = &quot;prob&quot;) pred_rf &lt;- prediction(apply(yprob_rf, 1, max), credit_df_noNA$Customer[-train]) perf_rf &lt;- performance(pred_rf, &quot;tpr&quot;, &quot;fpr&quot;) plot(perf_rf) auc_rf &lt;- performance(pred_rf, &quot;auc&quot;) (auc &lt;- as.numeric(auc_rf@y.values)) #&gt; [1] 0.7725884 9.3 一般化ブースト回帰 パッケージgbm, 関数gbm()を用いて, 勾配ブースティング木 (gradient boosting/boosted tree) を実行する. 注: `gbm()`関数は, generalized boosted regression modelを適合する関数であるとマニュアルで説明されている. この関数では, 分布 (引数`distribution`) として, “gaussian”の他に, “bernoulli” (0/1), “possion” (カウント)等をカバーすることから,「一般化線形モデル (GLM)」における”generalized”の使い方をしていると思われる. なお, 標準的には, GBTの”G”は, 勾配 (Gradient) の”G”に対応し, “勾配ブースティング回帰 (”gradient boosting (または boosted) tree regression”) を表すと理解されるので, 注意が必要である. gbm()にデータセットを与える際, 事前に目的変数の値は {0,1}に変換しておく必要がある. # 予め反応変数を{0,1}に変換しておく credit_df_01 &lt;- credit_df_noNA # credit_df_01$Customer2 &lt;- model.matrix( ~ Customer, data = credit_df_noNA)[, -1] # 0:bad, 1:good credit_df_01$Customer2 &lt;- model.matrix( ~ Customer - 1, data = credit_df_noNA)[, -2] # 0:good, 1:bad credit_df_01$Customer &lt;- NULL # good → 0, bad → 1 GBTにおいては, パラメータの選択がパフォーマンスに大きく左右する. gbm(): 重要なパラメータ - n.trees 適合する木の総数=繰り返し計算回数 - interaction.depth 各木の最大深さ - shrnkage 縮小パラメータ (学習レート/ステップサイズ減少幅) (デフォルト = 0.1) - bag.fraction 次の木生成のためランダムに抽出される学習セットの割合 (デフォルト = 0.5). モデル適合にランダムネスを導入 GBMの実行 # gbm関数を分類問題に適用 (distribution = &quot;bernoulli&quot;) library(gbm) set.seed(1) fit_gbm &lt;- gbm(Customer2 ~ ., data = credit_df_01[train, ], distribution = &quot;bernoulli&quot;, n.trees = 5000, interaction.depth = 4, shrinkage = 0.001) summary(fit_gbm) # プロットの出力: plotit = T (デフォルト) #&gt; var rel.inf #&gt; StatusAccount StatusAccount 16.1322116 #&gt; CreditAmount CreditAmount 14.8832384 #&gt; DurationMonth DurationMonth 13.5681922 #&gt; Age Age 9.2205182 #&gt; EmploymentSince EmploymentSince 7.8868283 #&gt; Purpose Purpose 5.2379561 #&gt; CreditHistory CreditHistory 5.1830782 #&gt; InstallmentRate InstallmentRate 3.7427905 #&gt; InstallmentPlans InstallmentPlans 3.7201558 #&gt; SavingsAccount SavingsAccount 3.3310716 #&gt; Guarantors Guarantors 3.1216665 #&gt; StatusAndSex StatusAndSex 2.9160491 #&gt; Property Property 2.7019140 #&gt; ResidenceSince ResidenceSince 2.4379173 #&gt; Job Job 2.3755654 #&gt; Housing Housing 2.0459670 #&gt; NCredits NCredits 0.5883543 #&gt; Phone Phone 0.5773367 #&gt; NPeopleMain NPeopleMain 0.2193557 #&gt; ForeignWorker ForeignWorker 0.1098330 # 各変数の相対的影響度 (relative influence): 損失関数 (二乗誤差) の減少幅 plot(fit_gbm) partial dependence plot (PDP) PDPは, (自分以外の変数について積分して除去することで) 1変数の反応変数に対する効果を可視化するXAI (Explainable AI) の一手法である. ここでは, 重要な2変数であるAge, CreditAmountについて PDPを作成する. # partial dependence plot (PDP) # 最重要な2変数について par(mfrow = c(1, 2)) plot(fit_gbm, i = &quot;Age&quot;) plot(fit_gbm, i = &quot;CreditAmount&quot;) 自主課題 他の変数についても, PDPを作図してみよう テストデータにより外挿予測 predict()において引数type = \"response\"を指定すると, 先のgbm()実行時にdistribution=\"bernoulli\"と指定していた場合には, 確率が返される. そこで, これを目的変数 ({0,1}) の予測に変換するために, 閾値を設定し大小での2値分類を行う. また, predict()では, 予測に用いるため gbm()で得られた木の数をn.treesで指定する. yprob_gbm &lt;- predict(fit_gbm, newdata = credit_df_01[-train, ], n.trees = 5000, type = &quot;response&quot;) # (現状) type = &quot;response&quot;: &quot;bernoulli&quot;では, 確率を返す # → 2値{0,1}予測するには, 閾値を指定して変換(!) thrshld &lt;- 0.5 yhat_gbm &lt;- ifelse(yprob_gbm &gt; thrshld, 1, 0) # yhat_gbm &lt;- predict(fit_gbm, newdata = credit_df_01[-train, ], n.trees = 5000) # head(yhat_gbm) y_test &lt;- credit_df_01[-train, &quot;Customer2&quot;] # 0/1 上と同様に, 混同行列を作成し, accuracyや, F1 (“F尺度”) の数値を取り出す. (tbl_gbm &lt;- table(yhat_gbm, y_test)) #&gt; y_test #&gt; yhat_gbm 0 1 #&gt; 0 129 35 #&gt; 1 13 18 (confusionMatrix(tbl_gbm)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.7538462 (confusionMatrix(tbl_gbm, mode = &quot;prec_recall&quot;)$byClass[&quot;F1&quot;]) #&gt; F1 #&gt; 0.8431373 → shrinkage, interaction.depth等のパラメータ・チューニングにより, パフォーマンスを改善できる余地が有る. 自主課題 ハイパーパラメータを変えて, パフォーマンスを比較してみよう. ROC曲線, AUC指標 # ROC曲線, AUC pred_gbm &lt;- prediction(yprob_gbm, y_test) perf_gbm &lt;- performance(pred_gbm, &quot;tpr&quot;, &quot;fpr&quot;) plot(perf_gbm) auc_gbm &lt;- performance(pred_gbm, &quot;auc&quot;) (auc &lt;- as.numeric(auc_gbm@y.values)) #&gt; [1] 0.8070688 自主課題 0/1予測の閾値を変えて, パフォーマンスを比較してみよう 自主課題 bad/goodの符号化 (0/1) の際, 0/1を入れ替えるとどうなるか試してみよう credit_df_01$Customer2 &lt;- model.matrix( ~ Customer - 1, data = credit_df_noNA)[, -1] # 0:bad, 1:good 注)上述の手法と合わせる為に, 予め学習データとテスト分割を分割 (固定) したが, gbmパッケージ内で, 分割→テストする機能有り "],["サポートベクターマシン-svm.html", "10 サポートベクターマシン (SVM) 10.1 SVM (RBFカーネル) の動作の確認 10.2 2値分類 10.3 3値分類 10.4 代替パッケージe1071の利用 10.5 ハイパーパラメータのチューニング", " 10 サポートベクターマシン (SVM) サポートベクターマシン (SVM) は, 説明変数の空間を分割する決定境界を データより直接推定するアプローチである識別モデル (discriminative model) と呼ばれる クラス分類問題解決手法の一つである. SVMは, 原理的には, 2クラス分類問題に対して適用される. 他クラス分類問題に対しては, 複数の2クラス問題に分割し, 各々にSVMを適合して得られた個々の分類結果を統合する形で解決を図る. SVMは, 多次元の説明変数の空間上に布置された2クラスの点の集合に対して, “マージン” (境界面から一番近い点までの距離) が最大となるという意味において, 2グループができるだけうまく分離されるような平面 (境界面) を引くことを試みる. その際, 完全な分離ではなく, 分類の失敗 (誤判別) やマージン違反を許容するような 平面を引く. 許容の程度はハイパーパラメータを通じて分析者がSVMに与える. また, SVMは, カーネル関数を上手に選択することで, 説明変数空間における 境界面がドーナツ型を含む非線形な形状をしているようなケースにも柔軟に対応することができる. どのように非線形なケースに対応するかについてその原理を大雑把に述べると, 数学的には, 再生核ヒルベルト空間 (RKHS) 理論に基づいている データ点が配置されている元の空間から, 非線形な空間に拡張される 注: 非線形な写像によって高次元 (または無限次元空間) へマッピングされる その非線形な世界で, 線形な平面を引いて分割する 注: 非線形な空間 (RKHS) では線形な識別関数 (すなわち, 超平面) を学習 すると, 元の空間で非線形な分割がなされている 注: 高次元空間で線形な分割境界は, 元の空間に戻すと非線形な境界 (非線形SVM) 各データ点を非線形な空間にマップするような非線形な変換のことは考える必要ない (実際には計算不要). よって, 非線形な変換のことは忘れて (無視して), カーネル関数を選べば良い (“カーネルトリック”) ちなみに, 本章では扱わないが, SVMは, 回帰問題にも応用することができる (サポートベクター回帰). そこでは, やはりカーネル関数をうまく選択することで説明変数の元の空間内では非線形な形状の適合回帰平面を得ることができる. また, 分類問題におけるマージン違反の許容に対応するものとして, 最小二乗法の代わりに 回帰残差が閾値以上の絶対値を持つもののみを残すような損失関数を最小化する問題を問いている. なお, SVMにおいても, 学習データに対するモデルの過剰・過小学習を避けるには, 交差検証などによりパラメータ・チューニングを適切行う必要がある. 本章では, SVMの中でも, 恐らくもっともポピュラーな (Gaussian) RBFカーネルの操作に限定して説明する. 本章では, パッケージkernlabの関数ksvm(), e1071のsvm()を用いて, SVMを動かしてみる. また, 異なるモデル同士のパフォーマンスを同じ“土俵”で比較したり, 各モデルのパラメータ・チューニングを共通のやり方で行えるような, Rによる機械学習実装の“プラットフォーム”として代表的なパッケージ caretの機能の一部も紹介する. 10.1 SVM (RBFカーネル) の動作の確認 データセット1: iris (あやめ) データ iris - 150 x 4 - Sepal.Length (萼片の長さ) (cm) - Sepal.Width (萼片の幅 (cm) - Petal.Length (花弁の長さ) (cm) - Petal.Width (花弁の幅 (cm) - Species (あやめの種類): Setosa, Versicolour, Virginica - ※ petal 花弁 (花びら), sepal: 萼片 (がくへん) - Introduced in Ronald A. Fisher (1934) - cf. https://rpubs.com/vidhividhi/irisdataeda SVM (RBFカーネル) の特徴を理解するため, もともとは 目的変数Speciesは3分類 (50件x3) を持つが, ここでは 2分類に限定し, かつ, 予測変数も2つに絞って分析を行う iris1 &lt;- iris[51:150, 3:5] # (Petal.Length, Petal.Width, Species) iris1$Species &lt;- droplevels(iris1$Species) SVMの実行 パッケージkernlab, 関数ksvm()を使用する. library(kernlab) iris_ksvm &lt;- ksvm(Species ~ ., data = iris1) # デフォルト (kpar = &quot;automatic&quot;) table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 47 2 #&gt; virginica 3 48 plot(iris_ksvm, data = iris1[, 1:2]) 2予測変数 + 2値分類のケース → 境界を可視化できる. 黒点…サポートベクター ハイパーパラメータ (C, sigma) の値を変えて実行してみる. 学習データを (内挿) 予測させ分離境界を描く. iris_ksvm &lt;- ksvm(Species ~ ., data = iris1, kpar = list(sigma = 10)) # → より複雑 table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 49 1 #&gt; virginica 1 49 plot(iris_ksvm, data = iris1[, 1:2]) iris_ksvm &lt;- ksvm(Species ~ ., data = iris1, kpar = list(sigma = 0.1)) # → より大雑把 table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 47 3 #&gt; virginica 3 47 plot(iris_ksvm, data = iris1[, 1:2]) iris_ksvm &lt;- ksvm(Species ~ ., data = iris1, kpar = list(sigma = 0.1), C = 10) # → より少ないSV table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 47 2 #&gt; virginica 3 48 plot(iris_ksvm, data = iris1[, 1:2]) iris_ksvm &lt;- ksvm(Species ~ ., data = iris1, kpar = list(sigma = 0.1), C = 1) plot(iris_ksvm, data = iris1[, 1:2]) table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 47 3 #&gt; virginica 3 47 iris_ksvm &lt;- ksvm(Species ~ ., data = iris1, kpar = list(sigma = 0.1), C = 0.1) plot(iris_ksvm, data = iris1[, 1:2]) table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 48 3 #&gt; virginica 2 47 iris_ksvm &lt;- ksvm(Species ~ ., data = iris1, kpar = list(sigma = 0.1), C = 0.01) # → より多くのSV table(predict(iris_ksvm, iris1[, 1:2]), iris1$Species) #&gt; #&gt; versicolor virginica #&gt; versicolor 48 3 #&gt; virginica 2 47 plot(iris_ksvm, data = iris1[, 1:2]) すなわち, sigma (gamma) 大 → より複雑な分離境界 C (cost) 小 → より多数のsupport vectors が確認される. 10.2 2値分類 データセット2: spamデータ spam: - 4601 x 58 - 目的変数 type (58列目): &quot;nonspam&quot;/&quot;spam&quot; - 予測変数 (1--57) はすべてnumeric - 注) 2値分類問題としては使い易いデータセット library(kernlab) # ksvm() data(spam); dim(spam) # spamデータの分析 #&gt; [1] 4601 58 head(spam) #&gt; make address all num3d our over remove internet order mail receive will #&gt; 1 0.00 0.64 0.64 0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.64 #&gt; 2 0.21 0.28 0.50 0 0.14 0.28 0.21 0.07 0.00 0.94 0.21 0.79 #&gt; 3 0.06 0.00 0.71 0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 #&gt; 4 0.00 0.00 0.00 0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 #&gt; 5 0.00 0.00 0.00 0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 #&gt; 6 0.00 0.00 0.00 0 1.85 0.00 0.00 1.85 0.00 0.00 0.00 0.00 #&gt; people report addresses free business email you credit your font num000 #&gt; 1 0.00 0.00 0.00 0.32 0.00 1.29 1.93 0.00 0.96 0 0.00 #&gt; 2 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0.00 1.59 0 0.43 #&gt; 3 0.12 0.00 1.75 0.06 0.06 1.03 1.36 0.32 0.51 0 1.16 #&gt; 4 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0 0.00 #&gt; 5 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0 0.00 #&gt; 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0 0.00 #&gt; money hp hpl george num650 lab labs telnet num857 data num415 num85 #&gt; 1 0.00 0 0 0 0 0 0 0 0 0 0 0 #&gt; 2 0.43 0 0 0 0 0 0 0 0 0 0 0 #&gt; 3 0.06 0 0 0 0 0 0 0 0 0 0 0 #&gt; 4 0.00 0 0 0 0 0 0 0 0 0 0 0 #&gt; 5 0.00 0 0 0 0 0 0 0 0 0 0 0 #&gt; 6 0.00 0 0 0 0 0 0 0 0 0 0 0 #&gt; technology num1999 parts pm direct cs meeting original project re edu #&gt; 1 0 0.00 0 0 0.00 0 0 0.00 0 0.00 0.00 #&gt; 2 0 0.07 0 0 0.00 0 0 0.00 0 0.00 0.00 #&gt; 3 0 0.00 0 0 0.06 0 0 0.12 0 0.06 0.06 #&gt; 4 0 0.00 0 0 0.00 0 0 0.00 0 0.00 0.00 #&gt; 5 0 0.00 0 0 0.00 0 0 0.00 0 0.00 0.00 #&gt; 6 0 0.00 0 0 0.00 0 0 0.00 0 0.00 0.00 #&gt; table conference charSemicolon charRoundbracket charSquarebracket #&gt; 1 0 0 0.00 0.000 0 #&gt; 2 0 0 0.00 0.132 0 #&gt; 3 0 0 0.01 0.143 0 #&gt; 4 0 0 0.00 0.137 0 #&gt; 5 0 0 0.00 0.135 0 #&gt; 6 0 0 0.00 0.223 0 #&gt; charExclamation charDollar charHash capitalAve capitalLong capitalTotal type #&gt; 1 0.778 0.000 0.000 3.756 61 278 spam #&gt; 2 0.372 0.180 0.048 5.114 101 1028 spam #&gt; 3 0.276 0.184 0.010 9.821 485 2259 spam #&gt; 4 0.137 0.000 0.000 3.537 40 191 spam #&gt; 5 0.135 0.000 0.000 3.537 40 191 spam #&gt; 6 0.000 0.000 0.000 3.000 15 54 spam データセットの2/3を学習用に, 残りをテスト用に分割する. table(spam$type) # [, 58] #&gt; #&gt; nonspam spam #&gt; 2788 1813 set.seed(50) train_idx &lt;- sample(nrow(spam), round(nrow(spam) * 2/3)) spam_train &lt;- spam[train_idx, ] spam_test &lt;- spam[-train_idx, ] SVMの実行 ksvm(): - supports the well known C-svc, nu-svc, (Speciesification) one-Species-svc (novelty) eps-svr, nu-svr (regression) formulations along with native multi-Species Speciesification formulations and the bound-constraint SVM formulations. - also supports Species-probabilities output and confidence intervals for regression. 交差検証 (CV) 実施 # set.seed(50) (spam_ksvm &lt;- ksvm(type ~ ., data = spam_train, cross = 3)) #&gt; Support Vector Machine object of class &quot;ksvm&quot; #&gt; #&gt; SV type: C-svc (classification) #&gt; parameter : cost C = 1 #&gt; #&gt; Gaussian Radial Basis kernel function. #&gt; Hyperparameter : sigma = 0.0278524323556927 #&gt; #&gt; Number of Support Vectors : 1008 #&gt; #&gt; Objective Function Value : -584.0049 #&gt; Training error : 0.04956 #&gt; Cross validation error : 0.074667 学習データに対して3-fold CVによってモデル適合の精度評価 → 最終行, 学習データに対する誤判別率 (CV誤差). ksvm(): - 引数`kernel`: &quot;polydot&quot; (多項式カーネル), &quot;vanilladot&quot; (線形カーネル), &quot;rbfdot&quot; (Gaussianカーネル) (デフォルト=&quot;rbfdot&quot;) - prob.model: 分類問題では, =TRUEの場合, 学習データに対する3-fold CV + シグモイド関数適用によりクラス所属計算. (デフォルト=FALSE) - パラメータの指定方法 → 例: kpar = list(sigma = 0.1) - sigma: kernel幅の逆数 (← 注: gammaに相当) - ksvm()のデフォルト設定: kernel = &quot;rbfdot&quot;, C = 1, kpar = &quot;automatic&quot; (← rbfdotに対しては, sigmaを自動的に決定) - 注) `spam`データの目的変数名`type`と, `ksvm()`の引数の一つ`type`と紛らわしい. 線形カーネル (spam_ksvm &lt;- ksvm(type ~ ., data = spam_train, kernel = &quot;vanilladot&quot;, C = 10, cross = 3, prob.model = T)) # #&gt; Setting default kernel parameters #&gt; Support Vector Machine object of class &quot;ksvm&quot; #&gt; #&gt; SV type: C-svc (classification) #&gt; parameter : cost C = 10 #&gt; #&gt; Linear (vanilla) kernel function. #&gt; #&gt; Number of Support Vectors : 587 #&gt; #&gt; Objective Function Value : -5527.534 #&gt; Training error : 0.064558 #&gt; Cross validation error : 0.072711 #&gt; Probability model included. 出力表示の例 spam_ksvm@cross # CV誤差 spam_ksvm@alpha # 生成されたサポートベクター (alpha vector) spam_ksvm@alphaindex # 同インデックス spam_ksvm@nSV # 生成されたサポートベクターの数 spam_ksvm@coef テストデータに対する外挿予測 &amp; 精度評価 spam_pred &lt;- predict(spam_ksvm, spam_test[, -58]) (spam_tab &lt;- table(spam_pred, spam_test[, 58])) #&gt; #&gt; spam_pred nonspam spam #&gt; nonspam 916 54 #&gt; spam 46 518 # 1-sum(diag(spam.tab))/sum(spam.tab) # 誤判別率, 1-&quot;Accuracy&quot; library(caret) confusionMatrix(spam_tab, mode = &quot;prec_recall&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; spam_pred nonspam spam #&gt; nonspam 916 54 #&gt; spam 46 518 #&gt; #&gt; Accuracy : 0.9348 #&gt; 95% CI : (0.9213, 0.9466) #&gt; No Information Rate : 0.6271 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.8602 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.4839 #&gt; #&gt; Precision : 0.9443 #&gt; Recall : 0.9522 #&gt; F1 : 0.9482 #&gt; Prevalence : 0.6271 #&gt; Detection Rate : 0.5971 #&gt; Detection Prevalence : 0.6323 #&gt; Balanced Accuracy : 0.9289 #&gt; #&gt; &#39;Positive&#39; Class : nonspam #&gt; (confusionMatrix(spam_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.934811 kernelの種類に応じて適切なハイパーパラメータを 引数kparの中で設定する必要. マニュアルでの確認が必要. (コストCの設定はkernelは共通) Gaussianカーネル (spam_ksvm &lt;- ksvm(type ~ ., data = spam_train, C = 10, cross = 3, prob.model = T, kpar = list(sigma = 0.1))) #&gt; Support Vector Machine object of class &quot;ksvm&quot; #&gt; #&gt; SV type: C-svc (classification) #&gt; parameter : cost C = 10 #&gt; #&gt; Gaussian Radial Basis kernel function. #&gt; Hyperparameter : sigma = 0.1 #&gt; #&gt; Number of Support Vectors : 1560 #&gt; #&gt; Objective Function Value : -1548.941 #&gt; Training error : 0.008477 #&gt; Cross validation error : 0.099441 #&gt; Probability model included. テストデータに対する外挿予測 &amp; 精度評価 spam_pred &lt;- predict(spam_ksvm, spam_test[, -58]) (spam_tab &lt;- table(spam_pred, spam_test[, 58])) #&gt; #&gt; spam_pred nonspam spam #&gt; nonspam 913 90 #&gt; spam 49 482 #1-sum(diag(spam_tab))/sum(spam_tab) # 誤判別率, 1-&quot;Accuracy&quot; library(caret) confusionMatrix(spam_tab, mode = &quot;prec_recall&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; #&gt; spam_pred nonspam spam #&gt; nonspam 913 90 #&gt; spam 49 482 #&gt; #&gt; Accuracy : 0.9094 #&gt; 95% CI : (0.8939, 0.9233) #&gt; No Information Rate : 0.6271 #&gt; P-Value [Acc &gt; NIR] : &lt; 2.2e-16 #&gt; #&gt; Kappa : 0.8034 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.0006919 #&gt; #&gt; Precision : 0.9103 #&gt; Recall : 0.9491 #&gt; F1 : 0.9293 #&gt; Prevalence : 0.6271 #&gt; Detection Rate : 0.5952 #&gt; Detection Prevalence : 0.6538 #&gt; Balanced Accuracy : 0.8959 #&gt; #&gt; &#39;Positive&#39; Class : nonspam #&gt; (confusionMatrix(spam_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.9093872 ハイパーパラメータを変えて実行してみる. (spam_ksvm &lt;- ksvm(type ~ ., data = spam_train, C = 0.1, cross = 3, prob.model = T, kpar = list(sigma = 0.1))) #&gt; Support Vector Machine object of class &quot;ksvm&quot; #&gt; #&gt; SV type: C-svc (classification) #&gt; parameter : cost C = 0.1 #&gt; #&gt; Gaussian Radial Basis kernel function. #&gt; Hyperparameter : sigma = 0.1 #&gt; #&gt; Number of Support Vectors : 2210 #&gt; #&gt; Objective Function Value : -168.8087 #&gt; Training error : 0.161396 #&gt; Cross validation error : 0.213562 #&gt; Probability model included. spam_pred &lt;- predict(spam_ksvm, spam_test[, -58]) (spam_tab &lt;- table(spam_pred, spam_test[, 58])) #&gt; #&gt; spam_pred nonspam spam #&gt; nonspam 942 237 #&gt; spam 20 335 (confusionMatrix(spam_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.8324641 (spam_ksvm &lt;- ksvm(type ~ ., data = spam_train, C = 10, cross = 3, prob.model = T, kpar = list(sigma = 10))) #&gt; Support Vector Machine object of class &quot;ksvm&quot; #&gt; #&gt; SV type: C-svc (classification) #&gt; parameter : cost C = 10 #&gt; #&gt; Gaussian Radial Basis kernel function. #&gt; Hyperparameter : sigma = 10 #&gt; #&gt; Number of Support Vectors : 2756 #&gt; #&gt; Objective Function Value : -1362.458 #&gt; Training error : 0.002282 #&gt; Cross validation error : 0.28758 #&gt; Probability model included. spam_pred &lt;- predict(spam_ksvm, spam_test[, -58]) (spam_tab &lt;- table(spam_pred, spam_test[, 58])) #&gt; #&gt; spam_pred nonspam spam #&gt; nonspam 961 365 #&gt; spam 1 207 (confusionMatrix(spam_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.7614081 10.3 3値分類 irisデータセットをオリジナルの3分類のまま使用する. データセットの2/3を学習用に, 残りをテスト用に分割する. 注) irisデータセットは, 件数が150件, 予測変数が4つのスモールデータセットであり, かつ, データの構造が単純であるため, 外挿予測は難しいタスクとは言えないが, 基本的操作を理解することを目的として ここでも使用を続けることにする. set.seed(1) train_idx &lt;- sample(nrow(iris), round(nrow(iris) * 2/3)) iris_train &lt;- iris[train_idx, ] iris_test &lt;- iris[-train_idx, ] SVMの実行 引き続き, ライブラリkernlab, 関数ksvm()を使用する. ksvm()は, クラス数\\(k&gt;2\\)の場合, “One-vs-One”アプローチを実行する. # iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train) # デフォルト (kpar = &quot;automatic&quot;) iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train, kpar = list(sigma = 10)) # → より複雑 (iris_tab &lt;- table(predict(iris_ksvm, iris_test[, 1:4]), iris_test$Species)) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 15 0 0 #&gt; versicolor 0 14 0 #&gt; virginica 1 5 15 (confusionMatrix(iris_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.88 iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train, kpar = list(sigma = 0.1)) # → より大雑把 (iris_tab &lt;- table(predict(iris_ksvm, iris_test[, 1:4]), iris_test$Species)) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 16 0 0 #&gt; versicolor 0 19 1 #&gt; virginica 0 0 14 (confusionMatrix(iris_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.98 iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train, kpar = list(sigma = 0.1), C = 10) # → より少ないSV (iris_tab &lt;- table(predict(iris_ksvm, iris_test[, 1:4]), iris_test$Species)) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 16 0 0 #&gt; versicolor 0 17 0 #&gt; virginica 0 2 15 (confusionMatrix(iris_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.96 iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train, kpar = list(sigma = 0.1), C = 1) (iris_tab &lt;- table(predict(iris_ksvm, iris_test[, 1:4]), iris_test$Species)) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 16 0 0 #&gt; versicolor 0 19 1 #&gt; virginica 0 0 14 (confusionMatrix(iris_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.98 iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train, kpar = list(sigma = 0.1), C = 0.1) (iris_tab &lt;- table(predict(iris_ksvm, iris_test[, 1:4]), iris_test$Species)) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 16 0 0 #&gt; versicolor 0 14 1 #&gt; virginica 0 5 14 (confusionMatrix(iris_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.88 iris_ksvm &lt;- ksvm(Species ~ ., data = iris_train, kpar = list(sigma = 0.1), C = 0.01) # → より多くのSV (iris_tab &lt;- table(predict(iris_ksvm, iris_test[, 1:4]), iris_test$Species)) #&gt; #&gt; setosa versicolor virginica #&gt; setosa 0 0 0 #&gt; versicolor 0 0 0 #&gt; virginica 16 19 15 (confusionMatrix(iris_tab)$overall[&quot;Accuracy&quot;]) #&gt; Accuracy #&gt; 0.3 10.4 代替パッケージe1071の利用 ここでは, パッケージe1071, 関数svm()を使用する. (注: ISLRでもe1071を使用している: Sec. 9.6) パッケージcaretの関数createDataPartition()を利用して, # irisデータセットを学習用とテスト用に分割 set.seed(0) # library(caret) train_idx &lt;-caret::createDataPartition(iris$Species, p = 0.7, list = F) iris_train &lt;- iris[train_idx, ] iris_test &lt;- iris[-train_idx, ] SVMの実行 e1071では, 多クラス(\\(k&gt;2\\))のケース, “One-vs-One”アプローチ採用している. 関数svm()では, RBFカーネルのハイパーパラメータは gamma, costである. library(&#39;e1071&#39;) iris_svm = svm(Species ~ ., data = iris_train, method = &quot;C-Speciesification&quot;, kernel = &quot;radial&quot;, cost = 10, gamma = 0.1) summary(iris_svm) #&gt; #&gt; Call: #&gt; svm(formula = Species ~ ., data = iris_train, method = &quot;C-Speciesification&quot;, #&gt; kernel = &quot;radial&quot;, cost = 10, gamma = 0.1) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: radial #&gt; cost: 10 #&gt; #&gt; Number of Support Vectors: 27 #&gt; #&gt; ( 3 13 11 ) #&gt; #&gt; #&gt; Number of Classes: 3 #&gt; #&gt; Levels: #&gt; setosa versicolor virginica plot(iris_svm, iris_train, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4)) title(main = &quot;\\n\\n(cost = 10, gamma = 0.1)&quot;) iris_svm = svm(Species~., data = iris_train, method = &quot;C-classification&quot;, kernel = &quot;radial&quot;, cost = 10, gamma = 0.4) plot(iris_svm, iris_train, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4)) title(main = &quot;\\n\\n(cost = 10, gamma = 0.4)&quot;) iris_predict &lt;- predict(iris_svm, iris_test, decision.values = T) table(iris_predict, iris_test[, 3]) #&gt; #&gt; iris_predict 1.2 1.3 1.4 1.5 1.6 1.7 3 3.6 3.8 3.9 4 4.4 4.5 4.6 4.8 4.9 5 5.1 #&gt; setosa 1 3 2 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; versicolor 0 0 0 0 0 0 1 1 1 2 1 2 3 1 0 1 0 0 #&gt; virginica 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 #&gt; #&gt; iris_predict 5.4 5.5 5.6 5.8 5.9 6.1 6.3 6.4 6.6 #&gt; setosa 0 0 0 0 0 0 0 0 0 #&gt; versicolor 0 0 0 0 0 0 0 0 0 #&gt; virginica 1 1 3 2 1 1 1 1 1 10.5 ハイパーパラメータのチューニング 引き続き, パッケージe1071内の関数tune()を使用する. 線形カーネルの場合 set.seed(1) tune_svm = tune(svm, Species ~ ., data = iris_train, kernel = &quot;linear&quot;, ranges = list(cost = 10^(-3:2))) RBFカーネルの場合 tune_svm = tune(svm, Species ~ ., data = iris_train, ranges = list(gamma = 10^(-3:1), cost = 10^(-3:2))) summary(tune_svm) #&gt; #&gt; Parameter tuning of &#39;svm&#39;: #&gt; #&gt; - sampling method: 10-fold cross validation #&gt; #&gt; - best parameters: #&gt; gamma cost #&gt; 0.001 100 #&gt; #&gt; - best performance: 0.02818182 #&gt; #&gt; - Detailed performance results: #&gt; gamma cost error dispersion #&gt; 1 1e-03 1e-03 0.74272727 0.17897012 #&gt; 2 1e-02 1e-03 0.74272727 0.17897012 #&gt; 3 1e-01 1e-03 0.75181818 0.16991598 #&gt; 4 1e+00 1e-03 0.73363636 0.19194285 #&gt; 5 1e+01 1e-03 0.78272727 0.12386862 #&gt; 6 1e-03 1e-02 0.74272727 0.17897012 #&gt; 7 1e-02 1e-02 0.74272727 0.17897012 #&gt; 8 1e-01 1e-02 0.75181818 0.16991598 #&gt; 9 1e+00 1e-02 0.73363636 0.19194285 #&gt; 10 1e+01 1e-02 0.78272727 0.12386862 #&gt; 11 1e-03 1e-01 0.74272727 0.17897012 #&gt; 12 1e-02 1e-01 0.70636364 0.19787902 #&gt; 13 1e-01 1e-01 0.20181818 0.24479147 #&gt; 14 1e+00 1e-01 0.10545455 0.14130443 #&gt; 15 1e+01 1e-01 0.78272727 0.12386862 #&gt; 16 1e-03 1e+00 0.67818182 0.18891211 #&gt; 17 1e-02 1e+00 0.12363636 0.11110927 #&gt; 18 1e-01 1e+00 0.03727273 0.04819039 #&gt; 19 1e+00 1e+00 0.04727273 0.04994028 #&gt; 20 1e+01 1e+00 0.11454545 0.04070093 #&gt; 21 1e-03 1e+01 0.12363636 0.11110927 #&gt; 22 1e-02 1e+01 0.03727273 0.04819039 #&gt; 23 1e-01 1e+01 0.03818182 0.04938557 #&gt; 24 1e+00 1e+01 0.04727273 0.04994028 #&gt; 25 1e+01 1e+01 0.11454545 0.04070093 #&gt; 26 1e-03 1e+02 0.02818182 0.04544444 #&gt; 27 1e-02 1e+02 0.03818182 0.04938557 #&gt; 28 1e-01 1e+02 0.02818182 0.04544444 #&gt; 29 1e+00 1e+02 0.03727273 0.04819039 #&gt; 30 1e+01 1e+02 0.11454545 0.04070093 # 最適なハイパーパラメータを用いたモデル bestmod_svm = tune_svm$best.model summary(bestmod_svm) #&gt; #&gt; Call: #&gt; best.tune(METHOD = svm, train.x = Species ~ ., data = iris_train, #&gt; ranges = list(gamma = 10^(-3:1), cost = 10^(-3:2))) #&gt; #&gt; #&gt; Parameters: #&gt; SVM-Type: C-classification #&gt; SVM-Kernel: radial #&gt; cost: 100 #&gt; #&gt; Number of Support Vectors: 42 #&gt; #&gt; ( 4 21 17 ) #&gt; #&gt; #&gt; Number of Classes: 3 #&gt; #&gt; Levels: #&gt; setosa versicolor virginica tune()の使い方のヒント: - 引数tunecontrolの値を関数tune.control()を使って設定できる - 例: tune.control(sampling = &quot;boot&quot;) - sampling: サンプリング・スキームの指定 - &quot;boot&quot;(ブートストラップ), &quot;cross&quot;(クロスバリデーション), &quot;fix&quot;(学習/評価に1回分割のみ) e1071内にあるチューニング用の便利な関数群 e1071は, ウィーン工科大学 (TU Wien) の統計学科の確率論グループが作成している, 様々な 機械学習や数値計算を行う関数をカバーするパッケージである. https://cran.r-project.org/web/packages/e1071/e1071.pdf パッケージ内には, tune.XXXと命名された関数群がある. これらは, 上記tune()を使用したwrapper関数で, これを用いることで機械学習アルゴリズムXXXの ハイパーパラメータ・チューニングを行うことができる. tune.svm(), tune.rpart(), tune.randomForest(), tune.knn(), 等. 注) 呼び出す関数により, 予測変数と目的変数を個別に指定するもの, (x=…, y=…)と, 式 (formula), すなわち, y ~ xのように指定をするものがあることに注意が必要. kNN法 関数tune.knn()は, knn()を呼び出して実行する. knn()のパラメータkのサーチ範囲を指定することができる. x &lt;- iris[, -5] y &lt;- iris[, 5] res_tune_knn &lt;- tune.knn(x, y, k = 1:10, tunecontrol = tune.control(sampling = &quot;boot&quot;)) summary(res_tune_knn) #&gt; #&gt; Parameter tuning of &#39;knn.wrapper&#39;: #&gt; #&gt; - sampling method: bootstrapping #&gt; #&gt; - best parameters: #&gt; k #&gt; 1 #&gt; #&gt; - best performance: 0.04242248 #&gt; #&gt; - Detailed performance results: #&gt; k error dispersion #&gt; 1 1 0.04242248 0.01767743 #&gt; 2 2 0.04935387 0.01928082 #&gt; 3 3 0.05304685 0.02914612 #&gt; 4 4 0.05404239 0.04167451 #&gt; 5 5 0.05522874 0.03213148 #&gt; 6 6 0.05548034 0.03610984 #&gt; 7 7 0.04847679 0.02678632 #&gt; 8 8 0.05191999 0.03424645 #&gt; 9 9 0.04650720 0.02991595 #&gt; 10 10 0.04459118 0.02676795 plot(res_tune_knn) 決定木 関数tune.rpart(): rpart()を呼び出して実行する. rpart()のパラメータするための関数rpart.conctrol() に与えるパラメータに対して, チューニングのためのサーチの範囲を 指定することができる. 以下は, minsplit (ノード内の最小観測点の数) を与える例を示す: data(mtcars) res_tune_rpart &lt;- tune.rpart(mpg ~ ., data = mtcars, minsplit = c(5, 10, 15)) summary(res_tune_rpart) #&gt; #&gt; Parameter tuning of &#39;rpart.wrapper&#39;: #&gt; #&gt; - sampling method: 10-fold cross validation #&gt; #&gt; - best parameters: #&gt; minsplit #&gt; 5 #&gt; #&gt; - best performance: 10.40174 #&gt; #&gt; - Detailed performance results: #&gt; minsplit error dispersion #&gt; 1 5 10.40174 8.416546 #&gt; 2 10 11.44564 8.841332 #&gt; 3 15 19.12135 11.868442 plot(res_tune_rpart) 呼び出すアルゴリズム/モデルごとに異なるハイパーパラメータを持つため, 正しい設定を行うには, 各々の関数のマニュアルを確認する必要. ハイパーパラメータのチューニングは, 他のパッケージ, 例えば, caretの機能を用いても実行することができる. "],["主成分分析-pca.html", "11 主成分分析 (PCA) 11.1 基本操作 11.2 2種類のPCA: 相関行列ベース vs 分散共分散行列ベース", " 11 主成分分析 (PCA) 11.1 基本操作 データセット: 従業員スキル評価データ(仮想) 文字化けする場合には, 英語版”testdat_eng.csv”を使用しても良い. &quot;testdat_30_jap.csv&quot; (日本語版/英語版) - 氏名/Name - 専門性/Expertise (0-100) - 分析力/Analytics (同) - リーダーシップ/Leadership (同) - プレゼン力/Presentation (同) - コミュ力/Communication (同) - n = 9, p = 5 tokuten &lt;- read.csv(&quot;testdat_jap.csv&quot;, header = T, row.names = 1, skip = 1) #colnames(tokuten) &lt;- c(&quot;name&quot;, &quot;E&quot;, &quot;A&quot;, &quot;L&quot;, &quot;P&quot;, &quot;C&quot;) tokuten # 散布図 pairs(tokuten) #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 山田 80 87 70 65 67 #&gt; 鈴木 85 83 65 68 63 #&gt; 田中 75 70 80 79 75 #&gt; 中村 70 72 85 83 79 #&gt; 大野 88 85 85 90 91 #&gt; 松井 75 78 79 85 80 #&gt; 高木 80 80 75 75 80 #&gt; 三浦 83 72 85 80 83 #&gt; 佐藤 69 82 77 70 80 # 分散共分散行列に対するPCA（デフォルト) tokuten_pc &lt;- princomp(tokuten) summary(tokuten_pc) plot(tokuten_pc) # Scree plot = barplot of PC variances (eigenvalues) screeplot(tokuten_pc) # same as above #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; Standard deviation 12.4807420 7.1424787 4.73037885 2.9724343 1.57269337 #&gt; Proportion of Variance 0.6477709 0.2121478 0.09305346 0.0367422 0.01028558 #&gt; Cumulative Proportion 0.6477709 0.8599188 0.95297222 0.9897144 1.00000000 # 主成分負荷量(係数) tokuten_pc$loadings # t(tokuten_pc$loadings) %*% tokuten_pc$loadings # 直交性の確認 # 注) 下のprcompの結果と, 符号が逆転 # scatter plot for 1st &amp; 2nd PC #plot(tokuten_pc$loadings[, 1:2]) plot(tokuten_pc$loadings[, 1:2], type = &quot;n&quot;, main = &quot;PC loadings: PC1 vs PC2&quot;) # plot only axis text(tokuten_pc$loadings, colnames(tokuten)) #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; 専門性 0.775 0.536 0.332 #&gt; 分析力 0.227 0.554 -0.634 -0.325 0.366 #&gt; リーダーシップ -0.510 -0.137 0.323 0.786 #&gt; プレゼン力 -0.591 0.153 0.275 -0.741 #&gt; コミュ力 -0.582 0.223 -0.486 0.362 -0.496 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 # 主成分得点(データ毎) tokuten_pc$scores plot(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], type = &quot;n&quot;, main = &quot;PC scores: PC1 vs PC2&quot;) #text(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], text(tokuten_pc$scores[, 1], tokuten_pc$scores[, 2], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # バイプロット biplot(tokuten_pc, family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) #biplot(tokuten_pc) # plot PC &amp; PC scores ===&gt; display first 2 PCs only... # # 計算結果確認 # (yama &lt;- tokuten[&quot;山田&quot;, ]-apply(tokuten, 2, mean)) # demeaned score # sum(tokuten_pc$loadings[, &quot;Comp.1&quot;] * yama) # Yamada&#39;s PC1 score # sum(tokuten_pc$loadings[, &quot;Comp.2&quot;] * yama) # Yamada&#39;s PC2 score # # compare this with tokuten_pc$scores[&quot;Yamada&quot;, ] #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; 山田 19.304343 2.7048291 -2.4411238 0.5754498 2.6750248 #&gt; 鈴木 21.655520 4.6155395 5.6073003 -1.7490457 -0.7368287 #&gt; 田中 -2.739766 -8.0346039 5.4765050 0.1887216 -0.4687677 #&gt; 中村 -9.682974 -9.9826824 0.6190434 -2.0247358 1.8769769 #&gt; 大野 -17.277425 14.9224986 -1.8787424 -1.1322419 0.8666327 #&gt; 松井 -6.867523 -1.4328205 -0.3577567 -5.3750278 -1.0885751 #&gt; 高木 1.694979 2.5684468 -1.6399325 1.7537323 -2.8802837 #&gt; 三浦 -9.824535 0.5291295 4.8161616 5.9565572 0.4056066 #&gt; 佐藤 3.737381 -5.8903368 -10.2014547 1.8065903 -0.6497859 # 第1, 2主成分 tokuten_pc$scores[, 1:2] tokuten_pc$loadings[, 1:2] # biplot(tokuten_pc$scores[, 1:2], tokuten_pc$loadings[, 1:2]) # loadings accompany &quot;arrows&quot; &lt;-- Looks more useful!! biplot(tokuten_pc$scores[, 1:2], tokuten_pc$loadings[, 1:2], family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # biplot(tokuten_pc$loadings[, 1:2], tokuten_pc$scores[, 1:2]) # scores accompany &quot;arrows&quot; biplot(tokuten_pc$loadings[, 1:2], tokuten_pc$scores[, 1:2], family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) #&gt; Comp.1 Comp.2 #&gt; 山田 19.304343 2.7048291 #&gt; 鈴木 21.655520 4.6155395 #&gt; 田中 -2.739766 -8.0346039 #&gt; 中村 -9.682974 -9.9826824 #&gt; 大野 -17.277425 14.9224986 #&gt; 松井 -6.867523 -1.4328205 #&gt; 高木 1.694979 2.5684468 #&gt; 三浦 -9.824535 0.5291295 #&gt; 佐藤 3.737381 -5.8903368 #&gt; Comp.1 Comp.2 #&gt; 専門性 0.03162775 0.7753770 #&gt; 分析力 0.22710092 0.5541994 #&gt; リーダーシップ -0.50974398 -0.1365068 #&gt; プレゼン力 -0.59111858 0.1530044 #&gt; コミュ力 -0.58151936 0.2227311 # 評価項目ペア別, 各従業員のスコア散布図 #plot(tokuten) # par(mfrow = c(2, 2)) plot(tokuten[, c(1, 2)], type = &quot;n&quot;, main = &quot;item 1 vs item 2&quot;) # plot only axis # text(tokuten[, c(1, 2)], rownames(tokuten)) text(tokuten[, c(1, 2)], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) plot(tokuten[, c(2, 3)], type = &quot;n&quot;, main = &quot;item 2 vs item 3&quot;) # plot only axis # text(tokuten[, c(2, 3)], rownames(tokuten)) text(tokuten[, c(2, 3)], rownames(tokuten), family = &quot;HiraKakuProN-W3&quot;) # 日本語文字化け対応 (mac) # plot(tokuten[, c(3, 4)], type = &quot;n&quot;) # plot only axis # text(tokuten[, c(3, 4)], rownames(tokuten)) # plot(tokuten[, c(4, 5)], type = &quot;n&quot;) # plot only axis # text(tokuten[, c(4, 5)], rownames(tokuten)) # 相関行列に対するPCA # tokuten_pc2 &lt;- princomp(tokuten, cor = T) # summary(tokuten_pc2) # tokuten_pc2$loadings # tokuten_pc2$scores[, 1:2] # tokuten_pc2$loadings[, 1:2] # 代替的関数 # prcomp() # 不偏分散共分散行列の使用 (÷(n-1)) tokuten_prcomp &lt;- prcomp(tokuten) #tokuten_prcomp &lt;- prcomp(tokuten, scale = T) summary(tokuten_prcomp) tokuten_prcomp$rotation # PC負荷量 tokuten_prcomp$x # PC得点 #biplot(tokuten_prcomp) biplot(tokuten_prcomp, family = &quot;HiraKakuProN-W3&quot;) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; Standard deviation 13.2378 7.5757 5.01732 3.15274 1.66809 #&gt; Proportion of Variance 0.6478 0.2122 0.09305 0.03674 0.01029 #&gt; Cumulative Proportion 0.6478 0.8599 0.95297 0.98971 1.00000 #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 専門性 -0.03162775 0.7753770 0.53571555 0.3316583 -0.02831625 #&gt; 分析力 -0.22710092 0.5541994 -0.63357070 -0.3251925 -0.36623249 #&gt; リーダーシップ 0.50974398 -0.1365068 -0.01367592 0.3227645 -0.78559724 #&gt; プレゼン力 0.59111858 0.1530044 0.27483158 -0.7411911 0.04766342 #&gt; コミュ力 0.58151936 0.2227311 -0.48567230 0.3615403 0.49561793 #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 山田 -19.304343 2.7048291 -2.4411238 0.5754498 -2.6750248 #&gt; 鈴木 -21.655520 4.6155395 5.6073003 -1.7490457 0.7368287 #&gt; 田中 2.739766 -8.0346039 5.4765050 0.1887216 0.4687677 #&gt; 中村 9.682974 -9.9826824 0.6190434 -2.0247358 -1.8769769 #&gt; 大野 17.277425 14.9224986 -1.8787424 -1.1322419 -0.8666327 #&gt; 松井 6.867523 -1.4328205 -0.3577567 -5.3750278 1.0885751 #&gt; 高木 -1.694979 2.5684468 -1.6399325 1.7537323 2.8802837 #&gt; 三浦 9.824535 0.5291295 4.8161616 5.9565572 -0.4056066 #&gt; 佐藤 -3.737381 -5.8903368 -10.2014547 1.8065903 0.6497859 11.2 2種類のPCA: 相関行列ベース vs 分散共分散行列ベース データセット3: USArrests data USArrests - 全米50州, 人口10万人当たり - 1973年の凶悪犯罪(assault, murder, rape)件数, 及び都市部の人口割合(%) - n = 50, p = 4 summary(USArrests) pairs(USArrests) #&gt; Murder Assault UrbanPop Rape #&gt; Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 #&gt; 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 #&gt; Median : 7.250 Median :159.0 Median :66.00 Median :20.10 #&gt; Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 #&gt; 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 #&gt; Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 # どちらが良いか? # (pc_cr1 &lt;- princomp(USArrests)) # (pc_cr2 &lt;- princomp(USArrests, cor = TRUE)) (pc_cr3 &lt;- prcomp(USArrests)) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 83.732400 14.212402 6.489426 2.482790 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Murder 0.04170432 -0.04482166 0.07989066 -0.99492173 #&gt; Assault 0.99522128 -0.05876003 -0.06756974 0.03893830 #&gt; UrbanPop 0.04633575 0.97685748 -0.20054629 -0.05816914 #&gt; Rape 0.07515550 0.20071807 0.97408059 0.07232502 (pc_cr4 &lt;- prcomp(USArrests, scale = TRUE)) #&gt; Standard deviations (1, .., p=4): #&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494 #&gt; #&gt; Rotation (n x k) = (4 x 4): #&gt; PC1 PC2 PC3 PC4 #&gt; Murder -0.5358995 -0.4181809 0.3412327 0.64922780 #&gt; Assault -0.5831836 -0.1879856 0.2681484 -0.74340748 #&gt; UrbanPop -0.2781909 0.8728062 0.3780158 0.13387773 #&gt; Rape -0.5434321 0.1673186 -0.8177779 0.08902432 plot(pc_cr3) summary(pc_cr3) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 83.7324 14.21240 6.4894 2.48279 #&gt; Proportion of Variance 0.9655 0.02782 0.0058 0.00085 #&gt; Cumulative Proportion 0.9655 0.99335 0.9991 1.00000 plot(pc_cr4) summary(pc_cr4) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 #&gt; Standard deviation 1.5749 0.9949 0.59713 0.41645 #&gt; Proportion of Variance 0.6201 0.2474 0.08914 0.04336 #&gt; Cumulative Proportion 0.6201 0.8675 0.95664 1.00000 # PCAにかける変数の絞り込み # prcomp(~ Murder + Assault + Rape, data = USArrests, scale = TRUE) "],["探索的因子分析.html", "12 探索的因子分析 12.1 基本操作 12.2 ライブラリpsychの利用 12.3 因子分析に有用なツール", " 12 探索的因子分析 12.1 基本操作 データセット1: 従業員評価データ (仮想) ここでは, 上のPCAで使用した”testdat3_jap.csv”の拡大版を使用する. (文字化けする場合には, 英語版”testdat_eng.csv”を使用しても良い.) &quot;testdat_30_jap.csv&quot; (日本語版/英語版) - 氏名/Name - 専門性/Expertise (0-100) - 分析力/Analytics (同) - リーダーシップ/Leadership (同) - プレゼン力/Presentation (同) - コミュ力/Communication (同) - n = 30, p = 5 Rで標準的に用意されている関数factanal()を使用する. # (1) factanal: 最尤法only tokuten &lt;- read.csv(&quot;testdat_30_jap.csv&quot;, header = T, row.names = 1, skip = 1) # tokuten &lt;- read.csv(&quot;testdat_30_eng.csv&quot;, header = T, row.names = 1, skip = 1) # tokuten &lt;- read.csv(&quot;testdat_jap.csv&quot;, header = T, row.names = 1, skip = 1) tokuten_fac &lt;- factanal(tokuten, factors = 2) # default varimax回転 tokuten_fac tokuten_fac$loading # 確認(各因子のloadingsベクトルの2乗和--&gt; SS loadings)) sum(tokuten_fac$loading[, 1]^2) #&gt; #&gt; Call: #&gt; factanal(x = tokuten, factors = 2) #&gt; #&gt; Uniquenesses: #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 0.849 0.238 0.005 0.208 0.100 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.172 0.349 #&gt; 分析力 -0.368 0.792 #&gt; リーダーシップ 0.980 -0.186 #&gt; プレゼン力 0.890 #&gt; コミュ力 0.936 0.153 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.794 0.807 #&gt; Proportion Var 0.559 0.161 #&gt; Cumulative Var 0.559 0.720 #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The chi square statistic is 4.44 on 1 degree of freedom. #&gt; The p-value is 0.0351 #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.172 0.349 #&gt; 分析力 -0.368 0.792 #&gt; リーダーシップ 0.980 -0.186 #&gt; プレゼン力 0.890 #&gt; コミュ力 0.936 0.153 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.794 0.807 #&gt; Proportion Var 0.559 0.161 #&gt; Cumulative Var 0.559 0.720 #&gt; [1] 2.794063 # 因子負荷量barplot(factor loadings) par(mfrow = c(1, 2)) barplot(tokuten_fac$loading[, 1], col = &quot;red&quot;) barplot(tokuten_fac$loading[, 2], col = &quot;blue&quot;) Tips: プロット時, 日本語が文字化けする場合の対応 (特に, macユーザー): &gt; par(family = &quot;HiraKakuProN-W3&quot;) または, &gt; par(family = &quot;HG明朝E&quot;) # 因子（負荷量）行列の回転 # scores = &quot;regression&quot; ==&gt; Thomson&#39;s score返す （デフォルト, 戻り値無) tokuten_fac2 &lt;- factanal(tokuten, factors = 2, rotation = &quot;none&quot;, scores = &quot;regression&quot;) # 回転なし tokuten_fac3 &lt;- factanal(tokuten, factors = 2, rotation = &quot;promax&quot;, scores = &quot;regression&quot;) # promax回転（斜交回転) tokuten_fac4 &lt;- factanal(tokuten, factors = 2, scores = &quot;regression&quot;) # varimax回転（直交回転) # 因子得点(factor scores) tokuten_fac2$scores #&gt; Factor1 Factor2 #&gt; 山田 -1.26800407 0.38570292 #&gt; 鈴木 -1.95989876 0.01060196 #&gt; 田中 0.20645006 -1.63631682 #&gt; 中村 0.91696834 -1.38312209 #&gt; 大野 0.96895473 1.74205043 #&gt; 小林 0.09169331 0.01065859 #&gt; 伊藤 -0.48179366 0.42756367 #&gt; 高橋 0.93201465 -0.89712752 #&gt; 渡辺 -0.22275387 0.27605947 #&gt; 佐藤 -0.35296159 -0.25515430 #&gt; 山下 1.22524580 -0.78746751 #&gt; 木村 0.80403959 -0.04966399 #&gt; 山本 -0.22649622 0.19899511 #&gt; 宮崎 1.67523625 -0.20442665 #&gt; 山口 -1.12973504 0.24392701 #&gt; 阿部 1.54630529 -0.22549756 #&gt; 斎藤 -0.75968450 1.72490202 #&gt; 吉田 -1.68984987 -1.46995307 #&gt; 佐々木 -0.80152021 -0.47761662 #&gt; 石川 0.66965259 0.60177580 #&gt; 山崎 -0.66802546 -0.35549465 #&gt; 中山 0.78533445 0.67322981 #&gt; 藤田 -0.77928205 0.88172247 #&gt; 加藤 -0.83886445 -0.82003132 #&gt; 清水 0.53238028 1.91115337 #&gt; 池田 0.49224657 -0.22365300 #&gt; 井上 -0.63623615 -0.04435814 #&gt; 林 -0.94437205 0.48040513 #&gt; 中島 0.21941624 -1.01764392 #&gt; 森 1.69353978 0.27877943 tokuten_fac3$scores #&gt; Factor1 Factor2 #&gt; 山田 -1.03462508 0.66053651 #&gt; 鈴木 -1.83144475 0.43930549 #&gt; 田中 -0.45570109 -1.67056983 #&gt; 中村 0.31020607 -1.57450638 #&gt; 大野 1.59853100 1.51845067 #&gt; 小林 0.09010822 -0.00947258 #&gt; 伊藤 -0.28165229 0.53011595 #&gt; 高橋 0.51707362 -1.09504471 #&gt; 渡辺 -0.09913025 0.32295107 #&gt; 佐藤 -0.43179522 -0.17623394 #&gt; 山下 0.83521312 -1.05026730 #&gt; 木村 0.73336738 -0.22523546 #&gt; 山本 -0.13320378 0.24721940 #&gt; 宮崎 1.48794583 -0.56956087 #&gt; 山口 -0.96135865 0.48945650 #&gt; 阿部 1.35883050 -0.56228456 #&gt; 斎藤 -0.02732251 1.87959737 #&gt; 吉田 -2.16579425 -1.09045510 #&gt; 佐々木 -0.94015940 -0.29907982 #&gt; 石川 0.86590070 0.45126013 #&gt; 山崎 -0.76668658 -0.20697743 #&gt; 中山 1.00259189 0.49692945 #&gt; 藤田 -0.38013416 1.04632852 #&gt; 加藤 -1.11095876 -0.63104097 #&gt; 清水 1.25670992 1.78193683 #&gt; 池田 0.37232594 -0.32985202 #&gt; 井上 -0.61349683 0.09512940 #&gt; 林 -0.69394517 0.68380497 #&gt; 中島 -0.19815346 -1.05885961 #&gt; 森 1.69675803 -0.09358169 tokuten_fac4$scores #&gt; Factor1 Factor2 #&gt; 山田 -1.18038461 0.602738127 #&gt; 鈴木 -1.92747306 0.355194538 #&gt; 田中 -0.08460701 -1.647117489 #&gt; 中村 0.65937060 -1.522855235 #&gt; 大野 1.26028286 1.544441669 #&gt; 小林 0.09213844 -0.005637015 #&gt; 伊藤 -0.39907004 0.505647037 #&gt; 高橋 0.75967163 -1.047085528 #&gt; 渡辺 -0.17071995 0.310938604 #&gt; 佐藤 -0.39234102 -0.189087623 #&gt; 山下 1.06762028 -0.990716564 #&gt; 木村 0.78276599 -0.190324936 #&gt; 山本 -0.18796003 0.235734215 #&gt; 宮崎 1.61315438 -0.495923070 #&gt; 山口 -1.06921083 0.438850587 #&gt; 阿部 1.48252735 -0.493985703 #&gt; 斎藤 -0.44441816 1.831638615 #&gt; 吉田 -1.92207351 -1.149777378 #&gt; 佐々木 -0.87303770 -0.329176947 #&gt; 石川 0.76506664 0.474596410 #&gt; 山崎 -0.72014255 -0.232441752 #&gt; 中山 0.89151385 0.524587102 #&gt; 藤田 -0.61203044 1.005054108 #&gt; 加藤 -0.97003241 -0.659683285 #&gt; 清水 0.86026215 1.787703833 #&gt; 池田 0.44522905 -0.306754687 #&gt; 井上 -0.63411816 0.068251272 #&gt; 林 -0.84514031 0.639034836 #&gt; 中島 0.03698524 -1.040372398 #&gt; 森 1.71617133 -0.023471343 # 因子負荷量(factor loadings) tokuten_fac2$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.108 0.374 #&gt; 分析力 -0.502 0.715 #&gt; リーダーシップ 0.997 #&gt; プレゼン力 0.873 0.171 #&gt; コミュ力 0.895 0.316 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.821 0.780 #&gt; Proportion Var 0.564 0.156 #&gt; Cumulative Var 0.564 0.720 tokuten_fac3$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.249 0.348 #&gt; 分析力 -0.186 0.820 #&gt; リーダーシップ 0.930 -0.229 #&gt; プレゼン力 0.886 #&gt; コミュ力 0.963 0.118 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.675 0.859 #&gt; Proportion Var 0.535 0.172 #&gt; Cumulative Var 0.535 0.707 tokuten_fac4$loading #&gt; #&gt; Loadings: #&gt; Factor1 Factor2 #&gt; 専門性 0.172 0.349 #&gt; 分析力 -0.368 0.792 #&gt; リーダーシップ 0.980 -0.186 #&gt; プレゼン力 0.890 #&gt; コミュ力 0.936 0.153 #&gt; #&gt; Factor1 Factor2 #&gt; SS loadings 2.794 0.807 #&gt; Proportion Var 0.559 0.161 #&gt; Cumulative Var 0.559 0.720 # tokuten_fac$loading # 因子得点と因子負荷量のbiplot # biplot(tokuten_fac2$scores, tokuten_fac2$loading) # biplot(tokuten_fac3$scores, tokuten_fac3$loading) # biplot(tokuten_fac4$scores, tokuten_fac4$loading) biplot(tokuten_fac2$scores, tokuten_fac2$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;無回転&quot;) # 日本語文字化け対応 (mac) biplot(tokuten_fac3$scores, tokuten_fac3$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;promax回転&quot;) # 日本語文字化け対応 (mac) biplot(tokuten_fac4$scores, tokuten_fac4$loading, family = &quot;HiraKakuProN-W3&quot;, main = &quot;varimax回転&quot;) # 日本語文字化け対応 (mac) # 独自因子（共通因子で説明出来なかった変動の割合) tokuten_fac$uniquenesses # specific variance #&gt; 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 0.84871769 0.23762850 0.00500000 0.20796645 0.09970145 12.2 ライブラリpsychの利用 関数psych()では, モデル推定方法や因子負荷行列の回転方法等に選択肢がある. - モデル推定方法 (引数fm=): 最尤法 (&quot;ml&quot;), 一般化最小2乗法 (&quot;gls&quot;), 重み付き最小2乗法 (&quot;gls&quot;), 最小残差法 (&quot;mires&quot;). デフォルトは&quot;mires&quot; - 回転方法 (引数rotate=): オブリミン (&quot;oblimin&quot;), バリマックス(&quot;varimax&quot;)など. library(psych) # 主因子法 tokuten_fa &lt;- fa(r = tokuten, nfactors = 2 , rotate = &quot;none&quot;, fm = &quot;pa&quot;, scores = T) # 回転なし tokuten_fa2 &lt;- fa(r = tokuten, nfactors = 2 , rotate = &quot;oblimin&quot;, fm = &quot;pa&quot;, scores = T) # # tokuten_fa &lt;- fa(r = tokuten, nfactors = 2 , rotate = &quot;none&quot;, fm = &quot;ml&quot;, scores = T) # デフォルト: rotate = &quot;oblimin&quot;, fm = &quot;minres&quot; # 最尤法, fm = &quot;ml&quot;, 一般化最小2乗法, &quot;gls&quot;, 重み付き最小2乗法&quot;gls&quot;, 最小残差法&quot;mires&quot; # PCAとの比較 # tokuten_fa # standardized loadings (pattern matrix)表示 summary(tokuten_fa) tokuten_fa2$loadings tokuten_fa$scores par(mfrow = c(1, 2)) # plot(tokuten_fa) # plot(tokuten_fa2) biplot(tokuten_fa, main = &quot;無回転&quot;) biplot(tokuten_fa2, main = &quot;oblimin回転&quot;) par(mfrow = c(1, 1)) #&gt; #&gt; Factor analysis with Call: fa(r = tokuten, nfactors = 2, rotate = &quot;none&quot;, scores = T, fm = &quot;pa&quot;) #&gt; #&gt; Test of the hypothesis that 2 factors are sufficient. #&gt; The degrees of freedom for the model is 1 and the objective function was 0.35 #&gt; The number of observations was 30 with Chi Square = 8.79 with prob &lt; 0.003 #&gt; #&gt; The root mean square of the residuals (RMSA) is 0.04 #&gt; The df corrected root mean square of the residuals is 0.12 #&gt; #&gt; Tucker Lewis Index of factoring reliability = 0.163 #&gt; RMSEA index = 0.508 and the 10 % confidence intervals are 0.245 0.856 #&gt; BIC = 5.39 #&gt; Loadings: #&gt; PA1 PA2 #&gt; 専門性 0.355 0.470 #&gt; 分析力 -0.184 0.660 #&gt; リーダーシップ 0.911 -0.255 #&gt; プレゼン力 0.958 #&gt; コミュ力 0.914 #&gt; #&gt; PA1 PA2 #&gt; SS loadings 2.744 0.738 #&gt; Proportion Var 0.549 0.148 #&gt; Cumulative Var 0.549 0.696 #&gt; PA1 PA2 #&gt; 山田 -1.02034162 0.019100487 #&gt; 鈴木 -1.81609325 0.890601637 #&gt; 田中 0.11710274 -1.131035071 #&gt; 中村 0.97786615 -1.497888977 #&gt; 大野 1.18686813 1.448453889 #&gt; 小林 0.14568067 0.518419196 #&gt; 伊藤 -0.66256206 0.755510959 #&gt; 高橋 0.82501884 -1.016144138 #&gt; 渡辺 -0.37950698 -0.296849891 #&gt; 佐藤 -0.35540858 0.222170566 #&gt; 山下 1.38305140 -0.665023036 #&gt; 木村 0.62270049 -0.476316678 #&gt; 山本 -0.44361495 -0.426060496 #&gt; 宮崎 1.64994564 -0.484538347 #&gt; 山口 -0.76190378 -0.118039940 #&gt; 阿部 1.56078515 0.162024351 #&gt; 斎藤 -0.74336529 2.028239692 #&gt; 吉田 -1.93942159 -0.937774682 #&gt; 佐々木 -0.94328316 -0.363190871 #&gt; 石川 0.73867734 0.571203066 #&gt; 山崎 -0.93450472 -0.725912234 #&gt; 中山 1.01147500 0.113093275 #&gt; 藤田 -0.67086121 1.318200214 #&gt; 加藤 -0.80985800 -1.173736127 #&gt; 清水 0.62804134 1.271351417 #&gt; 池田 0.76781783 -0.165605242 #&gt; 井上 -0.64479954 0.598812374 #&gt; 林 -0.96156230 0.407994217 #&gt; 中島 -0.06969367 -0.843983587 #&gt; 森 1.54175000 -0.003076023 12.3 因子分析に有用なツール ライブラリpsychの関数fa.parallel(), vss()は, 因子数の決定に有用である. # library(psych) # 平行分析(parallel analysis) # デフォルト: fm = &quot;minres&quot; (res_parallel &lt;- fa.parallel(tokuten)) # minres法(デフォルト), PCA &amp; 因子分析 # fa.parallel(tokuten, fm = &quot;wls&quot;) # fa.parallel(tokuten, fm = &quot;ml&quot;, fa = &quot;fa&quot;) # 最尤法+因子分析(のみ)実行 # サンプルデータから作られるscreeプロットと, シミュレーションデータの行列(サンプルと同じサイズ)のscreeプロットとを比較 # → 因子数をsuggest # fa = &quot;both&quot; (デフォルト): # PCA, 主因子法の固有値を同時に表示 # VSS (Very Simple Structure) 規準 # Very Simple Structure criterion ( VSS) for estimating the optimal number of factors # 最大となる因子数を探す # (tokuten_vss &lt;- vss(tokuten, n = 5, rotate = &quot;oblimin&quot;, fm = &quot;wls&quot;) ) # n: Number of factors to extract (&gt; (初期仮説の)因子数) # デフォルト: # 因子数 n = 8, 回転 rotate = &quot;varimax&quot; # → VSS, MAP, その他の因子数決定基準の数値を表示 tmp_vss &lt;- vss(tokuten, n = 4, fm = &quot;ml&quot;) # VSS.plot(tmp_vss) print(tmp_vss) #&gt; Parallel analysis suggests that the number of factors = 2 and the number of components = 1 #&gt; Call: fa.parallel(x = tokuten) #&gt; Parallel analysis suggests that the number of factors = 2 and the number of components = 1 #&gt; #&gt; Eigen Values of #&gt; Original factors Resampled data Simulated data Original components #&gt; 1 2.78 1.07 1.05 2.96 #&gt; 2 0.39 0.28 0.26 1.23 #&gt; Resampled components Simulated components #&gt; 1 1.6 1.58 #&gt; 2 1.2 1.17 #&gt; #&gt; Very Simple Structure #&gt; Call: vss(x = tokuten, n = 4, fm = &quot;ml&quot;) #&gt; VSS complexity 1 achieves a maximimum of 0.81 with 1 factors #&gt; VSS complexity 2 achieves a maximimum of 0.92 with 3 factors #&gt; #&gt; The Velicer MAP achieves a minimum of 0.15 with 1 factors #&gt; BIC achieves a minimum of 1.04 with 2 factors #&gt; Sample Size adjusted BIC achieves a minimum of 4.15 with 2 factors #&gt; #&gt; Statistics by number of factors #&gt; vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex #&gt; 1 0.81 0.00 0.15 5 2.1e+01 0.00083 2.01 0.81 0.32 3.9 19.5 1.0 #&gt; 2 0.80 0.92 0.27 1 4.4e+00 0.03506 0.88 0.92 0.34 1.0 4.2 1.2 #&gt; 3 0.79 0.92 0.45 -2 0.0e+00 NA 0.52 0.95 NA NA NA 1.5 #&gt; 4 0.79 0.92 1.00 -4 2.1e-14 NA 0.54 0.95 NA NA NA 1.5 #&gt; eChisq SRMR eCRMS eBIC #&gt; 1 1.1e+01 1.4e-01 0.19 -5.7 #&gt; 2 1.6e+00 5.2e-02 0.16 -1.8 #&gt; 3 7.8e-17 3.6e-10 NA NA #&gt; 4 1.7e-14 5.3e-09 NA NA ライブラリpsych内の関数fa.plot()やfa.diagram()は, 因子間の関係性 (相関構造や階層構造) を調べるのに有用である. fa.plot(tokuten_fa, cut = 0.5) # fa.plot(tokuten_fa2, cut = 0.5) fa.diagram(tokuten_fa) # fa.diagram(tokuten_fa2) また, ライブラリqgraphの関数qgraph()も同様な目的で有用である. # 相関ネットワーク(参考) library(qgraph) qgraph(cor(tokuten), edge.labels = T, minimum = .2, edge.color = &quot;black&quot;) # 確証的因子分析 # library(lavaan) # cfa()関数 "],["クラスター分析.html", "13 クラスター分析 13.1 基本操作 13.2 データ分析例 13.3 発展的なクラスター分析", " 13 クラスター分析 13.1 基本操作 クラスター分析の基本操作を学ぶため, 主成分分析の章でも使用した, 従業員スキル評価データ \\((n=9,p=5)\\) をここでも使用する. tokuten &lt;- read.csv(&quot;testdat_eng.csv&quot;, skip = 1, header = T, row.names = 1) # tokuten &lt;- read.csv(&quot;testdat_jap.csv&quot;, skip = 1, header = T, row.names = 1) # tokuten &lt;- read.csv(&quot;testdat_30_eng.csv&quot;, skip = 1, header = T, row.names = 1) # tokuten &lt;- read.csv(&quot;testdat_30_jap.csv&quot;, skip = 1, header = T, row.names = 1) 階層型クラスタリング 階層型クラスタリングは, Rの標準パッケージの一つstatsに含まれる 関数hclust()を用いて実行することができる. hclust()への入力として, クラスタリング対象間の距離行列を与える必要がある. もし入力データセット (tokuten) の個体間 (行) のクラスタリングを行うのであれば そのまま関数dist()を適用すれば良い. 一方, 変数間 (列) のクラスタリングであれば, 一旦データセットを転置 (行と列の入れ替え) する必要がある. tokuten_dist &lt;- dist(tokuten) # method = &quot;binary&quot;, &quot;canberra&quot;, &quot;maximum&quot;, &quot;manhattan&quot; # 距離行列 round(tokuten_dist, 2) #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; suzuki 9.54 #&gt; tanaka 25.96 27.55 #&gt; nakamura 31.91 35.03 9.27 #&gt; ohno 38.65 41.00 28.21 26.19 #&gt; Matsui 27.50 29.98 11.22 10.10 20.00 #&gt; takagi 18.52 21.73 13.82 18.14 23.13 12.04 #&gt; miura 30.66 32.70 12.57 13.93 18.92 13.04 14.39 #&gt; sato 19.72 26.34 17.18 18.30 30.90 16.76 12.41 21.66 # 距離の近い人を集めて、クラスターを形成 # 1:C1 = {tanaka, nakamura} # 2:C2 = {yamada, suzuki}, or C2 = {C1, matsui} or else?? # --&gt; どちらが優先される? hclust()のデフォルト設定では, 距離尺度はユークリッド距離, クラスター結合方法は最遠隣法/完全連結法となっている. # 最遠隣法/完全連結法 (tokuten_hc_1 &lt;- hclust(tokuten_dist)) # method = &quot;complete&quot; #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist) #&gt; #&gt; Cluster method : complete #&gt; Distance : euclidean #&gt; Number of objects: 9 #summary(tokuten_hc_1) 出力オブジェクトの要素$mergeにクラスター形成過程が格納されている. # クラスター形成履歴 # マイナス付き =&gt; 個体番号. マイナス無し =&gt; クラスター番号 tokuten_hc_1$merge #&gt; [,1] [,2] #&gt; [1,] -3 -4 #&gt; [2,] -1 -2 #&gt; [3,] -6 1 #&gt; [4,] -7 -9 #&gt; [5,] -8 3 #&gt; [6,] 4 5 #&gt; [7,] -5 6 #&gt; [8,] 2 7 また, 出力オブジェクトの要素$heightには, クラスター間の結合が行われる際の距離が格納されている. # デンドログラム (樹形図) の枝の高さ tokuten_hc_1$height #&gt; [1] 9.273618 9.539392 11.224972 12.409674 13.928388 21.656408 30.903074 #&gt; [8] 41.000000 このクラスター形成過程と結合距離を使うことでデンドログラムが作成される. デンドログラムは階層的クラスタリングの結果 (クラスターの形成過程) を視覚化するための有効なツールである. # デンドログラム par(mfrow = c(1, 2)) plot(tokuten_hc_1) plot(tokuten_hc_1, hang = -1) # 葉の位置(高さ)を揃える 階層クラスタリングを実施しデンドログラムを作成したあと, cutreeにクラスター数を指定することで, 木を剪定し (細かい枝を切り落とし) て望ましい数のクラスターに絞り込んで, 個体がそれぞれ所属するクラスターに分類することができる. # 各個体の属するクラスター番号 cutree(tokuten_hc_1, k = 3) # k: クラスター数 #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 2 2 3 2 2 2 #&gt; sato #&gt; 2 次に, クラスター結合方法の相違によるデンドログラムの形状の違いをみてみよう. # 代替的手法 (tokuten_hc_2 &lt;- hclust(tokuten_dist, method = &quot;single&quot;)) # 最近隣法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;single&quot;) #&gt; #&gt; Cluster method : single #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_3 &lt;- hclust(tokuten_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_4 &lt;- hclust(tokuten_dist, method = &quot;average&quot;)) # 群平均法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;average&quot;) #&gt; #&gt; Cluster method : average #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_5 &lt;- hclust(tokuten_dist, method = &quot;centroid&quot;)) # 中心点法/重心法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;centroid&quot;) #&gt; #&gt; Cluster method : centroid #&gt; Distance : euclidean #&gt; Number of objects: 9 (tokuten_hc_6 &lt;- hclust(tokuten_dist, method = &quot;median&quot;)) # メジアン法 #&gt; #&gt; Call: #&gt; hclust(d = tokuten_dist, method = &quot;median&quot;) #&gt; #&gt; Cluster method : median #&gt; Distance : euclidean #&gt; Number of objects: 9 par(mfrow = c(2, 2)) plot(tokuten_hc_1); plot(tokuten_hc_2); plot(tokuten_hc_3) plot(tokuten_hc_4); par(mfrow = c(1, 2)) plot(tokuten_hc_5); plot(tokuten_hc_6) 距離尺度として中心点法 (重心法) やメジアン法を選択した場合には, 「inversion (逆転) 現象」 の発生が確認される. 「inversion現象」とは、階層的クラスタリングのプロセスにおいて, 本来ならばステップが進むにつれてより”遠く”にあるクラスターと結合していくべきところ, 実際にはより”近く”のクラスターが後に統合されるという状況を指す. すなわち, 結合の順序が距離に関する単調性を失い, 距離の近い順に結合するという直感的な (本来あるべき) 順序に反する状態であり, クラスタリングがうまくいっていないことを示す. この現象が生じた場合には, 異なる距離尺度や統合方法を検討する必要がある. k-means法 非階層型クラスタリングの主要な方法であるk-means法は, Rの標準パッケージの一つstatsに含まれる 関数kmeans()を用いて実行することができる. # k-means法 (tokuten_hc_k &lt;- kmeans(tokuten, 3)) #&gt; K-means clustering with 3 clusters of sizes 2, 6, 1 #&gt; #&gt; Cluster means: #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 82.50000 85.00000 67.50000 66.50000 65.0 #&gt; 2 75.33333 75.66667 80.16667 78.66667 79.5 #&gt; 3 88.00000 85.00000 85.00000 90.00000 91.0 #&gt; #&gt; Clustering vector: #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 2 2 3 2 2 2 #&gt; sato #&gt; 2 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 45.5000 540.3333 0.0000 #&gt; (between_SS / total_SS = 72.9 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; #&gt; [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; tokuten_hc_k$cluster #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 1 1 2 2 3 2 2 2 #&gt; sato #&gt; 2 (tokuten_hc_k &lt;- kmeans(tokuten, 2)) #&gt; K-means clustering with 2 clusters of sizes 7, 2 #&gt; #&gt; Cluster means: #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 77.14286 77 80.85714 80.28571 81.14286 #&gt; 2 82.50000 85 67.50000 66.50000 65.00000 #&gt; #&gt; Clustering vector: #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 2 2 1 1 1 1 1 1 #&gt; sato #&gt; 1 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 996.0 45.5 #&gt; (between_SS / total_SS = 51.9 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; #&gt; [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; tokuten_hc_k$cluster #&gt; yamada suzuki tanaka nakamura ohno Matsui takagi miura #&gt; 2 2 1 1 1 1 1 1 #&gt; sato #&gt; 1 変数に対するクラスタリング tokuten_dist3 &lt;- dist(t(tokuten)) tokuten_hc3 &lt;- hclust(tokuten_dist3, method = &quot;ward.D2&quot;) tokuten_hc3$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -3 -5 #&gt; [2,] -4 1 #&gt; [3,] -1 -2 #&gt; [4,] 2 3 tokuten_hc3$height # デンドログラム(樹形図)の枝の高さ #&gt; [1] 12.20656 14.61734 19.74842 44.74967 par(mfrow = c(1, 2)); plot(tokuten_hc3); plot(tokuten_hc3, hang = -1) # デンドログラム cutree(tokuten_hc3, k = 3) # 各個体の属するクラスター番号 (k: クラスター数) #&gt; Expertise Analytics Leadership Presentation Communication #&gt; 1 2 3 3 3 par(mfrow = c(1, 1)) 13.2 データ分析例 データセット (1): ワイン品質データ (再掲) 回帰木で使用したワイン品質データを使用する. ここでは, 回帰問題において目的変数であったワイン品質quality は除き, ワインの化学的特性を表す11の変数のみを用いて, 白ワインのデータセットをクラスター分類することを試みる. - winequality-white.csv - fixed acidity: 酢酸濃度 - volitle acidity: 揮発酸濃度 - citric acidity: クエン酸濃度 - chlorides: 塩化物 - sulfur dioxide: 二酸化硫黄 - sulphate: 硫酸塩 - fixed acidity: 酒石酸含有量（g/dm3) - volatile acidity: 酢酸含有量（g/dm3) - citric acid: クエン酸含有量（g/dm3) - residual sugar: 残留糖分含有量（g/dm3） - chlorides: 塩化ナトリウム含有量（g/dm3) - free sulfur dioxide: 遊離亜硫酸含有量（mg/dm3） - total sulfur dioxide: 総亜硫酸含有量（mg/dm3） - density: 密度（g/dm3) - pH: pH - sulphates: 硫酸カリウム含有量（g/dm3） - alcohol: アルコール度数（% vol.） - quality: ワインの品質 (0 (very bad) -- 10 (excellent)) winedat &lt;- read.csv(&quot;winequality-white.csv&quot;, sep = &quot;;&quot;, skip = 1, header = T) wine &lt;- winedat[, -12] # qualityを除く wine_s &lt;- scale(wine) # 標準化 レコードのクラスタリング 個別のワイン (行) を, 11個の化学的特性 (列) に関する類似性によりクラスタリングする. #wine_dist &lt;- dist(wine) wine_dist &lt;- dist(wine_s) # method = &quot;binary&quot;, &quot;canberra&quot;, &quot;maximum&quot;, &quot;manhattan&quot; (wine_hc &lt;- hclust(wine_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 4898 plot(wine_hc, labels = F, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + Ward&quot;, family = &quot;HiraKakuProN-W3&quot;); #plot(wine_hc, labels=F, main=&quot;wineデータ (標準化前): レコードのクラスタリング\\nEuclidean + Ward&quot;) # 指定したクラスター数kで, デンドログラムを切断 rect.hclust(wine_hc, k = 5, border = &quot;red&quot;) rect.hclust(wine_hc, k = 9, border = &quot;blue&quot;) 変数のクラスタリング 11個の化学的特性 (列) を, ワインに関する類似性によりクラスタリングする. #wine_dist &lt;- dist(t(wine)) wine_dist &lt;- dist(t(wine_s)) (wine_hc &lt;- hclust(wine_dist, method = &quot;ward.D2&quot;)) # ウォード法 #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;ward.D2&quot;) #&gt; #&gt; Cluster method : ward.D2 #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): 変数のクラスタリング\\nEuclidean + Ward&quot;, family = &quot;HiraKakuProN-W3&quot;) #plot(wine_hc, main=&quot;wineデータ (標準化前): 変数のクラスタリング\\nEuclidean + Ward&quot;) クラスター間距離による結果の相違 (wine_hc &lt;- hclust(wine_dist, method = &quot;complete&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;complete&quot;) #&gt; #&gt; Cluster method : complete #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + complete&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;single&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;single&quot;) #&gt; #&gt; Cluster method : single #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + single&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;average&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;average&quot;) #&gt; #&gt; Cluster method : average #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + average&quot;, family = &quot;HiraKakuProN-W3&quot;) (wine_hc &lt;- hclust(wine_dist, method = &quot;centroid&quot;)) #&gt; #&gt; Call: #&gt; hclust(d = wine_dist, method = &quot;centroid&quot;) #&gt; #&gt; Cluster method : centroid #&gt; Distance : euclidean #&gt; Number of objects: 11 plot(wine_hc, main = &quot;wineデータ (標準化後): レコードのクラスタリング\\nEuclidean + centroid&quot;, family = &quot;HiraKakuProN-W3&quot;) 重心法 (中心点法)において, inversion現象の発生が確認される. データセット (2): (ID無し) POSデータ 地方のサービスエリアの売店のレシートデータ500件 (実際のデータに加工を加えた上でランダムにサブセット化. オリジナルの地域・店舗が特定されないように工夫). &quot;pos_sample500_dist.csv&quot; - n = 500, p = 16 - 行: レシート (顧客の一回当り購入バスケット) - 列: &quot;洋菓子土産&quot;〜&quot;デザート類&quot;までの16分類の商品群 - 値: 対応するレシートに含まれる対応する商品の売上個数 (0,1,2,...) データ読み込み &amp; 距離行列の計算 # posデータの読み込み posdat &lt;- read.csv(file(&quot;pos_sample500_dist.csv&quot;, encoding = &#39;Shift_JIS&#39;)) # 500件 階層クラスタリング 16個の商品分類 (列) を, それらの買われ方 (レシートへの反映のされ方) の類似性によって, クラスタリングする. # 商品分類間の距離行列 pos_dist &lt;- dist(t(posdat)) # カテゴリー間の距離 #pos_dist &lt;- dist(posdat) # 顧客間の距離 pos_hc &lt;- hclust(pos_dist, method = &quot;ward.D2&quot;) pos_hc$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -9 -10 #&gt; [2,] -6 -13 #&gt; [3,] -7 1 #&gt; [4,] 2 3 #&gt; [5,] -8 4 #&gt; [6,] -3 5 #&gt; [7,] -16 6 #&gt; [8,] -11 7 #&gt; [9,] -5 8 #&gt; [10,] -12 9 #&gt; [11,] -14 10 #&gt; [12,] -4 11 #&gt; [13,] -15 12 #&gt; [14,] -1 13 #&gt; [15,] -2 14 pos_hc$height # デンドログラム(樹形図)の高さ #&gt; [1] 7.280110 8.062258 8.103497 8.644844 9.014803 9.549370 9.702724 #&gt; [8] 10.785793 12.548572 13.754999 15.099669 18.753461 30.374223 41.487118 #&gt; [15] 65.724108 plot(pos_hc, hang = -1, family = &quot;HiraKakuProN-W3&quot;) # 問題点? デンドログラム内に「chaining現象」が発生していることが分かる. 「chaining現象」は, クラスターが連鎖的に長く伸びる形で形成される現象を指す. この現象は, 階層クラスタリングの過程で, 一つのクラスターが次々に近くのデータ点を吸収して成長していくことで生する. chaining現象は, 特に, 単連結法で生じやすいと考えられる. 本来異なるクラスターに属すべきデータポイントが, 同一のクラスターに統合されてしまうことで, データの真の構造を見落としてしまう可能性がある. 対処法として, クラスター間の結合方法を変える (単連結法から他の方法) ことでchaining現象の影響を軽減できる場合がある. また, 距離尺度を変更することで軽減できる場合がある. このposデータの取る値は, 各レシートごとの商品群の購買個数を表している. 同時購買されやすい・されにくいが商品群間の距離に反映されてしまっている. ここでは, データに加工を施し, 値が購買個数 (頻度) → 購買有無 (0/1)に変換してみる. posdat2 &lt;- posdat posdat2[posdat2&gt;= 2] &lt;- 1 pos_dist2 &lt;- dist(t(posdat2)) # pos_hc2 &lt;- hclust(pos_dist2, method = &quot;ward.D2&quot;) pos_hc2$merge # クラスター形成履歴 #&gt; [,1] [,2] #&gt; [1,] -7 -9 #&gt; [2,] -10 1 #&gt; [3,] -16 2 #&gt; [4,] -3 -13 #&gt; [5,] 3 4 #&gt; [6,] -6 5 #&gt; [7,] -5 6 #&gt; [8,] -11 -14 #&gt; [9,] -8 7 #&gt; [10,] 8 9 #&gt; [11,] -12 10 #&gt; [12,] -4 11 #&gt; [13,] -1 -2 #&gt; [14,] -15 12 #&gt; [15,] 13 14 pos_hc2$height # デンドログラム(樹形図)の高さ #&gt; [1] 4.472136 4.760952 4.983305 5.099020 5.369668 5.550633 6.989788 #&gt; [8] 7.348469 7.348469 8.011356 9.468448 9.536032 15.874508 16.016132 #&gt; [15] 20.502178 plot(pos_hc2, hang = -1, family = &quot;HiraKakuProN-W3&quot;) 更新されたデンドログラムより, chainingが緩和されたのが確認される. 次に, 距離尺度およびクラスター結合方法の変更を試みる. ここでは, 距離尺度としてコサイン距離を, クラスター結合方法としてウォード法を採用する. # マンハッタン距離 #pos_dist &lt;- dist(t(posdat), method = &quot;manhattan&quot;) # cosine距離の使用 library(proxy) pos_dist &lt;- proxy::dist(t(posdat), method = &quot;cosine&quot;) # cosine距離 pos_hc &lt;- hclust(pos_dist, method = &quot;ward.D2&quot;) plot(pos_hc, hang = -1, family = &quot;HiraKakuProN-W3&quot;) pos_dist2 &lt;- proxy::dist(t(posdat2), method = &quot;cosine&quot;) # cosine距離 pos_hc2 &lt;- hclust(pos_dist2, method = &quot;ward.D2&quot;) plot(pos_hc2, hang = -1, family = &quot;HiraKakuProN-W3&quot;) 以上, データ間の距離やクラスター結合方法の選択が結果に大きく影響することが確認される. Hartiganルールによるクラスター数Kの決定 Hartiganルールでは, (k+1)番目のクラスターを加えるか否かの判定を逐次行い, 最適なクラスター数\\(K\\)を決定する. library(useful) ## pos_km &lt;- FitKMeans(posdat, max.cluster = 20, seed = 1) # 客 pos_km_item &lt;- FitKMeans(t(posdat), seed = 1) # 商品 pos_km_item #&gt; Clusters Hartigan AddCluster #&gt; 1 2 14.359657 TRUE #&gt; 2 3 8.985054 FALSE #&gt; 3 4 7.062050 FALSE #&gt; 4 5 3.181427 FALSE #&gt; 5 6 2.307692 FALSE #&gt; 6 7 2.131698 FALSE #&gt; 7 8 1.964241 FALSE #&gt; 8 9 1.112276 FALSE #&gt; 9 10 1.706499 FALSE #&gt; 10 11 1.190476 FALSE #&gt; 11 12 1.142857 FALSE # (クラスター数, Hartigan数, クラスターを追加するべきか否か) PlotHartigan(pos_km_item) # 閾値=10(デフォルト) 上記出力より, クラスター数\\(K=2\\)が選択される. 次元縮約 → 顧客 (レシート) のクラスタリング つぎに, 顧客 (レシート) のクラスタリングを行う. 変数の次元が大きい場合には, クラスタリングの実行に先立って, 類似性を測る変数の次元を落とすのが効果的なことが多い. 本データセットの商品分類数 (16) は大きいとは言えないが, 手順例を紹介する. ここでは, PCAによる次元縮約によって 事前に5つに絞り込んでおく. (なお, “超高次元”の場合には, 通常のPCAはうまく機能しないことが 理論的に示されているため, 別途対応が必要となる.) # 変数(商品)に関して次元縮約pca実行 #posdat_pc &lt;- prcomp(posdat) posdat_pc &lt;- prcomp(posdat, scale. = T) # 標準化 summary(posdat_pc) #&gt; Importance of components: #&gt; PC1 PC2 PC3 PC4 PC5 PC6 PC7 #&gt; Standard deviation 1.26384 1.22037 1.10340 1.07873 1.03901 1.0245 0.99523 #&gt; Proportion of Variance 0.09983 0.09308 0.07609 0.07273 0.06747 0.0656 0.06191 #&gt; Cumulative Proportion 0.09983 0.19291 0.26901 0.34174 0.40921 0.4748 0.53672 #&gt; PC8 PC9 PC10 PC11 PC12 PC13 PC14 #&gt; Standard deviation 0.9911 0.9847 0.96511 0.93339 0.91726 0.89735 0.8772 #&gt; Proportion of Variance 0.0614 0.0606 0.05821 0.05445 0.05259 0.05033 0.0481 #&gt; Cumulative Proportion 0.5981 0.6587 0.71693 0.77138 0.82397 0.87430 0.9224 #&gt; PC15 PC16 #&gt; Standard deviation 0.8070 0.76844 #&gt; Proportion of Variance 0.0407 0.03691 #&gt; Cumulative Proportion 0.9631 1.00000 plot(posdat_pc, family = &quot;HiraKakuProN-W3&quot;) posdat_score5 &lt;- posdat_pc$x[, 1:5] posdat_pc$rotation[, 1:5] #&gt; PC1 PC2 PC3 PC4 PC5 #&gt; 洋菓子土産 0.19801045 -0.41680964 0.07830286 0.204893941 -0.15189427 #&gt; 和菓子土産 0.31926545 -0.17429488 0.29451622 0.024368667 0.06823826 #&gt; 地域限定菓子 0.11093983 -0.21464500 0.08853735 0.122379290 -0.51464240 #&gt; 水産加工品 0.38402139 0.47345139 0.12406842 -0.018704524 0.05301032 #&gt; 地元名産品 0.28312609 0.25920695 0.07566671 -0.432182990 -0.21885500 #&gt; 畜産加工品 0.31510689 0.36152933 0.04970947 0.404289871 -0.10659114 #&gt; 農産加工品 0.14914158 -0.03604771 0.16126513 -0.142280575 0.30221851 #&gt; ご当地グロッサリー 0.20348286 0.20049364 0.03892426 0.150258935 0.38320847 #&gt; 玩具土産 -0.18783969 0.06869904 0.48245433 0.205983425 -0.07184299 #&gt; 雑貨土産 -0.08771897 0.09893981 0.46577309 0.185847154 0.18455564 #&gt; 雑貨類 -0.13980819 0.13804035 -0.38617293 0.166604346 0.17361526 #&gt; 菓子類 -0.33688987 0.10484451 0.32900498 0.001601135 -0.21715666 #&gt; 麺類 0.21239584 0.14992291 -0.10107137 0.106430151 -0.49276377 #&gt; パン.弁当類 -0.14576690 0.29113765 -0.32482427 0.311169546 -0.14077896 #&gt; 飲料 -0.46157964 0.32927414 0.15896997 -0.041046373 -0.11399644 #&gt; デザート等 -0.02334359 0.16142448 0.01003826 -0.578340358 -0.12251255 posdat_dist5 &lt;- dist(posdat_score5) # レコード間 #posdat_dist5 &lt;- dist(t(posdat_score5)) # 合成変数間 posdat_hc5 &lt;- hclust(posdat_dist5, method = &quot;ward.D2&quot;) plot(posdat_hc5, family = &quot;HiraKakuProN-W3&quot;) rect.hclust(posdat_hc5, k = 8, border = &quot;blue&quot;) # pos_km5 &lt;- FitKMeans(posdat_score5, max.cluster = 20, seed = 1) # PlotHartigan(pos_km5) 上では, 顧客 (レシート) に対して階層クラスタリングを実施したが, 件数の多い場合には, 注意が必要である. 主な問題点は計算コストと可視化の困難さである. 階層的クラスタリングにおいて, データ点の数が非常に多い場合, 計算時間やメモリ消費が膨大になる可能性がある (全てのデータ点間で距離を計算し, これらの距離に基づいて段階的にクラスターを形成する必要性) . また, データポイントの数が多い場合、デンドログラムは非常に密集してしまい、個々のクラスターやクラスタリングの構造を解釈することが困難で, 有益な情報を得られにくくなる. 13.3 発展的なクラスター分析 先に使用した従業員評価データ (拡張版) をここでも使用する. 距離としては, 相関係数 (類似度) をベースにしたcosine距離 (=1-相関係数) を使用する. tokuten &lt;- read.csv(&quot;testdat_30_jap.csv&quot;, skip = 1, header = T, row.names = 1) 関数agnes パッケージcluster内にある関数agnes (AGglomerative NESting) を使うことで, 階層型クラスタリングをより高度に制御しながら実行することが可能. library(cluster) dist_matrix &lt;- as.dist(1 - cor(tokuten)) res_agnes &lt;- agnes(dist_matrix, method = &quot;average&quot;) plot(res_agnes) dist_matrix &lt;- as.dist(1 - cor(t(tokuten))) res_agnes &lt;- agnes(dist_matrix, method = &quot;average&quot;) plot(res_agnes) 関数pheatmap パッケージpheatmapは, 主にヒートマップの作成に使用されるが, 相関に基づく階層型クラスタリングを行い, ヒートマップとして可視化する機能もあり. 遺伝子発現データなどのbioinformatics分野で活用. library(pheatmap) pheatmap(tokuten, clustering_distance_rows = as.dist(1 - cor(t(tokuten))), clustering_distance_cols = as.dist(1 - cor(tokuten))) 関数pam メドイド (medoid) は, クラスタ内の全ての点に対する距離の合計が最小となるような, クラスタ内に存在するデータ点であり, K-medoids法は, メドイドを中心としてクラスタリングを行う手法である. PAM (Partitioning Around Medoids) は, K-medoids法の考え方を実装した具体的アルゴリズムの一種である. Rでは, パッケージcluster内の関数pamを使用することができる. 一方, K-medoids法に類似したクラスタリング手法として, K-median法がある. K-median法は, 各クラスタ内のデータポイントの中央値に基づいて「中央点」決定し, クラスタ内の全データポイントと「中央点」との距離の合計を最小化するようにクラスタリングを行う. これらの手法を使うことで, 外れ値に強い, ロバストなクラスタリング結果が得られる. # pam関数によるK-medoids法の実行 k &lt;- 3 # クラスタの数 res_pam &lt;- pam(tokuten, k) print(res_pam) #&gt; Medoids: #&gt; ID 専門性 分析力 リーダーシップ プレゼン力 コミュ力 #&gt; 山口 15 77 87 71 68 66 #&gt; 伊藤 7 80 80 75 75 80 #&gt; 石川 20 80 80 83 86 86 #&gt; Clustering vector: #&gt; 山田 鈴木 田中 中村 大野 小林 伊藤 高橋 渡辺 佐藤 山下 #&gt; 1 1 2 3 3 3 2 3 2 2 3 #&gt; 木村 山本 宮崎 山口 阿部 斎藤 吉田 佐々木 石川 山崎 中山 #&gt; 3 2 3 1 3 2 1 1 3 1 3 #&gt; 藤田 加藤 清水 池田 井上 林 中島 森 #&gt; 2 1 3 3 2 1 2 3 #&gt; Objective function: #&gt; build swap #&gt; 10.01806 10.01806 #&gt; #&gt; Available components: #&gt; [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; &quot;isolation&quot; #&gt; [6] &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;data&quot; plot(res_pam) "],["xai-explainable-ai.html", "14 XAI (Explainable AI) 14.1 PFI 14.2 PDP 14.3 ICE 14.4 LIME 14.5 SHAP", " 14 XAI (Explainable AI) 深層学習に代表される“予測精度”を追求する (機械学習を含む) AIのモデルは,「高精度」な一方「ブラックボックス」なものが多い. しかし, これらのモデル・アルゴリズムの実用化に際して, 特に, 医療・金融・司法など人のいのちや財産, 社会秩序に関する領域などにおいては,「説明責任」「透明性」が強く求められる. XAI（Explainable AI）は, AI研究の一領域として 登場し, AIの予測力の向上に伴い, 2010年代後半頃から活発に研究が行われるようになってきている. XAIの進展の裏にある社会的・倫理的な背景として, 欧州を中心に, AIの開発・利用に対して, 意思決定の過程がわかること (透明性), 人間がモデルの判断に責任を持てること (説明責任), 差別やバイアスがないこと (公正性), 利用者が安心して使えること (信頼性) などを求める機運の高まりがある. (国際・国内のガイドラインとして, EU: Art. 22 GDPR, OECD: AI原則 (2019), 内閣府: 人間中心のAI原則 (2019), 等) このようなAI技術を取り巻く世界的な潮流の中, XAIは,「ブラックボックス」であるAIを「解釈可能」にすることを目的とする方法(論) の総称を指す. 本章では, XAIの以下の4つの手法を取り上げ, パッケージimlを用いて実際に動かしてみる. PDP (Partial Dependence Plot ICE (Individual Conditional Expectaion LIME (Local Interpretable Model-agnostic Explanations) SHAP (SHapley Additive exPlanations) 本章の主要な参考資料: (https://cran.r-project.org/web/packages/iml/vignettes/intro.html)[https://cran.r-project.org/web/packages/iml/vignettes/intro.html] (https://christophm.github.io/interpretable-ml-book/)[https://christophm.github.io/interpretable-ml-book/] # install.packages(&quot;iml&quot;) library(iml) 回帰問題のケース データセット (1): ボストン市内住宅物件価格データ ここでは, すでに馴染みのあるデータセットBoston Housingを利用する. - Boston Housingデータ - crim: 地域の一人当たり犯罪率 - zn: 25,000平方フィート以上の住宅用地の割合 - indus: 地域の非小売業の土地の割合 - chas: チャールズ川のダミー変数 (1: 川沿い, 0: それ以外) - nox: 窒素酸化物濃度（1000万ppm） - rm: 住宅の平均部屋数 - age: 1940年以前に建設された持ち家の割合 - dis: ボストンの5つの雇用中心地 (employment centers) までの距離の加重平均 - rad: 放射状高速道路へのアクセス指数 - tax: 10,000米ドル当たりの固定資産税率 - ptratio: 地域の生徒数・教師数比率 - b: 人種的指標, 1000(B - 0.63)^2, (Bは地域の黒人の割合) - lstat: 低所得者層の割合 - medv: 持ち家住宅の中央値（1000ドル単位） - 506件 x 14変数 (オリジナル版) - source: http://lib.stat.cmu.edu/datasets/boston data(&quot;Boston&quot;, package = &quot;MASS&quot;) housing &lt;- Boston head(housing, 10) #&gt; crim zn indus chas nox rm age dis rad tax ptratio black #&gt; 1 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 #&gt; 2 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 #&gt; 3 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 #&gt; 4 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 #&gt; 5 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 #&gt; 6 0.02985 0.0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 #&gt; 7 0.08829 12.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 395.60 #&gt; 8 0.14455 12.5 7.87 0 0.524 6.172 96.1 5.9505 5 311 15.2 396.90 #&gt; 9 0.21124 12.5 7.87 0 0.524 5.631 100.0 6.0821 5 311 15.2 386.63 #&gt; 10 0.17004 12.5 7.87 0 0.524 6.004 85.9 6.5921 5 311 15.2 386.71 #&gt; lstat medv #&gt; 1 4.98 24.0 #&gt; 2 9.14 21.6 #&gt; 3 4.03 34.7 #&gt; 4 2.94 33.4 #&gt; 5 5.33 36.2 #&gt; 6 5.21 28.7 #&gt; 7 12.43 22.9 #&gt; 8 19.15 27.1 #&gt; 9 29.93 16.5 #&gt; 10 17.10 18.9 # 目的変数の全体平均 mean(housing$medv) #&gt; [1] 22.53281 はじめに, 次に行うAIモデルによる予測およびそれに対するXAIとの比較のためのベースラインとして, 線形回帰を実行しておく. fit_lm &lt;- lm(medv ~ ., data = housing) summary(fit_lm) #&gt; #&gt; Call: #&gt; lm(formula = medv ~ ., data = housing) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -15.595 -2.730 -0.518 1.777 26.199 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** #&gt; crim -1.080e-01 3.286e-02 -3.287 0.001087 ** #&gt; zn 4.642e-02 1.373e-02 3.382 0.000778 *** #&gt; indus 2.056e-02 6.150e-02 0.334 0.738288 #&gt; chas 2.687e+00 8.616e-01 3.118 0.001925 ** #&gt; nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** #&gt; rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** #&gt; age 6.922e-04 1.321e-02 0.052 0.958229 #&gt; dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** #&gt; rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** #&gt; tax -1.233e-02 3.760e-03 -3.280 0.001112 ** #&gt; ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** #&gt; black 9.312e-03 2.686e-03 3.467 0.000573 *** #&gt; lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.745 on 492 degrees of freedom #&gt; Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 #&gt; F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 head(predict(fit_lm), 10) #&gt; 1 2 3 4 5 6 7 8 #&gt; 30.00384 25.02556 30.56760 28.60704 27.94352 25.25628 23.00181 19.53599 #&gt; 9 10 #&gt; 11.52364 18.92026 本セクションでは, XAIを適用するターゲットの”ブラックボックス”のAIモデルとして, ランダムフォレストを使って学習したモデルを採用することにする. # ランダムフォレストを予測モデルとして用いる例 library(randomForest) set.seed(123) fit_rf &lt;- randomForest(medv ~ ., data = housing, importance = T) 注) set.seed() は, ランダム性が導入される関数の直前で使うのが良い. # 予測値 head(predict(fit_rf), 10) #&gt; 1 2 3 4 5 6 7 8 #&gt; 28.13112 23.10451 35.59140 35.84795 33.27872 26.74190 20.59782 17.69223 #&gt; 9 10 #&gt; 18.12466 19.24212 # 予測値の全体平均 mean(predict(fit_rf), 10) #&gt; [1] 20.99478 ちなみに, パッケージimlを使わずとも, パッケージrandomForestの関数randomForest()の出力オブジェクトに対して 関数varImpPlot(), importance()を適用することで, 予測の精度に大きく影響を与える重要な変数を調べることができる (変数重要度, variable importance, または, 特徴量重要度, feature importance). varImpPlot(fit_rf) importance(fit_rf) # variable importance measuresの計算 #&gt; %IncMSE IncNodePurity #&gt; crim 16.791858 2466.9292 #&gt; zn 3.963144 339.2476 #&gt; indus 11.099242 2623.0736 #&gt; chas 4.552989 233.0437 #&gt; nox 17.347112 2798.0707 #&gt; rm 38.544414 12418.5753 #&gt; age 13.912714 1150.8200 #&gt; dis 19.502610 2548.5988 #&gt; rad 6.766537 373.6200 #&gt; tax 12.182840 1234.9363 #&gt; ptratio 16.404295 2613.0837 #&gt; black 10.020137 768.5835 #&gt; lstat 31.462441 12225.0302 importance()の出力として, 1つ目の指標は, 分類問題においては, 誤差率, 回帰問題においては, MSEを 予測誤差として採用し, OOB (Out-Of-Bounce) データを使用し, 学習モデルで計算される予測誤差と, 各予測変数につきpermutationしたモデルで計算される予測誤差の差 の大きさを全ての木にわたって標準化した値である. 2つ目の指標は, 分類問題においてはGini係数, 回帰問題においてはRSSを 各ノードの”不純度”として採用し, その予測変数での木の分割による不純度の減少の合計を 全ての木にわたって平均したものである. メモ - パッケージimlは, &quot;R6クラス&quot;と呼ばれる (オブジェクト指向型プログラミングのための) Rのオブジェクト・クラスを使用 - → 例えば, Predict$new()により, 新しいオブジェクトを生成 (なお, set.seed()は不要) X &lt;- housing[, -14] # medvを除く predictor &lt;- Predictor$new(fit_rf, data = X, y = housing$medv) 以下では, ランダムフォレストの適合をもとに生成されたオブジェクト predictorに対して, 各手法の関数を適用する. 14.1 PFI 特徴量lstatに対するPFI (Permuation Feature Importance) を見たい場合には, 以下のコマンドを実行する. # imp &lt;- FeatureImp$new(predictor, loss = &quot;mse&quot;) # 損失関数=MSE set.seed(111) imp &lt;- FeatureImp$new(predictor, loss = &quot;mae&quot;) # 同MAE library(&quot;ggplot2&quot;) imp$plot() # plot(imp) imp$results #&gt; feature importance.05 importance importance.95 permutation.error #&gt; 1 lstat 4.405448 4.542547 4.738602 4.2729783 #&gt; 2 rm 3.583264 3.673631 3.808067 3.4556260 #&gt; 3 nox 1.788788 1.804862 1.815528 1.6977554 #&gt; 4 dis 1.667062 1.699978 1.730544 1.5990959 #&gt; 5 crim 1.687770 1.698387 1.727456 1.5975997 #&gt; 6 ptratio 1.626979 1.673878 1.725824 1.5745446 #&gt; 7 indus 1.492030 1.509591 1.534473 1.4200074 #&gt; 8 age 1.398376 1.424283 1.428581 1.3397619 #&gt; 9 tax 1.347987 1.369031 1.401911 1.2877886 #&gt; 10 black 1.240707 1.259959 1.276161 1.1851890 #&gt; 11 rad 1.113053 1.124778 1.127646 1.0580301 #&gt; 12 zn 1.051388 1.058540 1.061038 0.9957227 #&gt; 13 chas 1.029757 1.037440 1.047849 0.9758753 これより, lstat, 次にrmの重要度が特に大きいことが分かる. 14.2 PDP 特徴量lstatに対するPDP (Partial Dependence Plot) を見たい場合には, 以下のコマンドを実行する. # 特徴量lstatに対するPDP pdp &lt;- FeatureEffect$new(predictor, feature = &quot;lstat&quot;, method = &quot;pdp&quot;, grid.size = 10) pdp$plot() 変数lstatが増えれば目的変数medvが減少すること, しかも非線形であること (下に凸) が観察される. 他の特徴量, たとえば, lstatについで重要度の次に大きいrmや, ptratioに対するICEは, 上の出力を再利用することで作成できる. # 他の特徴量に対するPDP # set.featureで指定 pdp$set.feature(&quot;rm&quot;) pdp$plot() # pdp$set.feature(&quot;ptratio&quot;) pdp$plot() 14.3 ICE データ点ごとの, 特徴量に対する予測値の変化はICE (Individual Conditional Expectaion) で知ることができる. 特徴量lstatに対するICEを見たい場合には, 以下のコマンドを実行する. ice &lt;- FeatureEffect$new(predictor, feature = &quot;lstat&quot;, method = &quot;ice&quot;, grid.size = 10) ice$plot() 特徴量rmやptrationに対するICEは, 上の出力を再利用することで作成できる. ice$set.feature(&quot;rm&quot;) ice$plot() # ice$set.feature(&quot;ptratio&quot;) ice$plot() PDP + ICE また, PDPとICEを同様にプロットにすることができる. pdp_ice &lt;- FeatureEffect$new(predictor, feature = &quot;ptratio&quot;, method = &quot;pdp+ice&quot;, grid.size = 10) pdp_ice$plot() 14.4 LIME 局所代理モデルの一手法であるLIME (Local Interpretable Model-agnostic Explanations), そのバリエーションを実行する. LocalModel(): - LIMEの一種. 局所的重み付け回帰モデルによる適合 - ターゲットのデータ点からの距離で重み付け - 回帰問題は線形回帰, 分析問題はロジスティック回帰 - L1-正則化によりスパース性に対応 - 内部では, 関数glmmet() 使用 - LIMEとの相違 - 近傍として, 乱数ではなく, オリジナルデータを使用する - ユークリッド距離に基づいたカーネルではなく, gower距離によって近さを評価 1番目のデータ点の予測値 (25.61) に対するLIMEは以下のように実行する. lime_explain &lt;- LocalModel$new(predictor, x.interest = X[1, ]) # 線形近似 (LocalModel()) による予測値 predict(lime_explain) #&gt; prediction #&gt; 1 28.83728 lime_explain$results #&gt; beta x.recoded effect x.original feature feature.value #&gt; rm 4.2921777 6.575 28.221068 6.575 rm rm=6.575 #&gt; ptratio -0.5211485 15.300 -7.973572 15.3 ptratio ptratio=15.3 #&gt; lstat -0.4304489 4.980 -2.143635 4.98 lstat lstat=4.98 lime_explain$plot() # plot(lime_explain) 注) 標準のLIMEと異なり, LocalModel()は近傍データ生成に乱数は非使用 (∴set.seed()は不要. 但し, 不確かな場合には常に実行するのが安全) 得られた結果は, 1番目のデータ点の予測に対する判断の根拠を 線形回帰の結果を解釈しながら与えることができる. すなわち, (LASSOにより選択された) 変数の回帰係数の推定値が \\(\\beta_{rm}=4.29\\), \\(\\beta_{ptratio}=-0.52\\), \\(\\beta_{lstat}=-0.43\\) であることから, 1番目の予測に対しては, 変数rmは (他の 全ての変数を固定した状態で) 1単位増えれば, \\(4,29\\)増加する要因と なっているが, 一方, lstat, ptratioはそれぞれ, \\(-0.52, -0.43\\)減少 させていることを示す. また, 各変数が1番目の予測にもたらしている貢献度の大きさは, 各変数の値を対応する係数と掛け合せることで, \\((4.29) \\cdot 6.58 \\approx 28.22\\), \\((-0.52) \\cdot 15.3 \\approx -7.97\\), \\((-0.43) \\cdot 4.98 \\approx -2.14\\) と (近似的に) 見積もることができ, これが横棒グラフで示されている. 同様に, 2番目のデータ点の予測値 (22.27) に対するLIMEは以下の通りである. lime_explain$explain(X[2, ]) lime_explain$results #&gt; beta x.recoded effect x.original feature feature.value #&gt; rm 4.1213297 6.421 26.463058 6.421 rm rm=6.421 #&gt; ptratio -0.5276824 17.800 -9.392746 17.8 ptratio ptratio=17.8 #&gt; lstat -0.4368639 9.140 -3.992936 9.14 lstat lstat=9.14 lime_explain$plot() # plot(lime_explain) 1番目と比較して, 各変数の値ばかりでなく, 回帰係数の大きさも多少ではあるが異なっている. すなわち, 貢献度の大きさとしては, rmは1.7程度減少し, 一方, ptratio, lstatはそれぞれ-1.4, -1.8程度マイナス方向に大きくなっており, これらが, 予測値を相対的に小さくすることに つながっている可能性が考えられる. ただし, LIMEでは, 各データ点ことに異なる 代理モデルによる説明がなされることから, 異なるデータ点間の結果を直接比較することは元来想定しておらず, 上のような推察の妥当性はただちに保証されるものではない. 異なるデータ点直接比較したい場合には, 例えば, 先のICEや, 次のSHAP等が有効である. 14.5 SHAP SHAP (SHapley Additive exPlanations) は, 協力ゲーム理論に基づいて, データ点に対するモデル予測値を, 平均的なモデル予測値と各特徴量の“貢献度”の和で表現する方法で, 各データ点の予測値に対して解釈性を与える一手法である. set.seed(123) # 1番目のデータ点での予測に対する分解 (局所的説明) shapley &lt;- Shapley$new(predictor, x.interest = X[1, ], sample.size = 50) shapley$plot() # plot(shapley) アウトプットの中で, SHAP 値（Shapley 値）は phiを読めばよい. phi.var はモンテカルロ標本から計算される当該特徴量の寄与の分散の大きさである (小さいほど信頼性が高い). sample.sizeを大きくすることで, 推定の精度を高めることができる. feature.valueは指定したデータ点における各特徴量の大きさを示す. 上記コードは実行時間がかかる. 引数sample.sizeの大きさで計算スピードとSHAP値計算の近似精度を コントロールできる. ここでは, 50に設定する. 注) Shapley 値の厳密計算は全特徴の順列 (coalitions) に対して計算する必要があるが, 特徴数が多い場合は計算量が爆発する. そこで, iml の Shapley クラスは, sample.sizeで指定した回数だけランダムに順列を生成し, 近似値を計算する. 結果の再現性を得るためには, Shapley$new() の直前に set.seed() を 呼び出すことで, 乱数を固定する必要がある. Shapley() - 引数sample.size: Monte Carlo samples for estimating the Shapley value. The number of times coalitions/marginals are sampled from data X. The higher the more accurate the explanations become. # 結果の取り出し shapley$results #&gt; feature phi phi.var feature.value #&gt; 1 crim -0.08912329 1.01012316 crim=0.00632 #&gt; 2 zn 0.03006019 0.01803672 zn=18 #&gt; 3 indus 0.73077923 0.93525970 indus=2.31 #&gt; 4 chas -0.04483238 0.03785947 chas=0 #&gt; 5 nox -0.10584816 0.46886359 nox=0.538 #&gt; 6 rm -0.53912733 17.44803543 rm=6.575 #&gt; 7 age 0.01470670 0.41226685 age=65.2 #&gt; 8 dis -0.09587154 1.21952939 dis=4.09 #&gt; 9 rad -0.28381549 0.04738203 rad=1 #&gt; 10 tax -0.05629366 0.27861079 tax=296 #&gt; 11 ptratio 0.82536628 1.28844605 ptratio=15.3 #&gt; 12 black -0.04250537 0.16910755 black=396.9 #&gt; 13 lstat 2.95122635 13.04978775 lstat=4.98 SHAPの方法論によれば, 1番目のデータ点に対する予測値 (25.49) は, 全体平均 (22.54) にプラスして, 各予測変数からの貢献に分解できる (乱数の発生具合で結果が変わる). \\[ \\hat{f}({\\bf x}_1) = E[\\hat{f}({\\bf X})] + \\phi_{crim} + \\phi_{zn} + \\cdots + \\phi_{black} + \\phi_{lstat} \\] \\[ 25.72 = 22.52 - 0.09 + 0.03 + \\cdots - 0.04 + 2.95 \\] 中でも, 貢献度の大きな項 (変数と値 (単位は千ドル)) は, \\[ \\phi_{lstat} = 2.95, \\phi_{ptratio} = 0.83,\\phi_{indus} = 0.73, \\phi_{rm} = - 0.54, \\phi_{rad} = - 0.28,\\phi_{nox} = - 0.11, \\] 等となっている. 注) SHAPでは, 全体平均を除いた部分について, 各変数の貢献度を線形に分解する. ここでの結果は, lstat, ptratiは大きくプラスに貢献, 一方, rmはマイナスに貢献となっており, 最初に行った 線形回帰はもとより, 上の局所線形モデルによる 解釈と符号が異なっていることに注意したい. # 同2番目のデータ点 (上で得られたオブジェクトの再利用) shapley$explain(x.interest = X[2, ]) shapley$plot() 注) x.interest をコンストラクタShapley$new()に渡した場合、その時点で当該インスタンスの Shapley 値が計算され, shapley$results に格納される. 一方, $explain() は, 別のデータ点 (インスタンス) を後から説明したいときに使うメソッドである. # 結果の取り出し shapley$results #&gt; feature phi phi.var feature.value #&gt; 1 crim -0.055606769 1.01576757 crim=0.02731 #&gt; 2 zn -0.038531200 0.01179324 zn=0 #&gt; 3 indus -0.260136695 0.47798871 indus=7.07 #&gt; 4 chas 0.000000000 0.00000000 chas=0 #&gt; 5 nox 0.382234296 1.01537087 nox=0.469 #&gt; 6 rm -1.422404701 19.08075359 rm=6.421 #&gt; 7 age -0.231572537 0.20677965 age=78.9 #&gt; 8 dis -0.179151233 0.34188332 dis=4.9671 #&gt; 9 rad -0.158878084 0.03891879 rad=2 #&gt; 10 tax 0.171894830 0.54395722 tax=242 #&gt; 11 ptratio 0.201743110 0.82863870 ptratio=17.8 #&gt; 12 black -0.007767299 0.04527014 black=396.9 #&gt; 13 lstat 0.455681606 12.42825560 lstat=9.14 注) 授業内配布済Rコード内では, 以下のブロックが記載されていましたが, マニュアルに記載されていない用法であったため, 訂正の上, 削除いたします # 以下のブロックは削除のこと # 全データセットの平均 (大局的説明) shapley$explain(x.interest = X) shapley$plot() # plot(shapley) # average 分類問題のケース 次に, 分類問題のケースについて扱う. #### データセット (2): irisデータ {-} データセットとしては, (レコード数も変数も少なく出力結果を理解しやすい) irisを使用する. 多値分類問題の例 (iris) ここでは, Shapley()の動作確認に主眼におき, 平易な設定を扱う. まず, irisのターゲットの変数Speciesは, 3水準のカテゴリー変数 (因子型)で, setosa (1〜50行), versicolor (51〜100行), virginica (101〜150行) の値を持つ. iris[1:3, ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa iris[51:53, ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 51 7.0 3.2 4.7 1.4 versicolor #&gt; 52 6.4 3.2 4.5 1.5 versicolor #&gt; 53 6.9 3.1 4.9 1.5 versicolor iris[101:103, ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 102 5.8 2.7 5.1 1.9 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica また, “ホワイトボックス”モデルである決定木をターゲットモデルに採用し, SHAPの出力がどのようなものになるかを確認する. # Species: setosa (1〜50), versicolor (51〜100), virginica (101〜150) library(rpart) set.seed(123) fit_rpart &lt;- rpart(Species ~ ., data = iris) library(rpart.plot) rpart.plot(fit_rpart, digit = 3) ローカルな分析を行うことから, 比較のために, ターゲット変数Speciesの3水準の各値に対応するデータ点 (インスタンス) として, 1,2 (setosa), 51 (versicolor), 101 (virginica) をピックアップする. # 決定木による確率予測値 predict(fit_rpart)[c(1, 2, 51, 101), ] #&gt; setosa versicolor virginica #&gt; 1 1 0.00000000 0.00000000 #&gt; 2 1 0.00000000 0.00000000 #&gt; 51 0 0.90740741 0.09259259 #&gt; 101 0 0.02173913 0.97826087 個々のデータ点における, モデルの出力である確率予測に対する 各特徴量の貢献度の評価を行いたい ($type = “prob”). X &lt;- iris[, -5] mod &lt;- Predictor$new(fit_rpart, data = X, type = &quot;prob&quot;) # 1番目のデータ点 (setosa) に対する説明 set.seed(123) shapley &lt;- Shapley$new(mod, x.interest = X[1, ]) shapley$results #&gt; feature class phi phi.var feature.value #&gt; 1 Sepal.Length setosa 0.0000000 0.00000000 Sepal.Length=5.1 #&gt; 2 Sepal.Width setosa 0.0000000 0.00000000 Sepal.Width=3.5 #&gt; 3 Petal.Length setosa 0.7200000 0.20363636 Petal.Length=1.4 #&gt; 4 Petal.Width setosa 0.0000000 0.00000000 Petal.Width=0.2 #&gt; 5 Sepal.Length versicolor 0.0000000 0.00000000 Sepal.Length=5.1 #&gt; 6 Sepal.Width versicolor 0.0000000 0.00000000 Sepal.Width=3.5 #&gt; 7 Petal.Length versicolor -0.5647665 0.19352196 Petal.Length=1.4 #&gt; 8 Petal.Width versicolor 0.1682770 0.12193984 Petal.Width=0.2 #&gt; 9 Sepal.Length virginica 0.0000000 0.00000000 Sepal.Length=5.1 #&gt; 10 Sepal.Width virginica 0.0000000 0.00000000 Sepal.Width=3.5 #&gt; 11 Petal.Length virginica -0.1552335 0.07769444 Petal.Length=1.4 #&gt; 12 Petal.Width virginica -0.1682770 0.12193984 Petal.Width=0.2 plot(shapley) 1番目のデータ点は, \\(\\phi_{Petal.Length}=0.70\\) であり, 予測モデルがSpecies==setosaの確率\\(\\approx1\\)と予測した 最大の理由となっていることを示している. # 2番目 (setosa) shapley$explain(x.interest = X[2, ]) shapley$results #&gt; feature class phi phi.var feature.value #&gt; 1 Sepal.Length setosa 0.0000000 0.0000000 Sepal.Length=4.9 #&gt; 2 Sepal.Width setosa 0.0000000 0.0000000 Sepal.Width=3 #&gt; 3 Petal.Length setosa 0.6800000 0.2197980 Petal.Length=1.4 #&gt; 4 Petal.Width setosa 0.0000000 0.0000000 Petal.Width=0.2 #&gt; 5 Sepal.Length versicolor 0.0000000 0.0000000 Sepal.Length=4.9 #&gt; 6 Sepal.Width versicolor 0.0000000 0.0000000 Sepal.Width=3 #&gt; 7 Petal.Length versicolor -0.4664734 0.2044554 Petal.Length=1.4 #&gt; 8 Petal.Width versicolor 0.1417069 0.1064894 Petal.Width=0.2 #&gt; 9 Sepal.Length virginica 0.0000000 0.0000000 Sepal.Length=4.9 #&gt; 10 Sepal.Width virginica 0.0000000 0.0000000 Sepal.Width=3 #&gt; 11 Petal.Length virginica -0.2135266 0.1226948 Petal.Length=1.4 #&gt; 12 Petal.Width virginica -0.1417069 0.1064894 Petal.Width=0.2 plot(shapley) # 51番目 (versicolor) shapley$explain(x.interest = X[51, ]) shapley$results #&gt; feature class phi phi.var feature.value #&gt; 1 Sepal.Length setosa 0.00000000 0.000000000 Sepal.Length=7 #&gt; 2 Sepal.Width setosa 0.00000000 0.000000000 Sepal.Width=3.2 #&gt; 3 Petal.Length setosa -0.38000000 0.237979798 Petal.Length=4.7 #&gt; 4 Petal.Width setosa 0.00000000 0.000000000 Petal.Width=1.4 #&gt; 5 Sepal.Length versicolor 0.00000000 0.000000000 Sepal.Length=7 #&gt; 6 Sepal.Width versicolor 0.00000000 0.000000000 Sepal.Width=3.2 #&gt; 7 Petal.Length versicolor 0.34481481 0.195949758 Petal.Length=4.7 #&gt; 8 Petal.Width versicolor 0.32769726 0.184692499 Petal.Width=1.4 #&gt; 9 Sepal.Length virginica 0.00000000 0.000000000 Sepal.Length=7 #&gt; 10 Sepal.Width virginica 0.00000000 0.000000000 Sepal.Width=3.2 #&gt; 11 Petal.Length virginica 0.03518519 0.002040293 Petal.Length=4.7 #&gt; 12 Petal.Width virginica -0.32769726 0.184692499 Petal.Width=1.4 plot(shapley) # 101番目 (verginica) shapley$explain(x.interest = X[101, ]) shapley$results #&gt; feature class phi phi.var feature.value #&gt; 1 Sepal.Length setosa 0.0000000 0.00000000 Sepal.Length=6.3 #&gt; 2 Sepal.Width setosa 0.0000000 0.00000000 Sepal.Width=3.3 #&gt; 3 Petal.Length setosa -0.3100000 0.21606061 Petal.Length=6 #&gt; 4 Petal.Width setosa 0.0000000 0.00000000 Petal.Width=2.5 #&gt; 5 Sepal.Length versicolor 0.0000000 0.00000000 Sepal.Length=6.3 #&gt; 6 Sepal.Width versicolor 0.0000000 0.00000000 Sepal.Width=3.3 #&gt; 7 Petal.Length versicolor 0.1307327 0.09925621 Petal.Length=6 #&gt; 8 Petal.Width versicolor -0.4782609 0.19681517 Petal.Width=2.5 #&gt; 9 Sepal.Length virginica 0.0000000 0.00000000 Sepal.Length=6.3 #&gt; 10 Sepal.Width virginica 0.0000000 0.00000000 Sepal.Width=3.3 #&gt; 11 Petal.Length virginica 0.1792673 0.13308338 Petal.Length=6 #&gt; 12 Petal.Width virginica 0.4782609 0.19681517 Petal.Width=2.5 plot(shapley) # 上と同じ理由により, 以下のブロックは削除のこと # 全データセットの平均 (大局的説明) shapley$explain(x.interest = X) shapley$results plot(shapley) 2値分類問題 最後に, (setosaか否か) に限定して動作を確認する. # 一つのクラス (setosa) にフォーカスする場合 # setosaか否か mod &lt;- Predictor$new(fit_rpart, data = X, type = &quot;prob&quot;, class = &quot;setosa&quot;) set.seed(123) shapley &lt;- Shapley$new(mod, x.interest = X[1, ]) shapley$results #&gt; feature phi phi.var feature.value #&gt; 1 Sepal.Length 0.00 0.0000000 Sepal.Length=5.1 #&gt; 2 Sepal.Width 0.00 0.0000000 Sepal.Width=3.5 #&gt; 3 Petal.Length 0.72 0.2036364 Petal.Length=1.4 #&gt; 4 Petal.Width 0.00 0.0000000 Petal.Width=0.2 plot(shapley) shapley$explain(x.interest = X[2, ]) shapley$plot() shapley$results #&gt; feature phi phi.var feature.value #&gt; 1 Sepal.Length 0.00 0.000000 Sepal.Length=4.9 #&gt; 2 Sepal.Width 0.00 0.000000 Sepal.Width=3 #&gt; 3 Petal.Length 0.68 0.219798 Petal.Length=1.4 #&gt; 4 Petal.Width 0.00 0.000000 Petal.Width=0.2 shapley$explain(x.interest = X[51, ]) shapley$plot() shapley$results #&gt; feature phi phi.var feature.value #&gt; 1 Sepal.Length 0.00 0.0000000 Sepal.Length=7 #&gt; 2 Sepal.Width 0.00 0.0000000 Sepal.Width=3.2 #&gt; 3 Petal.Length -0.38 0.2379798 Petal.Length=4.7 #&gt; 4 Petal.Width 0.00 0.0000000 Petal.Width=1.4 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
